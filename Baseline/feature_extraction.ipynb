{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf7e18-4d7c-4f39-998a-320f28b5b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import pandas as pd\n",
    "import string\n",
    "from datamuse import datamuse\n",
    "from pycorenlp import StanfordCoreNLP \n",
    "import json\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2bf04e-1411-4509-8f56-a61dd8f836cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts the features required for the CWI basline system as described in CWI3G3G2 in (Yimam 2017), with only the most basic frequency features.\n",
    "1) Number of vowels\n",
    "2)Number of syllables\n",
    "3) Number of characters\n",
    "\n",
    "4) Frewuency in simple Wiki\n",
    "5)Frquency of word in HIT paragraph\n",
    "6) Frequency of word in Ngram corpus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0d807-c5fa-46df-bded-0bb954e12fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1- New preprocessing to handle hyphenated and MWEs\n",
    "# Now hyphens are treated a seperate words and added to the split column.\n",
    "\n",
    "location = \"cwishareddataset/traindevset/english/Wikipedia_Train.tsv\"\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "data_frame = pd.read_table(location, names=('ID', 'sentence', 'start_index', 'end_index', 'word', 'total_native',\n",
    "                                            'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary',\n",
    "                                            'complex_probabilistic'), encoding='utf-8-sig')\n",
    "data_frame = data_frame.astype(str)\n",
    "\n",
    "# Cleaning function for words\n",
    "\n",
    "data_frame['sentence'] = data_frame['sentence'].apply(lambda x: x.replace(\"%\", \"percent\"))\n",
    "data_frame['sentence'] = data_frame['sentence'].apply(lambda x: x.replace(\"’\", \"'\"))\n",
    "\n",
    "\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"-\", \"\")\n",
    "remove = remove.replace(\"(\", \"\")\n",
    "remove = remove.replace(\",\", \"\")\n",
    "remove = remove.replace(\"'\", \"\")# don't remove apostrophies \n",
    "remove = remove + '“'\n",
    "remove = remove +'”'\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "\n",
    "\n",
    "# Split the words and add them to the 'split' column, treating hyphens as separate words\n",
    "data_frame['split'] = data_frame['word'].apply(lambda x: [word for word in x.replace('-', ' - ').split()])\n",
    "\n",
    "\n",
    "# Split the words and add them to the 'split' column, treating hyphens as separate words\n",
    "data_frame['split'] = data_frame['word'].apply(lambda x: [word for word in x.replace('-', ' - ').split()])\n",
    "print(\"Cleaned DataFrame:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1524c-e5ef-40a8-8ae1-3fe6868859ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_dataframe_to_csv(data_frame, file_path):\n",
    "#     # Save the DataFrame to a CSV file\n",
    "#     data_frame.to_csv(file_path, index=False)\n",
    "\n",
    "# # Assuming you have a DataFrame called \"df\"\n",
    "# save_dataframe_to_csv(data_frame, 'output2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab215c3-d8d5-4ee5-a337-8b6e1b5c0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New cleaning\n",
    "\n",
    "# def clean_word(word):\n",
    "#     if word.startswith('('):\n",
    "#         word = word[1:]  # Remove the opening parenthesis\n",
    "#     remove = string.punctuation.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "#     remove = remove.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "#     pattern = r\"[{}]\".format(remove)\n",
    "#     return ''.join(char for char in word if char not in remove)\n",
    "\n",
    "# # Split the words and add them to the 'split' column, treating hyphens as separate words\n",
    "# data_frame['split'] = data_frame['word'].apply(lambda x: [clean_word(word) for word in x.replace('-', ' - ').split()])\n",
    "# print(\"Cleaned DataFrame:\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc7bf2-b91b-4cff-a35f-5e7a8c62ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2 This works and return word - ) to split column\n",
    "\n",
    "# def clean_word(word):\n",
    "#     if word.startswith('('):\n",
    "#         word = word[1:]  # Remove the opening parenthesis\n",
    "#     remove = string.punctuation.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "#     remove = remove.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "#     pattern = r\"[{}]\".format(remove)\n",
    "#     cleaned_word = ''.join(char for char in word if char not in remove)\n",
    "#     return cleaned_word if cleaned_word != \"\" else None\n",
    "\n",
    "# # Split the words and add them to the 'split' column, treating hyphens as separate words\n",
    "# data_frame['split'] = data_frame['word'].apply(lambda x: [clean_word(word) for word in x.replace('-', ' - ').split() if clean_word(word)])\n",
    "# print(\"Cleaned DataFrame:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9879040-ffe6-47be-9a6d-24a374d77972",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#2 Get Datamuse data\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache\n",
    "\n",
    "# Function to get total syllables using Datamuse API\n",
    "@lru_cache(maxsize=None)\n",
    "def get_total_syllables(word):\n",
    "    base_url = 'https://api.datamuse.com/words'\n",
    "    params = {\n",
    "        'sp': word,\n",
    "        'max': 1,\n",
    "        'md': 'psf'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if data and isinstance(data, list):\n",
    "        word_data = data[0]\n",
    "        if 'word' in word_data and 'numSyllables' in word_data:\n",
    "            syllables = int(word_data['numSyllables'])\n",
    "            return syllables\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Function to process the DataFrame and print results\n",
    "def process_dataframe(data_frame):\n",
    "    data_frame['syllables'] = data_frame['word'].apply(lambda x: sum(get_total_syllables(word) for word in x))\n",
    "    data_frame['vowels'] = data_frame['word'].apply(lambda x: sum(1 for word in x for char in word if char.lower() in \"aeiou\"))\n",
    "    data_frame['characters'] = data_frame['word'].apply(lambda x: sum(len(word) for word in x))\n",
    "\n",
    "    # Print values for each word\n",
    "    for index, row in data_frame.iterrows():\n",
    "        print(f\"Word: {row['word']}, Syllables: {row['syllables']}, Vowels: {row['vowels']}, Characters: {row['characters']}\")\n",
    "\n",
    "    # Print the updated DataFrame\n",
    "    print(\"Updated DataFrame:\")\n",
    "    print(data_frame)\n",
    "\n",
    "\n",
    "\n",
    "# Call the function to process the DataFrame\n",
    "process_dataframe(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5d8ec-328f-4d94-9673-aa7f48ea6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 3 New faster code for Simple WIki data\n",
    "# \"\"\"\n",
    "# In this updated version, the occurrence counts of unigrams, bigrams, trigrams, fourgrams, and fivegrams are calculated \n",
    "# separately and then combined into a single column called simple_wiki_freq. The counts are obtained by summing the \n",
    "# occurrence counts of each ngram type using the apply method with lambda functions.\n",
    "# \"\"\"\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Function to count ngram occurrences in a file\n",
    "def search_ngrams(file_path, ngrams):\n",
    "    # Create a Counter object to store ngram occurrences\n",
    "    ngram_occurrences = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Convert to lowercase and split the line into words\n",
    "            words = nltk.word_tokenize(line.lower())\n",
    "            # Generate ngrams\n",
    "            line_ngrams = list(nltk.ngrams(words, len(ngrams[0])))\n",
    "            # Count ngram occurrences\n",
    "            ngram_occurrences.update(line_ngrams)\n",
    "\n",
    "    return ngram_occurrences\n",
    "\n",
    "\n",
    "# Create a list of ngrams from the words in the word column\n",
    "word_column_words = data_frame['word'].str.lower().str.split()\n",
    "unigrams = [word for words in word_column_words for word in words]\n",
    "bigrams = list(nltk.ngrams(word_column_words, 2))\n",
    "trigrams = list(nltk.ngrams(word_column_words, 3))\n",
    "fourgrams = list(nltk.ngrams(word_column_words, 4))\n",
    "fivegrams = list(nltk.ngrams(word_column_words, 5))\n",
    "\n",
    "# Count the occurrences of unigrams, bigrams, trigrams, fourgrams, and fivegrams in the Simple Wiki corpus\n",
    "file_path = \"corpus/simple_wiki.txt\"\n",
    "unigram_occurrences = search_ngrams(file_path, unigrams)\n",
    "bigram_occurrences = search_ngrams(file_path, bigrams)\n",
    "trigram_occurrences = search_ngrams(file_path, trigrams)\n",
    "fourgram_occurrences = search_ngrams(file_path, fourgrams)\n",
    "fivegram_occurrences = search_ngrams(file_path, fivegrams)\n",
    "\n",
    "# Combine the occurrence counts of all ngrams into a single column\n",
    "data_frame['simple_wiki_freq'] = (\n",
    "    data_frame['word'].apply(lambda x: sum(unigram_occurrences[word] for word in x.lower().split())) +\n",
    "    data_frame['word'].apply(lambda x: sum(bigram_occurrences[bigram] for bigram in nltk.ngrams(x.lower().split(), 2))) +\n",
    "    data_frame['word'].apply(lambda x: sum(trigram_occurrences[trigram] for trigram in nltk.ngrams(x.lower().split(), 3))) +\n",
    "    data_frame['word'].apply(lambda x: sum(fourgram_occurrences[fourgram] for fourgram in nltk.ngrams(x.lower().split(), 4))) +\n",
    "    data_frame['word'].apply(lambda x: sum(fivegram_occurrences[fivegram] for fivegram in nltk.ngrams(x.lower().split(), 5)))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507632a-5b24-4071-83fb-53101141c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#  4 New faster code to get HIT frquency with MWEs\n",
    "\n",
    "def HIT_freq(data_frame, words, ID):\n",
    "    words_lower = words.lower().split()\n",
    "    paragraph = data_frame[data_frame['ID'] == ID]\n",
    "    word_count = paragraph['word'].str.lower().value_counts().to_dict()\n",
    "    total_occurrences = paragraph['word'].str.lower().isin(words_lower).sum()\n",
    "    return total_occurrences\n",
    "\n",
    "word_counts = data_frame['word'].str.lower().value_counts().to_dict()\n",
    "data_frame['HIT_count'] = data_frame['ID'].map(data_frame.groupby('ID')['word'].apply(lambda x: x.str.lower().isin(word_counts).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae0216-2ab4-4ee2-9092-e7e9dee2c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 5 - New faster code to use multiple requests for Ngram data\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_ngram_counts(data_frame):\n",
    "    abs_counts = []\n",
    "    rel_counts = []\n",
    "\n",
    "    session = requests.Session()  # Create a session object for reusing connections\n",
    "\n",
    "    def process_word(word):\n",
    "        url = 'https://api.ngrams.dev/eng/search'\n",
    "        params = {\n",
    "            'query': word,\n",
    "            'flags': 'cs',\n",
    "            'limit': 1\n",
    "        }\n",
    "\n",
    "        response = session.get(url, params=params, verify=False)  # Reuse the session for subsequent requests\n",
    "        data = response.json()\n",
    "\n",
    "        if 'ngrams' in data and len(data['ngrams']) > 0:\n",
    "            abs_count = data['ngrams'][0]['absTotalMatchCount']\n",
    "            rel_count = data['ngrams'][0]['relTotalMatchCount']\n",
    "        else:\n",
    "            abs_count = None\n",
    "            rel_count = None\n",
    "\n",
    "        return abs_count, rel_count\n",
    "\n",
    "    words = data_frame['word']\n",
    "    num_workers = min(len(words), 10)  # Adjust the number of workers as per your requirements\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(process_word, words)\n",
    "\n",
    "    for abs_count, rel_count in results:\n",
    "        abs_counts.append(abs_count)\n",
    "        rel_counts.append(rel_count)\n",
    "\n",
    "    data_frame['absTotalMatchCount'] = abs_counts\n",
    "    data_frame['relTotalMatchCount'] = rel_counts\n",
    "\n",
    "    return data_frame\n",
    "get_ngram_counts(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83095d55-6ecb-4907-8e52-00e39a6e3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a980773-32c0-41b7-9695-ac3ffb315274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE out for baseline - 6 Get Ngrams frequency\n",
    "\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# def add_ngram_frequencies(dataframe):\n",
    "#     ngram_files = {\n",
    "#         1: glob.glob(\"corpus/eng_Ngram_corpus/1grams/*.csv\"),\n",
    "#         2: glob.glob(\"corpus/eng_Ngram_corpus/2grams/*.csv\"),\n",
    "#         3: glob.glob(\"corpus/eng_Ngram_corpus/3grams/*.csv\"),\n",
    "#         4: glob.glob(\"corpus/eng_Ngram_corpus/4grams/*.csv\"),\n",
    "#         5: glob.glob(\"corpus/eng_Ngram_corpus/5grams/*.csv\")\n",
    "#     }\n",
    "    \n",
    "#     for n in range(1, 6):\n",
    "#         for file_path in ngram_files[n]:\n",
    "#             ngram_data = pd.read_csv(file_path)\n",
    "#             freq_column_name = f\"{n}gram_freq\"\n",
    "            \n",
    "#             # Extract the filename (ngram name) from the file path\n",
    "#             ngram_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "            \n",
    "#             # Iterate over the \"word\" column\n",
    "#             for index, row in dataframe.iterrows():\n",
    "#                 word = row['word']\n",
    "                \n",
    "#                 # Look up the frequency in the ngram data\n",
    "#                 matching_row = ngram_data[ngram_data['ngram'] == word]\n",
    "#                 if not matching_row.empty:\n",
    "#                     frequency = matching_row.iloc[0]['freq']\n",
    "#                     dataframe.at[index, freq_column_name] = frequency\n",
    "    \n",
    "#     return dataframe\n",
    "\n",
    "# add_ngram_frequencies(data_frame)\n",
    "# print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568090c5-8a87-4522-b0a5-2898888d5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 6 Parse sentences\n",
    "#Start core from command line\n",
    "\n",
    "print(\"start core\")\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "sentences = data_frame[['sentence', 'ID']].copy()\n",
    "\n",
    "sentences = sentences.drop_duplicates()\n",
    "\n",
    "print(\"end core\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b3875-d770-4e2b-949b-de4d466cf63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 7 \n",
    "\n",
    "print(\"start parse sentence\")\n",
    "\n",
    "def parse(string):\n",
    "    output = nlp.annotate(string, properties={\n",
    "        'annotators': 'pos,depparse,ner',\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "    output_dict = json.loads(output)\n",
    "    return output_dict\n",
    "    # return output\n",
    "\n",
    "print(\"finish parsing sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d9c99-30ee-4b1c-9640-28463d37a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 8 Function to parse a sentence and return the parsed output as a string\n",
    "def parse_sentence(sentence):\n",
    "    parsed_output = parse(sentence)\n",
    "    return json.dumps(parsed_output)\n",
    "\n",
    "# Apply the parse_sentence function to the 'sentence' column and store the parsed output in a new 'Parse' column\n",
    "data_frame['Parse'] = data_frame['sentence'].apply(parse_sentence)\n",
    "\n",
    "# Print the updated dataframe\n",
    "data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7d6b1-d4ee-4d3b-a432-f5f4cf49811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 9 - Get dependencies\n",
    "import json\n",
    "\n",
    "def get_dep(row):\n",
    "    number = 0\n",
    "    word = row['word']\n",
    "    parse = json.loads(row['Parse'])\n",
    "\n",
    "    for dependency in parse['sentences'][0]['basicDependencies']:\n",
    "        comp_word = dependency['governorGloss']\n",
    "        comp_word = comp_word.lower()\n",
    "        comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "\n",
    "        if comp_word == word:\n",
    "            number += 1\n",
    "\n",
    "    return number\n",
    "data_frame['dep'] = data_frame.apply(get_dep, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199fbd42-14bc-4c23-a9f9-948fed64fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 10 - Get Entity\n",
    "def get_ner(row):\n",
    "    word = row['word']\n",
    "    parse = json.loads(row['Parse'])\n",
    "\n",
    "    for token in parse['sentences'][0]['entitymentions']:\n",
    "        if token['text'] == word:\n",
    "            return token['ner']\n",
    "\n",
    "data_frame['ner'] = data_frame.apply(get_ner, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cdf97-50c7-47c1-a928-cd34f51f6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 11 - Get POS\n",
    "def get_pos(row):\n",
    "    word = row['word']\n",
    "    parse = row['Parse']\n",
    "    \n",
    "    parse_dict = json.loads(parse)\n",
    "    tokens = parse_dict['sentences'][0]['tokens']\n",
    "    \n",
    "    for token in tokens:\n",
    "        comp_word = token['word'].lower()\n",
    "        comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "        \n",
    "        if comp_word == word.lower():\n",
    "            return token['pos']\n",
    "    \n",
    "    return None\n",
    "data_frame['pos'] = data_frame.apply(get_pos, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06f804-fe9e-47c8-a308-a67cbcf88961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 - Function to get the proper lemma\n",
    "\n",
    "def get_lemma(row):\n",
    "    word = row['word']\n",
    "    parse = row['Parse']\n",
    "    \n",
    "    parse_dict = json.loads(parse)\n",
    "    tokens = parse_dict['sentences'][0]['tokens']\n",
    "    \n",
    "    for token in tokens:\n",
    "        comp_word = token['word'].lower()\n",
    "        comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "        \n",
    "        if comp_word == word.lower():\n",
    "            return token['lemma']\n",
    "    \n",
    "    return None\n",
    "data_frame['lemma'] = data_frame.apply(get_lemma, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffeda29-143b-47ab-85f7-858569605037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 - \n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Get the base filename from the location variable\n",
    "base_filename = os.path.splitext(os.path.basename(location))[0]\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = os.path.join('features_NEW', base_filename + '_NEW_Feats1.pkl')\n",
    "\n",
    "# Pickle the word_features DataFrame\n",
    "with open(output_file_path, 'wb') as file:\n",
    "    pickle.dump(data_frame, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218f4e9-6fe2-48b9-a1ce-56ed4ecd3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('output1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af088ee3-c5e1-4a72-be7c-4f9242141228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2f0ff-7fd9-4a26-8c03-41c517dfd856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150ba9a-1e3d-403b-b6cf-62323273f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"start tagging\")\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if pd.isna(treebank_tag):\n",
    "        return None\n",
    "\n",
    "    if isinstance(treebank_tag, str) and treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif isinstance(treebank_tag, str) and treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif isinstance(treebank_tag, str) and treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif isinstance(treebank_tag, str) and treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf94e3-4ef4-468b-b7a7-841085284533",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert tree bank tags to ones that are compatible w google\n",
    "\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wordnet.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wordnet.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wordnet.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wordnet.VERB\n",
    "    return None\n",
    "\n",
    "def penn_to_google(tag):\n",
    "    if is_adjective(tag):\n",
    "        return 'adj'\n",
    "    elif is_noun(tag):\n",
    "        return 'n'\n",
    "    elif is_adverb(tag):\n",
    "        return 'adv'\n",
    "    elif is_verb(tag):\n",
    "        return 'v'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ad1f3-e5f5-4041-93b7-b99aa45304a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(\"debugging/word_parse_features_bug.csv\" , index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654910a-af7d-493d-8de8-32f5a62733e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Get the base filename from the location variable\n",
    "base_filename = os.path.splitext(os.path.basename(location))[0]\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = os.path.join('features_NEW', base_filename + '_NEW_Feats.pkl')\n",
    "\n",
    "# Pickle the word_features DataFrame\n",
    "with open(output_file_path, 'wb') as file:\n",
    "    pickle.dump(data_frame, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
