{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5b6b0a-8df3-4429-9ff4-7c85fa60e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be442cd9-3e5c-4c29-8a7b-1c59e571c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.impute import SimpleImputer\n",
    "import os\n",
    "\n",
    "# Load the word features DataFrames\n",
    "wikipedia_word_features_file = 'features_NEW/Wikipedia_Train_NEW_Feats1.pkl'\n",
    "wikinews_word_features_file = 'features_NEW/WikiNews_Train_NEW_Feats1.pkl'\n",
    "news_word_features_file = 'features_NEW/News_Train_NEW_Feats1.pkl'\n",
    "\n",
    "wikipedia_word_features = pd.read_pickle(wikipedia_word_features_file)\n",
    "wikinews_word_features = pd.read_pickle(wikinews_word_features_file)\n",
    "news_word_features = pd.read_pickle(news_word_features_file)\n",
    "\n",
    "# Replace empty values with NaN\n",
    "wikipedia_word_features.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "wikinews_word_features.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_word_features.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "imputer = SimpleImputer(missing_values=float(\"NaN\"), strategy=\"constant\", fill_value=0)\n",
    "wikipedia_word_features = pd.DataFrame(imputer.fit_transform(wikipedia_word_features), columns=wikipedia_word_features.columns)\n",
    "wikinews_word_features = pd.DataFrame(imputer.fit_transform(wikinews_word_features), columns=wikinews_word_features.columns)\n",
    "news_word_features = pd.DataFrame(imputer.fit_transform(news_word_features), columns=news_word_features.columns)\n",
    "\n",
    "# Train the individual models\n",
    "wikipedia_classifier = NearestCentroid()\n",
    "wikipedia_classifier.fit(\n",
    "    wikipedia_word_features[['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', 'absTotalMatchCount', 'relTotalMatchCount']].values,\n",
    "    wikipedia_word_features['complex_binary'].values\n",
    ")\n",
    "save_model(wikipedia_classifier, 'lmodel', 'Wikipedia')\n",
    "\n",
    "wikinews_classifier = NearestCentroid()\n",
    "wikinews_classifier.fit(\n",
    "    wikinews_word_features[['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', 'absTotalMatchCount', 'relTotalMatchCount']].values,\n",
    "    wikinews_word_features['complex_binary'].values\n",
    ")\n",
    "save_model(wikinews_classifier, 'lmodel', 'WikiNews')\n",
    "\n",
    "news_classifier = NearestCentroid()\n",
    "news_classifier.fit(\n",
    "    news_word_features[['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', 'absTotalMatchCount', 'relTotalMatchCount']].values,\n",
    "    news_word_features['complex_binary'].values\n",
    ")\n",
    "save_model(news_classifier, 'lmodel', 'News')\n",
    "\n",
    "# Combine the models\n",
    "combined_classifier = NearestCentroid()\n",
    "combined_classifier.fit(\n",
    "    pd.concat([\n",
    "        wikipedia_word_features,\n",
    "        wikinews_word_features,\n",
    "        news_word_features\n",
    "    ])[['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', 'absTotalMatchCount', 'relTotalMatchCount']].values,\n",
    "    pd.concat([\n",
    "        wikipedia_word_features,\n",
    "        wikinews_word_features,\n",
    "        news_word_features\n",
    "    ])['complex_binary'].values\n",
    ")\n",
    "save_model(combined_classifier, 'lmodel', 'Combined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377238c-d729-4608-ba1c-a0d3172377e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DO NOT ALTER\n",
    "\n",
    "# def train_classifier(word_features_file):\n",
    "#     # Load the word_features DataFrame from the pickle file\n",
    "#     with open(word_features_file, 'rb') as file:\n",
    "#         word_features = pickle.load(file)\n",
    "\n",
    "#     # Extract the features and labels from the word_features DataFrame\n",
    "#     features_columns = ['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', 'absTotalMatchCount','relTotalMatchCount', '1gram_freq', '2gram_freq', '3gram_freq', '4gram_freq', '5gram_freq']\n",
    "#     X = word_features[features_columns].values\n",
    "#     y = word_features['complex_binary'].values\n",
    "\n",
    "#     # Create and train the Nearest Centroid classifier\n",
    "#     clf = NearestCentroid()\n",
    "#     clf.fit(X, y)\n",
    "\n",
    "#     return clf\n",
    "\n",
    "# def save_model(classifier, output_folder, word_features_file):\n",
    "#     # Create the output folder if it doesn't exist\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     # Get the base filename from the word_features_file\n",
    "#     base_filename = os.path.splitext(os.path.basename(word_features_file))[0]\n",
    "    \n",
    "#     # Remove \"Feats\" from the base filename, if present\n",
    "#     base_filename = base_filename.replace(\"Feats\", \"\")\n",
    "\n",
    "#     # Save the classifier model to a file\n",
    "#     output_file_path = os.path.join(output_folder, f'{base_filename}_model.pkl')\n",
    "#     with open(output_file_path, 'wb') as file:\n",
    "#         pickle.dump(classifier, file)\n",
    "\n",
    "# # Train the classifier\n",
    "# #word_features_file = 'features/Wikipedia_Train_Feats.pkl'  # Replace with the path to your pickled word_features file\n",
    "# word_features_file = 'features_NEW/Wikipedia_Train_NEW_Feats1.pkl'\n",
    "# classifier = train_classifier(word_features_file)\n",
    "\n",
    "# # Save the classifier model to the \"lmodel\" folder\n",
    "# output_folder = 'lmodel'\n",
    "# save_model(classifier, output_folder, word_features_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f9b025a-89af-4500-8824-13e6007d3363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- Classifier should now handle NaN values and change them to zero\n",
    "\n",
    "def train_classifier(word_features_file):\n",
    "    # Load the word_features DataFrame from the pickle file\n",
    "    with open(word_features_file, 'rb') as file:\n",
    "        word_features = pickle.load(file)\n",
    "\n",
    "    # Replace empty values with NaN\n",
    "    word_features.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    word_features.fillna(0, inplace=True)\n",
    "\n",
    "    # Extract the features and labels from the word_features DataFrame\n",
    "    features_columns = ['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', 'absTotalMatchCount','relTotalMatchCount', '1gram_freq', '2gram_freq', '3gram_freq', '4gram_freq','5gram_freq'] \n",
    "    X = word_features[features_columns].values\n",
    "    y = word_features['complex_binary'].values\n",
    "\n",
    "    # Create and train the Nearest Centroid classifier\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def save_model(classifier, output_folder, word_features_file):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get the base filename from the word_features_file\n",
    "    base_filename = os.path.splitext(os.path.basename(word_features_file))[0]\n",
    "\n",
    "    # Remove \"Feats\" from the base filename, if present\n",
    "    base_filename = base_filename.replace(\"Feats\", \"\")\n",
    "\n",
    "    # Save the classifier model to a file\n",
    "    output_file_path = os.path.join(output_folder, f'{base_filename}_model.pkl')\n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n",
    "\n",
    "# Train the classifier\n",
    "word_features_file = 'features_NEW/Wikipedia_Train_NEW_Feats1.pkl'\n",
    "classifier = train_classifier(word_features_file)\n",
    "\n",
    "# Save the classifier model to the \"lmodel\" folder\n",
    "output_folder = 'lmodel'\n",
    "save_model(classifier, output_folder, word_features_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf4addbb-f7c0-42bf-997b-3531db160b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the classifier to without the raw Ngram data\n",
    "\n",
    "\n",
    "# 1- Classifier should now handle NaN values and change them to zero\n",
    "\n",
    "def train_classifier(word_features_file):\n",
    "    # Load the word_features DataFrame from the pickle file\n",
    "    with open(word_features_file, 'rb') as file:\n",
    "        word_features = pickle.load(file)\n",
    "\n",
    "    # Replace empty values with NaN\n",
    "    word_features.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    word_features.fillna(0, inplace=True)\n",
    "\n",
    "    # Extract the features and labels from the word_features DataFrame\n",
    "    features_columns = ['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', 'absTotalMatchCount','relTotalMatchCount'] \n",
    "    X = word_features[features_columns].values\n",
    "    y = word_features['complex_binary'].values\n",
    "\n",
    "    # Create and train the Nearest Centroid classifier\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def save_model(classifier, output_folder, word_features_file):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get the base filename from the word_features_file\n",
    "    base_filename = os.path.splitext(os.path.basename(word_features_file))[0]\n",
    "\n",
    "    # Remove \"Feats\" from the base filename, if present\n",
    "    base_filename = base_filename.replace(\"Feats\", \"\")\n",
    "\n",
    "    # Save the classifier model to a file\n",
    "    output_file_path = os.path.join(output_folder, f'{base_filename}_model.pkl')\n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n",
    "\n",
    "# Train the classifier\n",
    "word_features_file = 'features_NEW/Wikipedia_Train_NEW_Feats1.pkl'\n",
    "classifier = train_classifier(word_features_file)\n",
    "\n",
    "# Save the classifier model to the \"lmodel\" folder\n",
    "output_folder = 'lmodel'\n",
    "save_model(classifier, output_folder, word_features_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00228d88-c521-456c-b3b3-a1d2a746651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New code for combined data do not change\n",
    "\n",
    "def train_classifier(word_features_file1, word_features_file2):\n",
    "    # Load the first word_features DataFrame from the pickle file\n",
    "    with open(word_features_file1, 'rb') as file:\n",
    "        word_features1 = pickle.load(file)\n",
    "\n",
    "    # Load the second word_features DataFrame from the pickle file\n",
    "    with open(word_features_file2, 'rb') as file:\n",
    "        word_features2 = pickle.load(file)\n",
    "\n",
    "    # Concatenate the two word_features DataFrames\n",
    "    word_features = pd.concat([word_features1, word_features2], ignore_index=True)\n",
    "\n",
    "    # Extract the features and labels from the combined word_features DataFrame\n",
    "    features_columns = ['syllables', 'length', 'vowels', 'simple_wiki_freq', 'HIT_freq', 'google frequency']\n",
    "    X = word_features[features_columns].values\n",
    "    y = word_features['complex_binary'].values\n",
    "\n",
    "    # Create and train the Nearest Centroid classifier\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def save_model(classifier, output_folder, model_name):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save the classifier model to a file\n",
    "    output_file_path = os.path.join(output_folder, f'{model_name}_model.pkl')\n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(classifier, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e3ee509-670b-4174-888d-05546e8ddb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_features_file1 = 'features/Wikipedia_Dev_allInfo_1.pkl'\n",
    "word_features_file2 = 'features/Wikipedia_Train_allInfo_1.pkl'\n",
    "classifier = train_classifier(word_features_file1, word_features_file2)\n",
    "\n",
    "\n",
    "# Save the classifier model to the \"lm\" folder\n",
    "output_folder = 'lm'\n",
    "model_name = 'Wikipedia_combined_Dev_Train_Fast'\n",
    "save_model(classifier, output_folder, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af747a2-7438-47e3-bffd-0737c9ceb45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in column 'syllables': 0\n",
      "NaN values in column 'characters': 0\n",
      "NaN values in column 'vowels': 0\n",
      "NaN values in column 'simple_wiki_freq': 0\n",
      "NaN values in column 'HIT_count': 0\n",
      "NaN values in column '1gram_freq': 113\n",
      "NaN values in column '2gram_freq': 666\n",
      "NaN values in column '3gram_freq': 691\n",
      "NaN values in column '4gram_freq': 692\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pickled DataFrame\n",
    "df = pd.read_pickle('features_NEW/Wikipedia_Dev_NEW_Feats1.pkl')\n",
    "\n",
    "# Define the columns to check for NaN values\n",
    "features_columns = ['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', '1gram_freq', '2gram_freq', '3gram_freq', '4gram_freq',] #'5gram_freq']\n",
    "\n",
    "\n",
    "# Check for NaN values in the specified columns\n",
    "nan_values = df[features_columns].isna().sum()\n",
    "\n",
    "# Print the number of NaN values for each column\n",
    "for column, count in nan_values.items():\n",
    "    print(f\"NaN values in column '{column}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c703a9c2-02d8-40c9-bbf4-3436ed10b4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero counts in column 'syllables': 78\n",
      "Zero counts in column 'characters': 0\n",
      "Zero counts in column 'vowels': 62\n",
      "Zero counts in column 'simple_wiki_freq': 5551\n",
      "Zero counts in column 'HIT_count': 6\n",
      "Zero counts in column '1gram_freq': 0\n",
      "Zero counts in column '2gram_freq': 0\n",
      "Zero counts in column '3gram_freq': 0\n",
      "Zero counts in column '4gram_freq': 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pickled DataFrame\n",
    "df = pd.read_pickle('features_NEW/Wikipedia_Train_NEW_Feats1.pkl')\n",
    "\n",
    "# Define the columns to check for zero values\n",
    "features_columns = ['syllables', 'characters', 'vowels', 'simple_wiki_freq', 'HIT_count', '1gram_freq', '2gram_freq', '3gram_freq', '4gram_freq']\n",
    "\n",
    "# Check for zero values in the specified columns\n",
    "zero_counts = (df[features_columns] == 0).sum()\n",
    "\n",
    "# Print the number of zero values for each column\n",
    "for column, count in zero_counts.items():\n",
    "    print(f\"Zero counts in column '{column}': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f5b57d5-aaa0-47da-afcf-5e0129814367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  ID  \\\n",
      "777   34D9ZRXCYRVYV0Y20MTM8EXYR5MASA   \n",
      "1002  3UQ1LLR26A9BRN3CGDWLWSJ3ZBEALW   \n",
      "2716  3D06DR5225KVX5LXGPX0W5YSZG4AMY   \n",
      "2823  3P458N04Q1IHMEPXHH6U14VSUWW2X3   \n",
      "4212  3O2Y2UIUCQVV38226T6CVTHF7RMKFG   \n",
      "4842  3YZ7A3YHR5U1PUML5Q2503HCZRLS5M   \n",
      "\n",
      "                                               sentence start_index end_index  \\\n",
      "777   Two also has the unique property such that and...          33        42   \n",
      "1002  Aemilian was killed by them at Spoletium or th...         100       108   \n",
      "2716  It is also well known for being the seat of im...          11        21   \n",
      "2823  As of the census of 2000 , there were 24,276 p...           0         5   \n",
      "4212  Koca Mi 'm âr Sinân Âğâ ( Ottoman Turkish : مع...          44        55   \n",
      "4842  Zhang et al. suggest that unless Epidexipteryx...           6        11   \n",
      "\n",
      "             word total_native total_non_native native_complex  \\\n",
      "777     such that           10               10              1   \n",
      "1002     half way           10               10              0   \n",
      "2716   well known           10               10              0   \n",
      "2823        As of           10               10              0   \n",
      "4212  معمار سينان           10               10              0   \n",
      "4842        et al           10               10              0   \n",
      "\n",
      "     non_native_complex complex_binary  ... 1gram_freq   2gram_freq  \\\n",
      "777                   0              1  ...        NaN    6433505.0   \n",
      "1002                  0              0  ...        NaN     295063.0   \n",
      "2716                  1              1  ...        NaN    2911639.0   \n",
      "2823                  1              1  ...        NaN     965993.0   \n",
      "4212                  0              0  ...        NaN          NaN   \n",
      "4842                  3              1  ...        NaN  102681988.0   \n",
      "\n",
      "      3gram_freq  4gram_freq  5gram_freq  \\\n",
      "777          NaN         NaN         NaN   \n",
      "1002         NaN         NaN         NaN   \n",
      "2716         NaN         NaN         NaN   \n",
      "2823         NaN         NaN         NaN   \n",
      "4212         NaN         NaN         NaN   \n",
      "4842         NaN         NaN         NaN   \n",
      "\n",
      "                                                  Parse  dep   ner   pos  \\\n",
      "777   {\"sentences\": [{\"index\": 0, \"basicDependencies...    0  None  None   \n",
      "1002  {\"sentences\": [{\"index\": 0, \"basicDependencies...    0  None  None   \n",
      "2716  {\"sentences\": [{\"index\": 0, \"basicDependencies...    0  None  None   \n",
      "2823  {\"sentences\": [{\"index\": 0, \"basicDependencies...    0  None  None   \n",
      "4212  {\"sentences\": [{\"index\": 0, \"basicDependencies...    0  None  None   \n",
      "4842  {\"sentences\": [{\"index\": 0, \"basicDependencies...    0  None  None   \n",
      "\n",
      "      lemma  \n",
      "777    None  \n",
      "1002   None  \n",
      "2716   None  \n",
      "2823   None  \n",
      "4212   None  \n",
      "4842   None  \n",
      "\n",
      "[6 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pickled DataFrame\n",
    "df = pd.read_pickle('features_NEW/Wikipedia_Train_NEW_Feats1.pkl')\n",
    "\n",
    "# Find the rows with zero value in \"HIT_count\" column\n",
    "zero_hit_count_rows = df[df['HIT_count'] == 0]\n",
    "\n",
    "# Print the rows with zero \"HIT_count\"\n",
    "print(zero_hit_count_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d6b8948-e771-428a-b88f-3f762a97673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN values:\n",
      "                                  ID  \\\n",
      "531   3OEWW2KGQJCHVF9LDF3FEAV3EC8OD6   \n",
      "1282  3WYZV0QBFJEBARPT0AZ52XYWNV1BXR   \n",
      "2371  3LCXHSGDLT71LDFEGRV84XNPBDCESM   \n",
      "4683  3WPCIUYH1A9X87ET9WPE8K1QDKOTDW   \n",
      "4720  3DIIW4IV8PWUZXFPM9PHR95BPQG4IE   \n",
      "\n",
      "                                               sentence start_index end_index  \\\n",
      "531   #42-110 I still have some questions left unans...          60        64   \n",
      "1282  #3-7 Anastasia Slonina, an actress working in ...          73        82   \n",
      "2371  #42-103 Dalial Freitak, who is also on this pa...          82        86   \n",
      "4683  #30-2 A woman in Spain has been fined €800 aft...         143       163   \n",
      "4720  #39-1 Austrian police find dozens dead inside ...         140       151   \n",
      "\n",
      "                      word total_native total_non_native native_complex  \\\n",
      "531                   ph.d           10               10              1   \n",
      "1282             teatr.doc           10               10              0   \n",
      "2371                  ph.d           10               10              1   \n",
      "4683  disabled/handicapped           10               10              4   \n",
      "4720           austria.svg           10               10              1   \n",
      "\n",
      "     non_native_complex complex_binary  ...                   split count  \\\n",
      "531                   0              1  ...                  [Ph.D]     1   \n",
      "1282                  1              1  ...             [Teatr.doc]     1   \n",
      "2371                  1              1  ...                  [Ph.D]     1   \n",
      "4683                  0              1  ...  [disabled/handicapped]     1   \n",
      "4720                  0              1  ...           [Austria.svg]     1   \n",
      "\n",
      "             original word syllables  length  vowels  consonants  \\\n",
      "531                   Ph.D       NaN     NaN     NaN         NaN   \n",
      "1282             Teatr.doc       NaN     NaN     NaN         NaN   \n",
      "2371                  Ph.D       NaN     NaN     NaN         NaN   \n",
      "4683  disabled/handicapped       NaN     NaN     NaN         NaN   \n",
      "4720           Austria.svg       NaN     NaN     NaN         NaN   \n",
      "\n",
      "      simple_wiki_freq  HIT_count  google frequency  \n",
      "531                  0          1               0.0  \n",
      "1282                 0          1               0.0  \n",
      "2371                 0          1               0.0  \n",
      "4683                 0          1               0.0  \n",
      "4720                 0          1               0.0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pickled DataFrame\n",
    "df = pd.read_pickle('features/WikiNews_Train_Feats.pkl')\n",
    "\n",
    "# Define the columns to check for NaN values\n",
    "features_columns = ['syllables', 'length', 'vowels', 'simple_wiki_freq', 'HIT_count', 'google frequency']\n",
    "\n",
    "# Check for NaN values in the specified columns\n",
    "nan_rows = df[df[features_columns].isna().any(axis=1)]\n",
    "\n",
    "# Print the rows where NaN values occur\n",
    "print(\"Rows with NaN values:\")\n",
    "print(nan_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619fadca-bde4-4885-b5b8-1967ee5854e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion for final baseline model\n",
    "\n",
    "def train_classifier(word_features_file1, word_features_file2, word_features_file3, word_features_file4, word_features_file5, word_features_file6):\n",
    "    # Load the word_features DataFrames from the pickle files\n",
    "    with open(word_features_file1, 'rb') as file:\n",
    "        word_features1 = pickle.load(file)\n",
    "    with open(word_features_file2, 'rb') as file:\n",
    "        word_features2 = pickle.load(file)\n",
    "    with open(word_features_file3, 'rb') as file:\n",
    "        word_features3 = pickle.load(file)\n",
    "    with open(word_features_file4, 'rb') as file:\n",
    "        word_features4 = pickle.load(file)\n",
    "    with open(word_features_file5, 'rb') as file:\n",
    "        word_features5 = pickle.load(file)\n",
    "    with open(word_features_file6, 'rb') as file:\n",
    "        word_features6 = pickle.load(file)\n",
    "\n",
    "    # Concatenate the word_features DataFrames\n",
    "    word_features = pd.concat([word_features1, word_features2, word_features3, word_features4, word_features5, word_features6], ignore_index=True)\n",
    "\n",
    "    # Extract the features and labels from the combined word_features DataFrame\n",
    "    features_columns = ['syllables', 'length', 'vowels', 'simple_wiki_freq', 'HIT_freq', 'google frequency']\n",
    "    X = word_features[features_columns].values\n",
    "    y = word_features['complex_binary'].values\n",
    "\n",
    "   # Create and train the Nearest Centroid classifier\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def save_model(model, output_folder, model_name):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save the model to a file\n",
    "    output_file_path = os.path.join(output_folder, f'{model_name}_model.pkl')\n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009f7a80-44e5-4643-9fa2-0ef5a8458ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier and save the model\n",
    "word_features_file1 = 'features/WikiNews_Train_allInfo_1.pkl'\n",
    "word_features_file2 = 'features/WikiNews_Dev_allInfo_1.pkl'\n",
    "word_features_file3 = 'features/News_Train_allInfo_1.pkl'\n",
    "word_features_file4 = 'features/News_Dev_allInfo_1.pkl'\n",
    "word_features_file5 = 'features/WikiNews_Train_allInfo_1.pkl'\n",
    "word_features_file6 = 'features/WikiNews_Dev_allInfo_1.pkl'\n",
    "model = train_classifier(word_features_file1, word_features_file2, word_features_file3, word_features_file4, word_features_file5, word_features_file6)\n",
    "output_folder = 'lm'\n",
    "model_name = 'Baseline_Binary_allInfo'\n",
    "save_model(model, output_folder, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88601bbe-c80e-489e-bb2b-abf96fc0e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model with all features\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "def train_classifier(word_features_file1, word_features_file2, word_features_file3, word_features_file4, word_features_file5, word_features_file6):\n",
    "    # Load the word_features DataFrames from the pickle files\n",
    "    with open(word_features_file1, 'rb') as file:\n",
    "        word_features1 = pickle.load(file)\n",
    "    with open(word_features_file2, 'rb') as file:\n",
    "        word_features2 = pickle.load(file)\n",
    "    with open(word_features_file3, 'rb') as file:\n",
    "        word_features3 = pickle.load(file)\n",
    "    with open(word_features_file4, 'rb') as file:\n",
    "        word_features4 = pickle.load(file)\n",
    "    with open(word_features_file5, 'rb') as file:\n",
    "        word_features5 = pickle.load(file)\n",
    "    with open(word_features_file6, 'rb') as file:\n",
    "        word_features6 = pickle.load(file)\n",
    "\n",
    "    # Concatenate the word_features DataFrames\n",
    "    word_features = pd.concat([word_features1, word_features2, word_features3, word_features4, word_features5, word_features6], ignore_index=True)\n",
    "\n",
    "    # Extract the features and labels from the combined word_features DataFrame\n",
    "    features_columns = ['syllables', 'length', 'vowels', 'pos', 'dep num', 'lemma', 'synonyms', 'hypernyms', 'hyponyms', 'wikipedia_freq', 'subtitles_freq', 'learner_corpus_freq', 'complex_lexicon', 'bnc_freq', 'ogden', 'simple_wiki', 'cald', 'sub_imdb', 'cnc', 'img', 'aoa', 'fam', 'google frequency', 'KFCAT', 'KFSMP', 'KFFRQ', 'NPHN', 'TLFRQ', 'holonyms', 'meronyms', 'consonants', 'learners_bigrams', 'simple_wiki_bigrams', 'ner', 'google_char_bigram', 'google_char_trigram', 'simple_wiki_fourgram', 'learner_fourgram', 'simple_wiki_freq', 'HIT_freq']\n",
    "    X = word_features[features_columns].values\n",
    "    y = word_features['complex_binary'].values\n",
    "\n",
    "    # Create and train the Nearest Centroid classifier\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def save_model(model, output_folder, model_name):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save the model to a file\n",
    "    output_file_path = os.path.join(output_folder, f'{model_name}_model.pkl')\n",
    "    with open(output_file_path, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a4d1123-f0a9-4851-804c-a52ce2b45e74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mword_features\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_features' is not defined"
     ]
    }
   ],
   "source": [
    "word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af880c-54aa-4e34-86cc-b34fae5ecbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
