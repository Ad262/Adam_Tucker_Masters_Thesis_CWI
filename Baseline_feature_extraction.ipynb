{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fc68d0-e579-4685-8225-317d2897aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from datamuse import datamuse\n",
    "from pycorenlp import StanfordCoreNLP \n",
    "import json\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c29bb-80da-4980-a691-580b0ede247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts the features required for the CWI basline system as described in CWI3G3G2 in (Yimam 2017), with only the most basic frequency features.\n",
    "1) Number of vowels\n",
    "2)Number of syllables\n",
    "3) Number of characters\n",
    "\n",
    "4) Frewuency in simple Wiki\n",
    "5)Frquency of word in HIT paragraph\n",
    "6) Frequency of word in Ngram corpus\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb599b0-8a5f-4eb4-b309-3d2ccc3cbde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New preprocessing to handle hyphenated and MWEs\n",
    "# Now hyphens are treated a seperate words and added to the split column.\n",
    "\n",
    "location = \"cwishareddataset/traindevset/english/Wikipedia_Toy.tsv\"\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "data_frame = pd.read_table(location, names=('ID', 'sentence', 'start_index', 'end_index', 'word', 'total_native',\n",
    "                                            'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary',\n",
    "                                            'complex_probabilistic'), encoding='utf-8-sig')\n",
    "data_frame = data_frame.astype(str)\n",
    "\n",
    "# Cleaning function for words\n",
    "\n",
    "data_frame['sentence'] = data_frame['sentence'].apply(lambda x: x.replace(\"%\", \"percent\"))\n",
    "data_frame['sentence'] = data_frame['sentence'].apply(lambda x: x.replace(\"’\", \"'\"))\n",
    "\n",
    "\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"'\", \"\") + '“”()'\n",
    "remove = remove.replace(\"'\", \"\")# don't remove apostrophies \n",
    "remove = remove + '“'\n",
    "remove = remove +'”'\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "word_set['word'] = word_set['word'].apply(lambda x :x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "\n",
    "# Split the words and add them to the 'split' column, treating hyphens as separate words\n",
    "data_frame['split'] = data_frame['word'].apply(lambda x: [word for word in x.replace('-', ' - ').split()])\n",
    "\n",
    "print(\"Cleaned DataFrame:\")\n",
    "data_frame\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c15882c0-3d60-4e28-b2c8-6fec76a25b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame:\n"
     ]
    }
   ],
   "source": [
    "# 1- New preprocessing to handle hyphenated and MWEs\n",
    "# Now hyphens are treated a seperate words and added to the split column.\n",
    "\n",
    "location = \"cwishareddataset/traindevset/english/News_Train.tsv\"\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "data_frame = pd.read_table(location, names=('ID', 'sentence', 'start_index', 'end_index', 'word', 'total_native',\n",
    "                                            'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary',\n",
    "                                            'complex_probabilistic'), encoding='utf-8-sig')\n",
    "data_frame = data_frame.astype(str)\n",
    "\n",
    "# Cleaning function for words\n",
    "\n",
    "data_frame['sentence'] = data_frame['sentence'].apply(lambda x: x.replace(\"%\", \"percent\"))\n",
    "data_frame['sentence'] = data_frame['sentence'].apply(lambda x: x.replace(\"’\", \"'\"))\n",
    "\n",
    "\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\"-\", \"\")\n",
    "remove = remove.replace(\"(\", \"\")\n",
    "remove = remove.replace(\",\", \"\")\n",
    "remove = remove.replace(\"'\", \"\")# don't remove apostrophies \n",
    "remove = remove + '“'\n",
    "remove = remove +'”'\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "\n",
    "\n",
    "# Split the words and add them to the 'split' column, treating hyphens as separate words\n",
    "data_frame['split'] = data_frame['word'].apply(lambda x: [word for word in x.replace('-', ' - ').split()])\n",
    "\n",
    "\n",
    "# Split the words and add them to the 'split' column, treating hyphens as separate words\n",
    "data_frame['split'] = data_frame['word'].apply(lambda x: [word for word in x.replace('-', ' - ').split()])\n",
    "print(\"Cleaned DataFrame:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db7457-4d8b-42b1-94b6-5936e98898c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 speeds up api\n",
    "\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_ngram_counts(data_frame):\n",
    "    abs_counts = []\n",
    "    rel_counts = []\n",
    "\n",
    "    session = requests.Session()  # Create a session object for reusing connections\n",
    "\n",
    "    def process_word(word):\n",
    "        url = 'https://api.ngrams.dev/eng/search'\n",
    "        params = {\n",
    "            'query': word,\n",
    "            'flags': 'cs',\n",
    "            'limit': 1\n",
    "        }\n",
    "\n",
    "        response = session.get(url, params=params, verify=False)  # Reuse the session for subsequent requests\n",
    "        data = response.json()\n",
    "\n",
    "        if 'ngrams' in data and len(data['ngrams']) > 0:\n",
    "            abs_count = data['ngrams'][0]['absTotalMatchCount']\n",
    "            rel_count = data['ngrams'][0]['relTotalMatchCount']\n",
    "        else:\n",
    "            abs_count = None\n",
    "            rel_count = None\n",
    "\n",
    "        return abs_count, rel_count\n",
    "\n",
    "    words = data_frame['word']\n",
    "    num_workers = min(len(words), 10)  # Adjust the number of workers as per your requirements\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(process_word, words)\n",
    "\n",
    "    for abs_count, rel_count in results:\n",
    "        abs_counts.append(abs_count)\n",
    "        rel_counts.append(rel_count)\n",
    "\n",
    "    data_frame['absTotalMatchCount'] = abs_counts\n",
    "    data_frame['relTotalMatchCount'] = rel_counts\n",
    "\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20beaa6-aed6-436e-89c6-85ed610f7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngram_counts(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb501f3-6dcd-4864-83b0-90634dd5f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('output_N.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9293f4af-e5fa-4b79-903b-8d597789fca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 46\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data_frame)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Call the function to process the DataFrame\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43mprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_frame\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 30\u001b[0m, in \u001b[0;36mprocess_dataframe\u001b[0;34m(data_frame)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_dataframe\u001b[39m(data_frame):\n\u001b[0;32m---> 30\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyllables\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_frame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mget_total_syllables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvowels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m word \u001b[38;5;28;01mif\u001b[39;00m char\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maeiou\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     32\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[35], line 30\u001b[0m, in \u001b[0;36mprocess_dataframe.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_dataframe\u001b[39m(data_frame):\n\u001b[0;32m---> 30\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyllables\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mget_total_syllables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvowels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m word \u001b[38;5;28;01mif\u001b[39;00m char\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maeiou\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     32\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x))\n",
      "Cell \u001b[0;32mIn[35], line 30\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_dataframe\u001b[39m(data_frame):\n\u001b[0;32m---> 30\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyllables\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(\u001b[43mget_total_syllables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x))\n\u001b[1;32m     31\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvowels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m word \u001b[38;5;28;01mif\u001b[39;00m char\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maeiou\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     32\u001b[0m     data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x))\n",
      "Cell \u001b[0;32mIn[35], line 20\u001b[0m, in \u001b[0;36mget_total_syllables\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     word_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m word_data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumSyllables\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m word_data:\n\u001b[1;32m     22\u001b[0m         syllables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(word_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumSyllables\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache\n",
    "\n",
    "# Function to get total syllables using Datamuse API\n",
    "@lru_cache(maxsize=None)\n",
    "def get_total_syllables(word):\n",
    "    base_url = 'https://api.datamuse.com/words'\n",
    "    params = {\n",
    "        'sp': word,\n",
    "        'max': 1,\n",
    "        'md': 'psf'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if len(data) > 0:\n",
    "        word_data = data[0]\n",
    "        if 'word' in word_data and 'numSyllables' in word_data:\n",
    "            syllables = int(word_data['numSyllables'])\n",
    "            return syllables\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Function to process the DataFrame and print results\n",
    "def process_dataframe(data_frame):\n",
    "    data_frame['syllables'] = data_frame['word'].apply(lambda x: sum(get_total_syllables(word) for word in x))\n",
    "    data_frame['vowels'] = data_frame['word'].apply(lambda x: sum(1 for word in x for char in word if char.lower() in \"aeiou\"))\n",
    "    data_frame['characters'] = data_frame['word'].apply(lambda x: sum(len(word) for word in x))\n",
    "\n",
    "    # Print values for each word\n",
    "    for index, row in data_frame.iterrows():\n",
    "        print(f\"Word: {row['word']}, Syllables: {row['syllables']}, Vowels: {row['vowels']}, Characters: {row['characters']}\")\n",
    "\n",
    "    # Print the updated DataFrame\n",
    "    print(\"Updated DataFrame:\")\n",
    "    print(data_frame)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Call the function to process the DataFrame\n",
    "process_dataframe(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21f79e-9c10-474f-8da3-857edb64a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to process the DataFrame\n",
    "process_dataframe(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313d7b9-7fac-4a90-b57a-849c6f54681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "def search_ngrams(file_path, ngrams):\n",
    "    # Create a dictionary to store ngram occurrences\n",
    "    ngram_occurrences = defaultdict(int)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Convert to lowercase and split the line into words\n",
    "            words = nltk.word_tokenize(line.lower())\n",
    "            # Generate ngrams\n",
    "            line_ngrams = set(nltk.ngrams(words, len(next(iter(ngrams)))))\n",
    "            # Search for each ngram occurrence\n",
    "            for ngram in line_ngrams:\n",
    "                if ngram in ngrams:\n",
    "                    ngram_occurrences[ngram] += 1\n",
    "                \n",
    "    return ngram_occurrences\n",
    "\n",
    "# Create a list of ngrams from the words in the word column\n",
    "word_column_words = list(chain.from_iterable(data_frame['word'].str.lower().str.split()))\n",
    "unigrams = set(word for word in word_column_words)\n",
    "bigrams = set(nltk.ngrams(word_column_words, 2))\n",
    "trigrams = set(nltk.ngrams(word_column_words, 3))\n",
    "fourgrams = set(nltk.ngrams(word_column_words, 4))\n",
    "fivegrams = set(nltk.ngrams(word_column_words, 5))\n",
    "\n",
    "# Count the occurrences of unigrams, bigrams, trigrams, fourgrams, and fivegrams in the Simple Wiki corpus\n",
    "file_path = \"corpus/simple_wiki.txt\"\n",
    "unigram_occurrences = search_ngrams(file_path, unigrams)\n",
    "bigram_occurrences = search_ngrams(file_path, bigrams)\n",
    "trigram_occurrences = search_ngrams(file_path, trigrams)\n",
    "fourgram_occurrences = search_ngrams(file_path, fourgrams)\n",
    "fivegram_occurrences = search_ngrams(file_path, fivegrams)\n",
    "\n",
    "# Combine the occurrence counts of all ngrams into a single column\n",
    "data_frame['simple_wiki_freq'] = (\n",
    "    data_frame['word'].apply(lambda x: sum(unigram_occurrences[word] for word in x.lower().split() if word in unigram_occurrences)) +\n",
    "    data_frame['word'].apply(lambda x: sum(bigram_occurrences[bigram] for bigram in nltk.ngrams(x.lower().split(), 2) if bigram in bigram_occurrences)) +\n",
    "    data_frame['word'].apply(lambda x: sum(trigram_occurrences[trigram] for trigram in nltk.ngrams(x.lower().split(), 3) if trigram in trigram_occurrences)) +\n",
    "    data_frame['word'].apply(lambda x: sum(fourgram_occurrences[fourgram] for fourgram in nltk.ngrams(x.lower().split(), 4) if fourgram in fourgram_occurrences)) +\n",
    "    data_frame['word'].apply(lambda x: sum(fivegram_occurrences[fivegram] for fivegram in nltk.ngrams(x.lower().split(), 5) if fivegram in fivegram_occurrences))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9110c9a3-caf6-4e63-ab21-30d1432cb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Function to count ngram occurrences in a file\n",
    "def search_ngrams(file_path, ngrams):\n",
    "    # Create a Counter object to store ngram occurrences\n",
    "    ngram_occurrences = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Convert to lowercase and split the line into words\n",
    "            words = nltk.word_tokenize(line.lower())\n",
    "            # Generate ngrams\n",
    "            line_ngrams = list(nltk.ngrams(words, len(ngrams[0])))\n",
    "            # Count ngram occurrences\n",
    "            ngram_occurrences.update(line_ngrams)\n",
    "\n",
    "    return ngram_occurrences\n",
    "\n",
    "\n",
    "# Create a list of ngrams from the words in the word column\n",
    "word_column_words = data_frame['word'].str.lower().str.split()\n",
    "unigrams = [word for words in word_column_words for word in words]\n",
    "bigrams = list(nltk.ngrams(word_column_words, 2))\n",
    "trigrams = list(nltk.ngrams(word_column_words, 3))\n",
    "fourgrams = list(nltk.ngrams(word_column_words, 4))\n",
    "fivegrams = list(nltk.ngrams(word_column_words, 5))\n",
    "\n",
    "# Count the occurrences of unigrams, bigrams, trigrams, fourgrams, and fivegrams in the Simple Wiki corpus\n",
    "file_path = \"corpus/simple_wiki.txt\"\n",
    "unigram_occurrences = search_ngrams(file_path, unigrams)\n",
    "bigram_occurrences = search_ngrams(file_path, bigrams)\n",
    "trigram_occurrences = search_ngrams(file_path, trigrams)\n",
    "fourgram_occurrences = search_ngrams(file_path, fourgrams)\n",
    "fivegram_occurrences = search_ngrams(file_path, fivegrams)\n",
    "\n",
    "# Combine the occurrence counts of all ngrams into a single column\n",
    "data_frame['simple_wiki_freq'] = (\n",
    "    data_frame['word'].apply(lambda x: sum(unigram_occurrences[word] for word in x.lower().split())) +\n",
    "    data_frame['word'].apply(lambda x: sum(bigram_occurrences[bigram] for bigram in nltk.ngrams(x.lower().split(), 2))) +\n",
    "    data_frame['word'].apply(lambda x: sum(trigram_occurrences[trigram] for trigram in nltk.ngrams(x.lower().split(), 3))) +\n",
    "    data_frame['word'].apply(lambda x: sum(fourgram_occurrences[fourgram] for fourgram in nltk.ngrams(x.lower().split(), 4))) +\n",
    "    data_frame['word'].apply(lambda x: sum(fivegram_occurrences[fivegram] for fivegram in nltk.ngrams(x.lower().split(), 5)))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0c7c91f-141e-4b63-a8cd-715862711539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_index</th>\n",
       "      <th>end_index</th>\n",
       "      <th>word</th>\n",
       "      <th>total_native</th>\n",
       "      <th>total_non_native</th>\n",
       "      <th>native_complex</th>\n",
       "      <th>non_native_complex</th>\n",
       "      <th>complex_binary</th>\n",
       "      <th>complex_probabilistic</th>\n",
       "      <th>split</th>\n",
       "      <th>simple_wiki_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3P7RGTLO6EE07HLUVDKKHS6O7CCKA5</td>\n",
       "      <td>The barren islands, reefs and coral outcrops a...</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>barren</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>[barren]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3P7RGTLO6EE07HLUVDKKHS6O7CCKA5</td>\n",
       "      <td>The barren islands, reefs and coral outcrops a...</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>barren islands</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[barren, islands]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3P7RGTLO6EE07HLUVDKKHS6O7CCKA5</td>\n",
       "      <td>The barren islands, reefs and coral outcrops a...</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>reefs</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[reefs]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3P7RGTLO6EE07HLUVDKKHS6O7CCKA5</td>\n",
       "      <td>The barren islands, reefs and coral outcrops a...</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>islands</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[islands]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3P7RGTLO6EE07HLUVDKKHS6O7CCKA5</td>\n",
       "      <td>The barren islands, reefs and coral outcrops a...</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>coral</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[coral]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>3BDORL6HKKEAN0VN5BP8OZIQ163CR1</td>\n",
       "      <td>Stephen Biddle, a defense analyst at the Counc...</td>\n",
       "      <td>185</td>\n",
       "      <td>201</td>\n",
       "      <td>security partner</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>[security, partner]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>3BDORL6HKKEAN0VN5BP8OZIQ163CR1</td>\n",
       "      <td>Stephen Biddle, a defense analyst at the Counc...</td>\n",
       "      <td>185</td>\n",
       "      <td>193</td>\n",
       "      <td>security</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[security]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>3BDORL6HKKEAN0VN5BP8OZIQ163CR1</td>\n",
       "      <td>Stephen Biddle, a defense analyst at the Counc...</td>\n",
       "      <td>194</td>\n",
       "      <td>201</td>\n",
       "      <td>partner</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[partner]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>3BDORL6HKKEAN0VN5BP8OZIQ163CR1</td>\n",
       "      <td>Stephen Biddle, a defense analyst at the Counc...</td>\n",
       "      <td>217</td>\n",
       "      <td>223</td>\n",
       "      <td>troops</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[troops]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14001</th>\n",
       "      <td>3BDORL6HKKEAN0VN5BP8OZIQ163CR1</td>\n",
       "      <td>Stephen Biddle, a defense analyst at the Counc...</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>home</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[home]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14002 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ID  \\\n",
       "0      3P7RGTLO6EE07HLUVDKKHS6O7CCKA5   \n",
       "1      3P7RGTLO6EE07HLUVDKKHS6O7CCKA5   \n",
       "2      3P7RGTLO6EE07HLUVDKKHS6O7CCKA5   \n",
       "3      3P7RGTLO6EE07HLUVDKKHS6O7CCKA5   \n",
       "4      3P7RGTLO6EE07HLUVDKKHS6O7CCKA5   \n",
       "...                               ...   \n",
       "13997  3BDORL6HKKEAN0VN5BP8OZIQ163CR1   \n",
       "13998  3BDORL6HKKEAN0VN5BP8OZIQ163CR1   \n",
       "13999  3BDORL6HKKEAN0VN5BP8OZIQ163CR1   \n",
       "14000  3BDORL6HKKEAN0VN5BP8OZIQ163CR1   \n",
       "14001  3BDORL6HKKEAN0VN5BP8OZIQ163CR1   \n",
       "\n",
       "                                                sentence start_index  \\\n",
       "0      The barren islands, reefs and coral outcrops a...           4   \n",
       "1      The barren islands, reefs and coral outcrops a...           4   \n",
       "2      The barren islands, reefs and coral outcrops a...          20   \n",
       "3      The barren islands, reefs and coral outcrops a...          11   \n",
       "4      The barren islands, reefs and coral outcrops a...          30   \n",
       "...                                                  ...         ...   \n",
       "13997  Stephen Biddle, a defense analyst at the Counc...         185   \n",
       "13998  Stephen Biddle, a defense analyst at the Counc...         185   \n",
       "13999  Stephen Biddle, a defense analyst at the Counc...         194   \n",
       "14000  Stephen Biddle, a defense analyst at the Counc...         217   \n",
       "14001  Stephen Biddle, a defense analyst at the Counc...         234   \n",
       "\n",
       "      end_index              word total_native total_non_native  \\\n",
       "0            10            barren           10               10   \n",
       "1            18    barren islands           10               10   \n",
       "2            25             reefs           10               10   \n",
       "3            18           islands           10               10   \n",
       "4            35             coral           10               10   \n",
       "...         ...               ...          ...              ...   \n",
       "13997       201  security partner           10               10   \n",
       "13998       193          security           10               10   \n",
       "13999       201           partner           10               10   \n",
       "14000       223            troops           10               10   \n",
       "14001       238              home           10               10   \n",
       "\n",
       "      native_complex non_native_complex complex_binary complex_probabilistic  \\\n",
       "0                  6                  2              1                   0.4   \n",
       "1                  0                  1              1                  0.05   \n",
       "2                  1                  2              1                  0.15   \n",
       "3                  0                  0              0                   0.0   \n",
       "4                  0                  0              0                   0.0   \n",
       "...              ...                ...            ...                   ...   \n",
       "13997              2                  0              1                   0.1   \n",
       "13998              0                  0              0                   0.0   \n",
       "13999              0                  0              0                   0.0   \n",
       "14000              0                  0              0                   0.0   \n",
       "14001              0                  0              0                   0.0   \n",
       "\n",
       "                     split  simple_wiki_freq  \n",
       "0                 [barren]                 0  \n",
       "1        [barren, islands]                 0  \n",
       "2                  [reefs]                 0  \n",
       "3                [islands]                 0  \n",
       "4                  [coral]                 0  \n",
       "...                    ...               ...  \n",
       "13997  [security, partner]                 0  \n",
       "13998           [security]                 0  \n",
       "13999            [partner]                 0  \n",
       "14000             [troops]                 0  \n",
       "14001               [home]                 0  \n",
       "\n",
       "[14002 rows x 13 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca9a7b-d541-4b95-b346-564f3352bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('output_D.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316f13b-76f9-4eee-adca-41be29cf9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to work with pickled data\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "data_frame = pd.read_pickle(location)\n",
    "data_frame.columns = ['ID', 'sentence', 'start_index', 'end_index', 'word', 'total_native',\n",
    "                      'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary',\n",
    "                      'complex_probabilistic']\n",
    "\n",
    "# Cleaning function for words\n",
    "remove = string.punctuation.replace(\"'\", \"\")  # Remove all punctuation except apostrophes\n",
    "remove = remove.replace(\"“\", \"\").replace(\"”\", \"\")  # Remove double quotation marks\n",
    "pattern = r\"[{}]\".format(remove)  # Create the pattern\n",
    "\n",
    "# Split the words and add them to the 'split' column, treating hyphens as separate words\n",
    "data_frame['split'] = data_frame['word'].apply(lambda x: [word for word in x.replace('-', ' - ').split()])\n",
    "\n",
    "print(\"Cleaned DataFrame:\")\n",
    "print(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bee56-e568-4cbb-bda9-8662edc30a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives number of vowels, syllables and characters for single word ONLY\n",
    "#Removes the rows with MWEs but does return data for the individual words that the MWE is made up of\n",
    "# 100,000 requests per day.\n",
    "#We create a table that contains only the words\n",
    "words = data_frame[data_frame['count'] == 1]\n",
    "\n",
    "word_set = words.word.str.lower().unique()\n",
    "\n",
    "word_set = pd.DataFrame(word_set)\n",
    "word_set.columns = ['word']\n",
    "\n",
    "api = datamuse.Datamuse()\n",
    "\n",
    "print(\"Getting syllables, length and vowels\")\n",
    "\n",
    "def get_syllables(word):\n",
    "    syllables = 0\n",
    "    word_results = api.words(sp=word, max=1, md='psf')\n",
    "    if len(word_results) > 0:\n",
    "        word = word_results[0][\"word\"]\n",
    "        syllables = int(word_results[0][\"numSyllables\"])\n",
    "    return syllables\n",
    "\n",
    "# Create a copy of the DataFrame with unique words\n",
    "unique_words = word_set['word'].unique()\n",
    "unique_word_set = pd.DataFrame(unique_words, columns=['word'])\n",
    "\n",
    "# Apply function to get syllables for unique words\n",
    "unique_word_set['syllables'] = unique_word_set['word'].apply(lambda x: get_syllables(x))\n",
    "\n",
    "# Apply function to get word length\n",
    "unique_word_set['length'] = unique_word_set['word'].apply(lambda x: len(x))\n",
    "\n",
    "# Apply function to get vowel count\n",
    "unique_word_set['vowels'] = unique_word_set['word'].apply(lambda x: sum([x.count(y) for y in \"aeiou\"]))\n",
    "\n",
    "# Apply function to get consonant count\n",
    "unique_word_set['consonants'] = unique_word_set['word'].apply(lambda x: sum([x.count(y) for y in \"bcdfghjklmnpqrstvwxyz\"]))\n",
    "\n",
    "# Merge unique_word_set DataFrame with word_features DataFrame\n",
    "words = words.copy()  # Create a copy of the DataFrame\n",
    "words.loc[:, 'original word'] = words['word']\n",
    "words.loc[:, 'word'] = words['word'].str.lower()\n",
    "word_features = pd.merge(words, unique_word_set, on='word', how='left')\n",
    "\n",
    "print(word_features)\n",
    "print('Finished getting syllables, length and vowels', \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa53db1-86a7-46c6-94af-b4e1c3b360a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives number of vowels, syllables and characters for all data\n",
    "\n",
    "#second attempt at getting syllables for hyphenated words\n",
    "\n",
    "def get_total_syllables(words):\n",
    "    api = datamuse.Datamuse()\n",
    "    total_syllables = 0\n",
    "    word_list = words.split()\n",
    "    for word in word_list:\n",
    "        if '-' in word:\n",
    "            hyphenated_words = word.split('-')\n",
    "            for hyphenated_word in hyphenated_words:\n",
    "                word_results = api.words(sp=hyphenated_word, max=1, md='psf')\n",
    "                if len(word_results) > 0:\n",
    "                    word = word_results[0][\"word\"]\n",
    "                    syllables = int(word_results[0][\"numSyllables\"])\n",
    "                    total_syllables += syllables\n",
    "        else:\n",
    "            word_results = api.words(sp=word, max=1, md='psf')\n",
    "            if len(word_results) > 0:\n",
    "                word = word_results[0][\"word\"]\n",
    "                syllables = int(word_results[0][\"numSyllables\"])\n",
    "                total_syllables += syllables\n",
    "    return total_syllables\n",
    "\n",
    "data_frame['syllables'] = data_frame['word'].apply(get_total_syllables)\n",
    "data_frame['vowels'] = data_frame['word'].apply(lambda x: sum(1 for char in x if char.lower() in \"aeiou\"))\n",
    "data_frame['characters'] = data_frame['word'].apply(len)\n",
    "\n",
    "# Print values for each word\n",
    "for index, row in data_frame.iterrows():\n",
    "    print(f\"Word: {row['word']}, Syllables: {row['syllables']}, Vowels: {row['vowels']}, Characters: {row['characters']}\")\n",
    "\n",
    "print(\"Updated DataFrame:\")\n",
    "print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2ec89-4c02-43c0-bee8-0e8622d1b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5e365-f41f-4d5d-aaf2-b970002cf8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word freq using ngram download from https://www.kaggle.com/datasets/wheelercode/english-word-frequency-list/code?resource=download\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_word_count(word, ngram_freq_file):\n",
    "    df = pd.read_csv(ngram_freq_file)\n",
    "    word_count = df.loc[df['word'] == word, 'count'].sum()\n",
    "    return word_count\n",
    "\n",
    "\n",
    "#ngram_freq_file = 'ngram_freq.csv'\n",
    "# Assuming 'data_frame' is your existing DataFrame with a 'word' column\n",
    "data_frame['ngram_freq'] = data_frame['word'].apply(lambda x: get_word_count(x, 'corpus/ngram_freq.csv') if len(x.split()) == 1 else None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8da0d1-3d4a-48ee-a6c7-9718f24fd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_length_distribution():\n",
    "    # Read the ngram_freq.csv file\n",
    "    df = pd.read_csv('corpus/ngram_freq.csv')\n",
    "\n",
    "    # Filter out float values in the 'word' column\n",
    "    df = df[df['word'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    # Calculate word lengths\n",
    "    df['word_length'] = df['word'].apply(lambda x: len(x))\n",
    "\n",
    "    # Group the data by word length and count the number of words\n",
    "    word_length_counts = df['word_length'].value_counts().sort_index()\n",
    "\n",
    "    # Calculate the total number of words\n",
    "    total_words = word_length_counts.sum()\n",
    "\n",
    "    # Create a DataFrame with the word length counts\n",
    "    word_length_table = pd.DataFrame({'Word Length': word_length_counts.index, 'Count': word_length_counts.values})\n",
    "\n",
    "    # Add a row for the total number of words\n",
    "    total_row = pd.DataFrame({'Word Length': ['Total'], 'Count': [total_words]})\n",
    "    word_length_table = word_length_table.append(total_row, ignore_index=True)\n",
    "\n",
    "    return word_length_table\n",
    "\n",
    "# Call the function to get the word length distribution table\n",
    "word_length_table = get_word_length_distribution()\n",
    "\n",
    "# Filter the table to keep only words with a length of 50\n",
    "words_with_length_50 = word_length_table[word_length_table['Word Length'] == 40]\n",
    "\n",
    "# Get the actual words with a length of 50\n",
    "words = pd.read_csv('corpus/ngram_freq.csv')\n",
    "words_with_length_50_words = words[words['word'].apply(lambda x: len(str(x)) == 40)]['word']\n",
    "\n",
    "# Print the words with a length of 50\n",
    "# Print the words with a length of 50\n",
    "print(\"Words with a length of 20:\")\n",
    "print(words_with_length_50_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a50ac5-51dc-4f7c-8562-8626a45a0687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bigram data https://github.com/orgtre/google-books-ngram-frequency/blob/main/ngrams/2grams_english.csv\n",
    "#this is the most common bigrams data- 5.000 most frequent 2-grams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedd34bc-a430-4f37-b2ca-f89ec6d1604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimenting with more accurate syllable count than Datamuse with NLTK- Conclusion was that Datamuse will be kept because that was what best-performing\n",
    "#CAMB system used. THIS WAS FOUND TO BE LESS EFFECTIVE\n",
    "\n",
    "\n",
    "# def get_syllables(word):\n",
    "#     syllables = 0\n",
    "#     word_results = api.words(sp=word, max=1, md='psf')\n",
    "#     if len(word_results) > 0:\n",
    "#         word = word_results[0][\"word\"]\n",
    "#         syllables = int(word_results[0][\"numSyllables\"])\n",
    "#     print(\"# of syllables for\", word, \":\", syllables)\n",
    "#     return syllables\n",
    "\n",
    "\n",
    "\n",
    "# nltk.download('cmudict')\n",
    "# def get_syllablesNLTK(word):\n",
    "#     cmudict = nltk.corpus.cmudict.dict()\n",
    "#     syllables = 0\n",
    "#     if word.lower() in cmudict:\n",
    "#         syllables = [len(list(y for y in x if y[-1].isdigit())) for x in cmudict[word.lower()]]\n",
    "#         syllables = max(syllables) if syllables else 0\n",
    "#     return syllables\n",
    "# get_syllablesNLTK('epidexipteryx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071ee56-f1a1-4b66-8463-25c1cc8b2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to find a legit way to get this data\n",
    "\n",
    "#Now for SimpleWiki Freqency\n",
    "#https://www.loc.gov/item/2019205402/\n",
    "\n",
    "#Need the wiki.simple dataset to run\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# #stop = stopwords.words('english')\n",
    "# nltk.download('stopwords')\n",
    "\"\"\"\n",
    "Function to create Simple Wiki corpus \n",
    "\n",
    "\"\"\"\n",
    "# def simple_wiki():\n",
    "\n",
    "#     df_wiki = pd.read_table('wiki.simple', names=['sentence'])\n",
    "\n",
    "#     # casing\n",
    "#     df_wiki['sentence'] = df_wiki.sentence.str.lower()\n",
    "#     # remove stop words\n",
    "#     df_wiki['sentence'] = df_wiki.sentence.apply(lambda x: ' '.join(\n",
    "#         [word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#     # remove punctuaction\n",
    "#     df_wiki['sentence'] = df_wiki.sentence.str.replace('[^\\w\\s]', '')\n",
    "#     # print(df_wiki)\n",
    "\n",
    "#     # remove numbers\n",
    "#     df_wiki['sentence'] = df_wiki.sentence.str.replace('\\d+', '')\n",
    "#     df_wiki['sentence'] = df_wiki.sentence.str.strip()\n",
    "\n",
    "#     series_top = pd.Series(\n",
    "#         \" \".join((df_wiki.sentence).str.lower()).split()).value_counts()\n",
    "\n",
    "#     df_top = series_top.to_frame(name=\"frequency\")\n",
    "#     df_top['word'] = df_top.index\n",
    "#     df_top['word'] = df_top.word[df_top.word.str.len() > 2]\n",
    "#     df_top = df_top.dropna(axis=0)\n",
    "#     print(df_top)\n",
    "\n",
    "#     df_top = df_top.nlargest(6386, \"frequency\")\n",
    "#     print(df_top)\n",
    "\n",
    "#     df_top = df_top[['word', 'frequency']]\n",
    "\n",
    "#     # # # make csv file\n",
    "#     df_top.to_csv(\"simple_wiki.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53075f2-343d-4338-b788-fb667047981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_word_occurrences(file_path, word):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         text = file.read()\n",
    "#     count = text.lower().count(word.lower())\n",
    "#     return count\n",
    "\n",
    "# # Specify the file path of simple_wiki.txt\n",
    "# file_path = \"corpus/simple_wiki.txt\"\n",
    "\n",
    "# # Apply the function to count occurrences and add the column to word_features DataFrame\n",
    "# word_features['simple_wiki_freq'] = word_features['word'].apply(lambda x: count_word_occurrences(file_path, x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee12d49-127f-4715-a9c5-d424c3be4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This only works for single words\n",
    "\"\"\"\n",
    "Counts all word occurrences in the simple Wiki corpus and stores the counts in a dictionary then looks up the count for each word in the dictionary. \n",
    "For a single word\n",
    "\"\"\"\n",
    "def count_all_word_occurrences(file_path):\n",
    "    # Create a dictionary to store word counts\n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Convert to lowercase and split the line into words\n",
    "            words = re.findall(r'\\b\\w+\\b', line.lower())\n",
    "            # Count each word\n",
    "            for word in words:\n",
    "                word_counts[word] += 1\n",
    "                \n",
    "    return word_counts\n",
    "\n",
    "# Count all word occurrences in the file\n",
    "file_path = \"corpus/simple_wiki.txt\"\n",
    "word_counts = count_all_word_occurrences(file_path)\n",
    "\n",
    "# Look up the count for each word in the dictionary and add it to the DataFrame\n",
    "data_frame['simple_wiki_freq'] = data_frame['word'].apply(lambda x: word_counts[x.lower()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5e620-3257-4efe-ac58-130320012db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4add9-accd-43d9-8a3f-42036d93c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CAUSED KERNAL TO QUIT\n",
    "#This looks up bigrams trigams fourgrams and five grams in simple_wiki corpus\n",
    "\n",
    "import itertools\n",
    "from itertools import chain\n",
    "\n",
    "def count_word_combinations(file_path, combinations):\n",
    "    # Create a dictionary to store combination counts\n",
    "    combination_counts = defaultdict(int)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.lower()\n",
    "            for combination in combinations:\n",
    "                count = line.count(combination)\n",
    "                if count > 0:\n",
    "                    combination_counts[combination] += count\n",
    "\n",
    "    return combination_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate all combinations of words up to a maximum of five words\n",
    "combinations = []\n",
    "\n",
    "# Flatten the word_column_words list\n",
    "word_column_words = list(chain.from_iterable(data_frame['word'].str.lower().str.split()))\n",
    "\n",
    "# Generate all combinations of words up to a maximum of five words\n",
    "combinations = []\n",
    "for n in range(1, 6):\n",
    "    combinations.extend([' '.join(combination) for combination in itertools.combinations(word_column_words, n)])\n",
    "    \n",
    "# Provide the path to the Simple Wiki corpus file\n",
    "file_path = \"corpus/simple_wiki.txt\"\n",
    "\n",
    "# Count the occurrences of the word combinations\n",
    "combination_counts = count_word_combinations(file_path, combinations)\n",
    "\n",
    "# Update the DataFrame with the combination counts\n",
    "data_frame['combination_count'] = data_frame['word'].apply(lambda x: combination_counts.get(x.lower(), 0))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d1b7d-b8c4-4980-883b-a73e1738a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bcccb-a51c-425e-967f-5c4d151bddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The count_all_word_occurrences function is modified to count unigrams, bigrams, trigrams and hyphenated words in simple Wikipedia.\n",
    "\"\"\"\n",
    "In this updated version, the occurrence counts of unigrams, bigrams, trigrams, fourgrams, and fivegrams are calculated \n",
    "separately and then combined into a single column called simple_wiki_freq. The counts are obtained by summing the \n",
    "occurrence counts of each ngram type using the apply method with lambda functions.\n",
    "\"\"\"\n",
    "\n",
    "def search_ngrams(file_path, ngrams):\n",
    "    # Create a dictionary to store ngram occurrences\n",
    "    ngram_occurrences = defaultdict(int)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Convert to lowercase and split the line into words\n",
    "            words = nltk.word_tokenize(line.lower())\n",
    "            # Generate ngrams\n",
    "            line_ngrams = list(nltk.ngrams(words, len(ngrams[0])))\n",
    "            # Search for each ngram occurrence\n",
    "            for ngram in line_ngrams:\n",
    "                if ngram in ngrams:\n",
    "                    ngram_occurrences[ngram] += 1\n",
    "                \n",
    "    return ngram_occurrences\n",
    "\n",
    "# Create a list of ngrams from the words in the word column\n",
    "word_column_words = data_frame['word'].str.lower().str.split()\n",
    "unigrams = [word for words in word_column_words for word in words]\n",
    "bigrams = list(nltk.ngrams(word_column_words, 2))\n",
    "trigrams = list(nltk.ngrams(word_column_words, 3))\n",
    "fourgrams = list(nltk.ngrams(word_column_words, 4))\n",
    "fivegrams = list(nltk.ngrams(word_column_words, 5))\n",
    "\n",
    "# Count the occurrences of unigrams, bigrams, trigrams, fourgrams, and fivegrams in the Simple Wiki corpus\n",
    "file_path = \"corpus/simple_wiki.txt\"\n",
    "unigram_occurrences = search_ngrams(file_path, unigrams)\n",
    "bigram_occurrences = search_ngrams(file_path, bigrams)\n",
    "trigram_occurrences = search_ngrams(file_path, trigrams)\n",
    "fourgram_occurrences = search_ngrams(file_path, fourgrams)\n",
    "fivegram_occurrences = search_ngrams(file_path, fivegrams)\n",
    "\n",
    "# Combine the occurrence counts of all ngrams into a single column\n",
    "data_frame['simple_wiki_freq'] = (\n",
    "    data_frame['word'].apply(lambda x: sum(unigram_occurrences[word] for word in x.lower().split())) +\n",
    "    data_frame['word'].apply(lambda x: sum(bigram_occurrences[bigram] for bigram in nltk.ngrams(x.lower().split(), 2))) +\n",
    "    data_frame['word'].apply(lambda x: sum(trigram_occurrences[trigram] for trigram in nltk.ngrams(x.lower().split(), 3))) +\n",
    "    data_frame['word'].apply(lambda x: sum(fourgram_occurrences[fourgram] for fourgram in nltk.ngrams(x.lower().split(), 4))) +\n",
    "    data_frame['word'].apply(lambda x: sum(fivegram_occurrences[fivegram] for fivegram in nltk.ngrams(x.lower().split(), 5)))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e4b59-30b5-48f3-8293-78c1eb07df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_frame.iloc[100:130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebaafc-7748-458a-9e59-9a145b7b13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKING Code\n",
    "#Frequency in HIT paragraph\n",
    "\n",
    "# def HIT_freq(data_frame, word, ID):\n",
    "#     paragraph = data_frame[data_frame['ID'] == ID]\n",
    "#     word_count = paragraph['word'].str.lower().value_counts().to_dict()\n",
    "#     word_occurrences = word_count.get(word.lower(), 0)\n",
    "#     return word_occurrences\n",
    "\n",
    "# # Add word occurrences to word_features DataFrame\n",
    "# word_features['HIT_count'] = word_features.apply(lambda row: HIT_freq(data_frame, row['word'], row['ID']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4955f35-a404-4174-9c0a-fa170d394df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW CODE to get HIT frquency with MWEs\n",
    "\n",
    "def HIT_freq(data_frame, words, ID):\n",
    "    paragraph = data_frame[data_frame['ID'] == ID]\n",
    "    word_count = paragraph['word'].str.lower().value_counts().to_dict()\n",
    "    total_occurrences = 0\n",
    "    for word in words.split():\n",
    "        word_occurrences = word_count.get(word.lower(), 0)\n",
    "        total_occurrences += word_occurrences\n",
    "    return total_occurrences\n",
    "\n",
    "# Add word occurrences to data_frame\n",
    "data_frame['HIT_count'] = data_frame.apply(lambda row: HIT_freq(data_frame, row['word'], row['ID']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be0c05-b082-41fd-b133-a8876f073bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7f12b-7a44-4137-903c-a536322725ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Web IT 5-gram frequency was not freely available so this is got throght the Datamuse API and uses Google N-gram corpus\n",
    "#Accessing Google's Web 5-gram corpus directly is not possible as the dataset is not publicly available.\n",
    "\n",
    "#Need to parse sentence to return frquency using Datamuse api\n",
    "\n",
    "\n",
    "# import requests\n",
    "\n",
    "# def get_datamuse_frequency(word):\n",
    "#     api_url = f\"https://api.datamuse.com/words?sp={word}&md=f\"\n",
    "    \n",
    "#     try:\n",
    "#         response = requests.get(api_url)\n",
    "#         data = response.json()\n",
    "#         if isinstance(data, list) and len(data) > 0:\n",
    "#             frequency = data[0].get('f', 0)\n",
    "#             return frequency\n",
    "#     except (requests.exceptions.RequestException, KeyError) as e:\n",
    "#         print(\"Error occurred:\", e)\n",
    "    \n",
    "#     return 0  # Default value when encountering an error\n",
    "\n",
    "# # Apply the function to get Datamuse frequency for each word in the \"word\" column\n",
    "# word_features['Datamuse frequency'] = word_features['word'].apply(lambda x: get_datamuse_frequency(x))\n",
    "\n",
    "# word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b8afb-e47a-4ced-b94a-7fedca5547dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start core\")\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "sentences = data_frame[['sentence', 'ID']].copy()\n",
    "\n",
    "sentences = sentences1.drop_duplicates()\n",
    "\n",
    "print(\"end core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afbc3c4-555e-4b26-8dff-5495146b7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a dataframe named 'data_frame' with a column named 'sentence'\n",
    "\n",
    "# Function to parse a sentence and return the parsed output as a string\n",
    "def parse_sentence(sentence):\n",
    "    parsed_output = parse(sentence)\n",
    "    return json.dumps(parsed_output)\n",
    "\n",
    "# Apply the parse_sentence function to the 'sentence' column and store the parsed output in a new 'Parse' column\n",
    "data_frame['Parse'] = data_frame['sentence'].apply(parse_sentence)\n",
    "\n",
    "# Print the updated dataframe\n",
    "data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f26460-80a8-497d-aa87-ca96468f7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a99c9-f33b-4d06-909f-941254b2ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791d5e7-e0a4-4e1c-8f5c-aef4139aa44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start first token\")\n",
    "\n",
    "def removefirsttoken(x):\n",
    "    x = x.split(' ', 1)[1]\n",
    "    return x\n",
    "\n",
    "sentences['clean sentence'] = sentences['sentence']\n",
    "\n",
    "# sentences.to_csv(\"debugging/sentences_noparse.csv\", index=False) debugging\n",
    "\n",
    "print(\"start end token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0b976-d8b2-44d2-875e-edc304c68062",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e0023-c3c4-40a6-8a0c-0168d97c11db",
   "metadata": {},
   "outputs": [],
   "source": [
    " # function to parse sentences\n",
    "print(\"start parse sentence\")\n",
    "\n",
    "def parse(string):\n",
    "    output = nlp.annotate(string, properties={\n",
    "        'annotators': 'pos,depparse,ner',\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "    output_dict = json.loads(output)\n",
    "    return output_dict\n",
    "    # return output\n",
    "\n",
    "# apply parsing to sentences\n",
    "sentences['parse'] = sentences['clean sentence'].apply(lambda x: parse(x))\n",
    "#MERGING THESE -  The issue is related to the parse column in the word_parse_features DataFrame, which should contain dictionaries representing the parsed output from the Stanford CoreNLP library.\n",
    "word_parse_features = pd.merge(sentences, data_frame)\n",
    "#word_parse_features.to_csv(\"debugging/word_parse_features_bug.csv\" , index=False) \n",
    "\n",
    "print(\"finish parsing sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febd560-a3ee-46d9-ae6f-45a3b2487707",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c790edc7-c0bc-4cb7-9279-57e16ee994aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5654879-7d6d-43f0-83c0-3b2bf1ab5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(row):\n",
    "    word = row['word']\n",
    "    parse = row['parse']\n",
    "    pos = parse['sentences'][0]['tokens'][0]['pos']\n",
    "    print(f\"Token {i+1}: {pos}\") \n",
    "    for i in range(len(parse['sentences'][0]['tokens'])):\n",
    "\n",
    "\n",
    "        comp_word = parse['sentences'][0]['tokens'][i]['word']\n",
    "        comp_word = comp_word.lower()\n",
    "        comp_word = comp_word.translate(\n",
    "            {ord(char): None for char in remove})\n",
    "        print(parse['sentences'][0]['tokens'][i]['pos'])\n",
    "        if comp_word == word:\n",
    "\n",
    "            return parse['sentences'][0]['tokens'][i]['pos']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca6328-20d0-41c6-b3ae-6f2990ff46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(row):\n",
    "    word = row['word']\n",
    "    parse = row['parse']\n",
    "    \n",
    "    for token in parse['sentences'][0]['tokens']:\n",
    "        comp_word = token['word']\n",
    "        comp_word = comp_word.lower()\n",
    "        comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "        \n",
    "        if comp_word == word:\n",
    "            return token['pos']\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945603e-f770-4058-b9a0-eda9f0e5da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "['pos'] =data_frame.apply(get_pos, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c213863-555b-4843-b6c1-fa63ef544000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dep(row):\n",
    "        number = 0\n",
    "        word = row['word']\n",
    "        parse = row['parse']\n",
    "        for i in range(len(parse['sentences'][0]['basicDependencies'])):\n",
    "            comp_word = parse['sentences'][0]['basicDependencies'][i]['governorGloss']\n",
    "            comp_word = comp_word.lower()\n",
    "            comp_word = comp_word.translate(\n",
    "                {ord(char): None for char in remove})\n",
    "\n",
    "            if comp_word == word:\n",
    "                number += 1\n",
    "\n",
    "        return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bfd04-c5fd-40a4-9551-49c3372b65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['dep num'] = word_parse_features.apply(get_dep, axis=1)\n",
    "word_parse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ea778-75f7-4eaf-a8a5-af68bf92f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner(row):\n",
    "        word = row['word']\n",
    "        parse = row['parse']\n",
    "        # print(parse, \"TEDFS\")\n",
    "        # print(parse['sentences'][0]['tokens'], \"DOFISPFDSIU\")\n",
    "        # print(parse['sentences'][1]['tokens'], \"DOFISPFDSIU\")\n",
    "\n",
    "        for i in range(len(parse['sentences'][0]['tokens'])):\n",
    "            comp_word = parse['sentences'][0]['tokens'][i]['word']\n",
    "            comp_word = comp_word.lower()\n",
    "            comp_word = comp_word.translate(\n",
    "                {ord(char): None for char in remove})\n",
    "            print(parse['sentences'][0]['tokens'][i]['ner'])\n",
    "            if comp_word == word:\n",
    "                \n",
    "                return parse['sentences'][0]['tokens'][i]['ner']\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9143736-27ab-4b0e-ad1f-ea8114c8bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences['ner'] = word_parse_features.apply(get_ner, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481b9a3-bac6-4b73-b2ac-3699637dcb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_parse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8095dd1-9bae-466c-9efe-3c16aaff5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start get pos\")\n",
    "word_parse_features['pos'] = word_parse_features.apply(get_pos, axis=1)\n",
    "print(\"end get pos\")\n",
    "\n",
    "print(\"start get dep\")\n",
    "\n",
    "word_parse_features['dep num'] = word_parse_features.apply(get_dep, axis=1)\n",
    "word_parse_features['ner'] = word_parse_features.apply(get_ner, axis=1)\n",
    "\n",
    "print(\"end get dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad755523-5b75-46f7-9fcb-740dc3d44ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_parse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d56eab-a3cc-4989-80e3-b495f375f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function to get the proper lemma\n",
    "\n",
    "print(\"start tagging\")\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if pd.isna(treebank_tag):\n",
    "        return None\n",
    "\n",
    "    if isinstance(treebank_tag, str) and treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif isinstance(treebank_tag, str) and treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif isinstance(treebank_tag, str) and treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif isinstance(treebank_tag, str) and treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54062d7c-2f65-4fb7-af68-f16d0bef6503",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start lemmatizing\")\n",
    "    \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatiser(row):\n",
    "    word = row['word']\n",
    "    pos = row['pos']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_pos = get_wordnet_pos(pos)\n",
    "\n",
    "    if wordnet_pos:\n",
    "        lemma = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "        return lemma\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        return lemma\n",
    "print(\"finish lemmatizing\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301f72d-270b-4a83-adf8-d0137aeec8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Convert tree bank tags to ones that are compatible w google\n",
    "\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wordnet.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wordnet.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wordnet.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wordnet.VERB\n",
    "    return None\n",
    "\n",
    "def penn_to_google(tag):\n",
    "    if is_adjective(tag):\n",
    "        return 'adj'\n",
    "    elif is_noun(tag):\n",
    "        return 'n'\n",
    "    elif is_adverb(tag):\n",
    "        return 'adv'\n",
    "    elif is_verb(tag):\n",
    "        return 'v'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f3ae8-46fc-42c3-be01-1bf98ab9ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency(row):\n",
    "        nofreq = float(0.000000)\n",
    "        word = row[\"word\"]\n",
    "        print(\"word:\", word)\n",
    "        word = str(word)\n",
    "        tag = row[\"pos\"]\n",
    "        tag = penn_to_google(tag)\n",
    "\n",
    "        try:\n",
    "            word_results = api.words(sp=word, max=1, md='pf')\n",
    "            tag_list = (word_results[0]['tags'][:-1])\n",
    "\n",
    "            frequency = word_results[0]['tags'][-1][2:]\n",
    "\n",
    "            frequency = float(frequency)\n",
    "\n",
    "            if tag in tag_list:\n",
    "                print(\"frequency_1:\", frequency)\n",
    "                return frequency\n",
    "            else:\n",
    "                lemma = row['lemma']\n",
    "                try:\n",
    "                    word_results = api.words(sp=lemma, max=1, md='pf')\n",
    "                    tag_list = (word_results[0]['tags'][:-1])\n",
    "                    frequency = word_results[0]['tags'][-1][2:]\n",
    "                    frequency = float(frequency)\n",
    "\n",
    "                    if tag in tag_list:\n",
    "                        print(\"frequency_2:\", frequency)\n",
    "                        return frequency\n",
    "                    else:\n",
    "                        print(\"nofreq\")\n",
    "                        return nofreq\n",
    "                except:\n",
    "                    return nofreq\n",
    "\n",
    "        except:\n",
    "\n",
    "            return nofreq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649c150-56f6-4e66-be29-e09a6d64f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_parse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e331a-af25-47d0-974e-7be8faa70571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the \"word\" column in the DataFrame and calculate the frequency\n",
    "data_frame['frequency'] = word_parse_features.apply(get_frequency, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e83f85-517b-44b2-a796-98b5cbe4a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function for google freq\n",
    "print(\"get google frequency\")\n",
    "data_frame['google frequency'] = word_parse_features.apply(\n",
    "    get_frequency, axis=1)\n",
    "print(\"end google frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b3a04-c351-43c0-98a7-8b17caea8504",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d3633-276c-4bb0-b49c-dd89a7752125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Get the base filename from the location variable\n",
    "base_filename = os.path.splitext(os.path.basename(location))[0]\n",
    "\n",
    "# Specify the output file path\n",
    "output_file_path = os.path.join('features_NEW', base_filename + '_NEW_Feats.pkl')\n",
    "\n",
    "# Pickle the word_features DataFrame\n",
    "with open(output_file_path, 'wb') as file:\n",
    "    pickle.dump(data_frame, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174fde6-39da-4cf1-aff7-e0223ad5f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_rows_with_zero(data_frame):\n",
    "    for index, row in data_frame.iterrows():\n",
    "        ngram_freq = row['ngram_freq']\n",
    "        if ngram_freq == 0:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac165a8a-a227-4228-bc70-a9709a0c024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rows_with_zero(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59698af3-621b-4e53-b2bc-8a938fe9b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zeros_in_column(data_frame, column_name):\n",
    "    count = 0\n",
    "    for index, row in data_frame.iterrows():\n",
    "        value = row[column_name]\n",
    "        if value == 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667a069-7b50-464e-a9d5-e0889af8df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_zeros_in_column(data_frame, \"ngram_freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c59cb0d9-3e04-4eb0-9835-90d32b17814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_levels = pd.read_table('corpus/cefrj-vocabulary-profile-1.5.csv', names=('headword', 'CEFR'))\n",
    "\n",
    "def levels(word):\n",
    "    word = ''.join(word.split()).lower()\n",
    "    try:\n",
    "        df = all_levels.loc[all_levels['headword'] == word]\n",
    "        level = df.iloc[0]['CEFR']\n",
    "        return level\n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            df = all_levels.loc[all_levels['headword'] == word]\n",
    "            level = df.iloc[0]['CEFR']\n",
    "            return level\n",
    "        except:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "237c69f9-a414-4f1e-ba84-d2880729c33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels(\"abandon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac3ea371-953b-4145-a735-f46f92fc894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def levels(word):\n",
    "    all_levels = pd.read_csv('corpus/cefrj-vocabulary-profile-1.5.csv')\n",
    "    word = ''.join(word.split()).lower()\n",
    "    df = all_levels.loc[all_levels['headword'] == word]\n",
    "    if not df.empty:\n",
    "        level = df.iloc[0]['CEFR']\n",
    "        return level\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8b63f-3b8e-4d67-a93f-62b75c8e3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
