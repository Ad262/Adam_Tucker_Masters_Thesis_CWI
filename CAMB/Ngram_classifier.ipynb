{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee372dbd-0dfa-4136-b2e9-85f1b329dfe1",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This pipeline first extracts features from the 'phrase' column using a CountVectorizer, which creates a \"bag-of-words\" representation of the text data. The 'absTotalMatchCount' and 'relTotalMatchCount' columns are scaled using a StandardScaler to ensure that these features have a mean of 0 and standard deviation of 1, which can help the AdaBoost algorithm converge faster.\n",
    "\n",
    "Finally, these features are combined and passed to the AdaBoostClassifier. You can then train this pipeline on your training data and use it to predict whether phrases are complex or not.\n",
    "\n",
    "This is just one possible pipeline. You might need to adjust it to fit your specific needs. In particular, you might need to select different transformations for the text data or use different methods for scaling or normalizing the numeric features.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35660b36-3bf6-46b0-837c-b4d0be0f10e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3QI9WAYOGQCX8YMZA9CAS9VCVMWS62</th>\n",
       "      <th>The tail of Epidexipteryx also bore unusual vertebrae towards the tip which resembled the feather-anchoring pygostyle of modern birds and some oviraptorosaurs .</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>tail</th>\n",
       "      <th>10</th>\n",
       "      <th>10.1</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <th>0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3QI9WAYOGQCX8YMZA9CAS9VCVMWS62</td>\n",
       "      <td>The tail of Epidexipteryx also bore unusual ve...</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>Epidexipteryx</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3QI9WAYOGQCX8YMZA9CAS9VCVMWS62</td>\n",
       "      <td>The tail of Epidexipteryx also bore unusual ve...</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>bore</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3QI9WAYOGQCX8YMZA9CAS9VCVMWS62</td>\n",
       "      <td>The tail of Epidexipteryx also bore unusual ve...</td>\n",
       "      <td>31</td>\n",
       "      <td>53</td>\n",
       "      <td>bore unusual vertebrae</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3QI9WAYOGQCX8YMZA9CAS9VCVMWS62</td>\n",
       "      <td>The tail of Epidexipteryx also bore unusual ve...</td>\n",
       "      <td>36</td>\n",
       "      <td>53</td>\n",
       "      <td>unusual vertebrae</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3QI9WAYOGQCX8YMZA9CAS9VCVMWS62</td>\n",
       "      <td>The tail of Epidexipteryx also bore unusual ve...</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>unusual</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   3QI9WAYOGQCX8YMZA9CAS9VCVMWS62  \\\n",
       "0  3QI9WAYOGQCX8YMZA9CAS9VCVMWS62   \n",
       "1  3QI9WAYOGQCX8YMZA9CAS9VCVMWS62   \n",
       "2  3QI9WAYOGQCX8YMZA9CAS9VCVMWS62   \n",
       "3  3QI9WAYOGQCX8YMZA9CAS9VCVMWS62   \n",
       "4  3QI9WAYOGQCX8YMZA9CAS9VCVMWS62   \n",
       "\n",
       "  The tail of Epidexipteryx also bore unusual vertebrae towards the tip which resembled the feather-anchoring pygostyle of modern birds and some oviraptorosaurs .  \\\n",
       "0  The tail of Epidexipteryx also bore unusual ve...                                                                                                                 \n",
       "1  The tail of Epidexipteryx also bore unusual ve...                                                                                                                 \n",
       "2  The tail of Epidexipteryx also bore unusual ve...                                                                                                                 \n",
       "3  The tail of Epidexipteryx also bore unusual ve...                                                                                                                 \n",
       "4  The tail of Epidexipteryx also bore unusual ve...                                                                                                                 \n",
       "\n",
       "    4   8                    tail  10  10.1  0  2  1   0.1  \n",
       "0  12  25           Epidexipteryx  10    10  6  3  1  0.45  \n",
       "1  31  35                    bore  10    10  0  3  1  0.15  \n",
       "2  31  53  bore unusual vertebrae  10    10  0  1  1  0.05  \n",
       "3  36  53       unusual vertebrae  10    10  2  0  1  0.10  \n",
       "4  36  43                 unusual  10    10  0  0  0  0.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"cwishareddataset/traindevset/english/pickled-dataframes/Wikipedia_Dev.csv\"\n",
    "data_frame = pd.read_csv(data_path)\n",
    "\n",
    "data_frame.head()  # Display the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87958242-fcc0-4b1f-852a-e94ea6605dc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(classifier, output_model_file)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_gram_Model\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDesktop/CWI_masters/camb_model/cwi_2018-master/Ngram_data/predictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(train_data, output_model_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_classifier\u001b[39m(train_data, output_model_file):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Extract features and target variables\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     train_features \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabsTotalMatchCount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelTotalMatchCount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m     train_targets \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m9\u001b[39m]  \u001b[38;5;66;03m# Select the 10th column as the target variable\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Build and train the classifier\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "N_gram_Model = \"N_gram_Model\"\n",
    "# Function to train the classifier\n",
    "def train_classifier(train_data, output_model_file):\n",
    "    # Extract features and target variables\n",
    "    train_features = train_data[['absTotalMatchCount', 'relTotalMatchCount']]\n",
    "    train_targets = train_data.iloc[:, 9]  # Select the 10th column as the target variable\n",
    "\n",
    "    # Build and train the classifier\n",
    "    classifier = AdaBoostClassifier(n_estimators=5000, random_state=67)\n",
    "    classifier.fit(train_features, train_targets)\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(classifier, output_model_file)\n",
    "\n",
    "\n",
    "train_classifier(N_gram_Model,\"Desktop/CWI_masters/camb_model/cwi_2018-master/Ngram_data/predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01616606-4915-4f76-ab49-e3cf324a0b20",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Desktop/CWI_masters/camb_model/cwi_2018-master/Ngram_data/predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_data_file, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train the classifier and save the model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_model_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 18\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(train_data, output_model_file)\u001b[0m\n\u001b[1;32m     15\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(train_features, train_targets)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_model_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/numpy_pickle.py:481\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    479\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    482\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Desktop/CWI_masters/camb_model/cwi_2018-master/Ngram_data/predictions'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Function to train the classifier\n",
    "def train_classifier(train_data, output_model_file):\n",
    "    # Extract features and target variables\n",
    "    train_features = train_data[['absTotalMatchCount', 'relTotalMatchCount']]\n",
    "    train_targets = train_data.iloc[:, 9]  # Select the 10th column as the target variable\n",
    "\n",
    "    # Build and train the classifier\n",
    "    classifier = AdaBoostClassifier(n_estimators=5000, random_state=67)\n",
    "    classifier.fit(train_features, train_targets)\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(classifier, output_model_file)\n",
    "\n",
    "# Define the file paths for training data and output model file\n",
    "train_data_file = \"Ngram_data/lexicon_ngram_train.tsv\"\n",
    "output_model_file = \"Desktop/CWI_masters/camb_model/cwi_2018-master/Ngram_data/predictions\"\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(train_data_file, sep='\\t')\n",
    "\n",
    "# Train the classifier and save the model\n",
    "train_classifier(train_data, output_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54633b86-c7cf-4c72-bbfa-879e27147538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to run the classifier and evaluate it\n",
    "def evaluate_classifier(test_data, model_file):\n",
    "    # Load the trained model\n",
    "    classifier = joblib.load(model_file)\n",
    "\n",
    "    # Extract features and target variables\n",
    "    test_features = test_data[['absTotalMatchCount', 'relTotalMatchCount']]\n",
    "    test_targets = test_data.iloc[:, 9]  # Select the 10th column as the target variable\n",
    "\n",
    "    # Convert 'phrase' column values to string\n",
    "    test_data['phrase'] = test_data['phrase'].astype(str)\n",
    "\n",
    "    # Print the values of the 4th column (phrase) before evaluating\n",
    "    print(\"Values of the 4th column (phrase):\")\n",
    "    print(test_data['phrase'])\n",
    "    print()\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_predictions = classifier.predict(test_features)\n",
    "\n",
    "    # Print the evaluated strings and their labels\n",
    "    print(\"Evaluated strings:\")\n",
    "    for index, row in test_data.iterrows():\n",
    "        phrase = row['phrase']\n",
    "        predicted_label = test_predictions[index]\n",
    "        real_label = test_targets[index]\n",
    "        if ' ' in phrase:\n",
    "            print(\"Phrase:\", phrase)\n",
    "            print(\"Predicted Label:\", predicted_label)\n",
    "            print(\"Real Label:\", real_label)\n",
    "            print()\n",
    "\n",
    "    # Evaluate the performance of the classifier\n",
    "    accuracy = accuracy_score(test_targets, test_predictions)\n",
    "    precision = precision_score(test_targets, test_predictions)\n",
    "    recall = recall_score(test_targets, test_predictions)\n",
    "    f1 = f1_score(test_targets, test_predictions)\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the lexicon data\n",
    "train_data = pd.read_csv('Ngram_data/lexicon_ngram_train.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('Ngram_data/lexicon_ngram_test.tsv', sep='\\t')\n",
    "\n",
    "# Check if the fourth column contains multiple words\n",
    "if train_data.iloc[:, 3].str.contains(' ').any() and test_data.iloc[:, 3].str.contains(' ').any():\n",
    "    # Train the classifier and save the model\n",
    "    train_classifier(train_data, 'basic_NgramModel')\n",
    "\n",
    "    # Run the classifier and evaluate it\n",
    "    evaluate_classifier(test_data, 'basic_NgramModel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b085a21e-c52e-47fd-b2f8-09fafc78f22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</th>\n",
       "      <th>#26-7 Initially, all three were considered victims, but the status of one has been changed to suspect.</th>\n",
       "      <th>6</th>\n",
       "      <th>15</th>\n",
       "      <th>Initially</th>\n",
       "      <th>10</th>\n",
       "      <th>10.1</th>\n",
       "      <th>7</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>0.4</th>\n",
       "      <th>absTotalMatchCount</th>\n",
       "      <th>relTotalMatchCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>considered</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>109512829</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>victims</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>305606548</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>status</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>63091031</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>83</td>\n",
       "      <td>90</td>\n",
       "      <td>changed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>122923946</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>94</td>\n",
       "      <td>101</td>\n",
       "      <td>suspect</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>54165465</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   374UMBUHN5QN3F8F90U3OEJ8SKCTCW  \\\n",
       "0  374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "1  374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "2  374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "3  374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "4  374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "\n",
       "  #26-7 Initially, all three were considered victims, but the status of one has been changed to suspect.  \\\n",
       "0  #26-7 Initially, all three were considered vic...                                                       \n",
       "1  #26-7 Initially, all three were considered vic...                                                       \n",
       "2  #26-7 Initially, all three were considered vic...                                                       \n",
       "3  #26-7 Initially, all three were considered vic...                                                       \n",
       "4  #26-7 Initially, all three were considered vic...                                                       \n",
       "\n",
       "    6   15   Initially  10  10.1  7  1  1.1   0.4  absTotalMatchCount  \\\n",
       "0  32   42  considered  10    10  2  3    1  0.25           109512829   \n",
       "1  43   50     victims  10    10  4  2    1  0.30           305606548   \n",
       "2  60   66      status  10    10  3  1    1  0.20            63091031   \n",
       "3  83   90     changed  10    10  0  2    1  0.10           122923946   \n",
       "4  94  101     suspect  10    10  3  2    1  0.25            54165465   \n",
       "\n",
       "   relTotalMatchCount  \n",
       "0            0.000055  \n",
       "1            0.000153  \n",
       "2            0.000032  \n",
       "3            0.000062  \n",
       "4            0.000027  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to get ngram counts\n",
    "def get_ngram_counts(data_frame):\n",
    "    abs_counts = []\n",
    "    rel_counts = []\n",
    "\n",
    "    session = requests.Session()  # Create a session object for reusing connections\n",
    "\n",
    "    def process_phrase(phrase):\n",
    "        url = 'https://api.ngrams.dev/eng/search'\n",
    "        params = {\n",
    "            'query': phrase,\n",
    "            'flags': 'cs',\n",
    "            'limit': 1\n",
    "        }\n",
    "\n",
    "        response = session.get(url, params=params, verify=True)  # Reuse the session for subsequent requests\n",
    "        data = response.json()\n",
    "\n",
    "        if 'ngrams' in data and len(data['ngrams']) > 0:\n",
    "            abs_count = data['ngrams'][0]['absTotalMatchCount']\n",
    "            rel_count = data['ngrams'][0]['relTotalMatchCount']\n",
    "        else:\n",
    "            abs_count = None\n",
    "            rel_count = None\n",
    "\n",
    "        return abs_count, rel_count\n",
    "\n",
    "    phrases = data_frame.iloc[:, 3]\n",
    "    num_workers = min(len(phrases), 10)  # Adjust the number of workers as per your requirements\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(process_phrase, phrases)\n",
    "\n",
    "    for abs_count, rel_count in results:\n",
    "        abs_counts.append(abs_count)\n",
    "        rel_counts.append(rel_count)\n",
    "\n",
    "    data_frame['absTotalMatchCount'] = abs_counts\n",
    "    data_frame['relTotalMatchCount'] = rel_counts\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "data_directory = \"cwishareddataset/traindevset/english/pickled-dataframes\"\n",
    "output_directory = \"Ngram_data\"  # Specify the directory where you want to save the lexicon files\n",
    "\n",
    "for file in os.listdir(data_directory):\n",
    "    if file.endswith(\"_Dev.pkl\"):\n",
    "        data_frame = pd.read_pickle(os.path.join(data_directory, file))\n",
    "        processed_data_frame = get_ngram_counts(data_frame)\n",
    "        output_file = os.path.join(output_directory, \"lexicon_ngram_test.tsv\")\n",
    "        processed_data_frame.to_csv(output_file, sep='\\t', index=False)\n",
    "    elif file.endswith(\"_Train.pkl\"):\n",
    "        data_frame = pd.read_pickle(os.path.join(data_directory, file))\n",
    "        processed_data_frame = get_ngram_counts(data_frame)\n",
    "        output_file = os.path.join(output_directory, \"lexicon_ngram_train.tsv\")\n",
    "        processed_data_frame.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "data_frame.head()  # Display the first few rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af3d5916-0c71-4806-a6e8-577d41440e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv('lexicon_Ngram.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f153e70-4d70-4fbc-851f-84a04bd89d85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 23 elements, new values have 11 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Rename the columns\u001b[39;00m\n\u001b[1;32m     74\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_native\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_non_native\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnative_complex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon_native_complex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplex_binary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplex_probabilistic\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 75\u001b[0m train_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m column_names\n\u001b[1;32m     76\u001b[0m test_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m column_names\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Extract the target variables from the 10th column of the data\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5915\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5913\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   5914\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 5915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   5917\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/properties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:823\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: \u001b[38;5;28mint\u001b[39m, labels: AnyArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    822\u001b[0m     labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:230\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: \u001b[38;5;28mint\u001b[39m, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/base.py:70\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 23 elements, new values have 11 elements"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "# Define a feature extraction pipeline for the phrase\n",
    "phrase_features = Pipeline([\n",
    "    ('selector', TextSelector(key='phrase')),\n",
    "    ('vect', CountVectorizer())\n",
    "])\n",
    "\n",
    "# Define feature scaling pipelines for the ngram counts\n",
    "abs_counts_scaler = Pipeline([\n",
    "    ('selector', NumberSelector(key='absTotalMatchCount')),\n",
    "    ('standard', StandardScaler())\n",
    "])\n",
    "\n",
    "rel_counts_scaler = Pipeline([\n",
    "    ('selector', NumberSelector(key='relTotalMatchCount')),\n",
    "    ('standard', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine these into a feature union\n",
    "feats = FeatureUnion([\n",
    "    ('phrase', phrase_features),\n",
    "    ('abs_counts', abs_counts_scaler),\n",
    "    ('rel_counts', rel_counts_scaler),\n",
    "])\n",
    "\n",
    "# Define the final pipeline as a combination of feature extraction and AdaBoost\n",
    "pipeline = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('classifier', AdaBoostClassifier(n_estimators=5000, random_state=67)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read all .pkl files and split into training and test sets\n",
    "data_path = \"cwishareddataset/traindevset/english/pickled-dataframes\"\n",
    "files = os.listdir(data_path)\n",
    "\n",
    "train_data = pd.concat([pd.read_pickle(os.path.join(data_path, file)) for file in files if file.endswith('_Train.pkl')])\n",
    "test_data = pd.concat([pd.read_pickle(os.path.join(data_path, file)) for file in files if file.endswith('_Dev.pkl')])\n",
    "\n",
    "# Rename the columns\n",
    "column_names = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "train_data.columns = column_names\n",
    "test_data.columns = column_names\n",
    "# Extract the target variables from the 10th column of the data\n",
    "train_targets = train_data.iloc[:, 9].values\n",
    "test_targets = test_data.iloc[:, 9].values\n",
    "\n",
    "\n",
    "    \n",
    "# Extract the phrase variables from the 4th column of the data\n",
    "train_data['phrase'] = train_data.iloc[:, 4]\n",
    "test_data['phrase'] = test_data.iloc[:, 4]\n",
    "\n",
    "# Check the type of each element in the 'phrase' column\n",
    "non_string_data = train_data[train_data['phrase'].apply(lambda x: not isinstance(x, str))]\n",
    "\n",
    "print(f\"Number of non-string entries: {len(non_string_data)}\")\n",
    "\n",
    "if len(non_string_data) > 0:\n",
    "    print(\"Some examples of non-string entries:\")\n",
    "    print(non_string_data.head())\n",
    "# Use FeatureUnion and pipelines to process features and fit the model\n",
    "pipeline = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('classifier', AdaBoostClassifier(n_estimators=5000, random_state=67)),\n",
    "])\n",
    "\n",
    "pipeline.fit(train_data, train_targets)\n",
    "\n",
    "# Apply the classifier pipeline to the data\n",
    "test_data['complex_prediction'] = pipeline.predict(test_data)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and f1-score\n",
    "accuracy = accuracy_score(test_targets, test_data['complex_prediction'])\n",
    "precision = precision_score(test_targets, test_data['complex_prediction'])\n",
    "recall = recall_score(test_targets, test_data['complex_prediction'])\n",
    "f1 = f1_score(test_targets, test_data['complex_prediction'])\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0a51185-7c17-4956-a644-2a40c14707d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f725912-230d-4daa-ba39-fec9edf9d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5192b8-83a1-46b1-8104-b90e931b1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_evaluate(df):\n",
    "    # Apply the classifier pipeline to the data\n",
    "    df['complex_prediction'] = pipeline.predict(df)\n",
    "    \n",
    "    # Calculate accuracy, precision, recall, and f1-score\n",
    "    evaluation_data = []\n",
    "    for phrase in df['phrase'].unique():\n",
    "        phrase_data = df[df['phrase'] == phrase]\n",
    "        accuracy = accuracy_score(phrase_data['complex_binary'], phrase_data['complex_prediction'])\n",
    "        precision = precision_score(phrase_data['complex_binary'], phrase_data['complex_prediction'])\n",
    "        recall = recall_score(phrase_data['complex_binary'], phrase_data['complex_prediction'])\n",
    "        f1 = f1_score(phrase_data['complex_binary'], phrase_data['complex_prediction'])\n",
    "        evaluation_data.append([phrase, accuracy, precision, recall, f1])\n",
    "    \n",
    "    # Convert the evaluation data into a DataFrame\n",
    "    evaluation_df = pd.DataFrame(evaluation_data, columns=['phrase', 'accuracy', 'precision', 'recall', 'f1_score'])\n",
    "    \n",
    "    return evaluation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc3c20-dd03-45bd-9591-81f931922c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "# Define a feature extraction pipeline for the phrase\n",
    "phrase_features = Pipeline([\n",
    "    ('selector', TextSelector(key='phrase')),\n",
    "    ('vect', CountVectorizer())\n",
    "])\n",
    "\n",
    "# Define feature scaling pipelines for the ngram counts\n",
    "abs_counts_scaler = Pipeline([\n",
    "    ('selector', NumberSelector(key='absTotalMatchCount')),\n",
    "    ('standard', StandardScaler())\n",
    "])\n",
    "\n",
    "rel_counts_scaler = Pipeline([\n",
    "    ('selector', NumberSelector(key='relTotalMatchCount')),\n",
    "    ('standard', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine these into a feature union\n",
    "feats = FeatureUnion([\n",
    "    ('phrase', phrase_features),\n",
    "    ('abs_counts', abs_counts_scaler),\n",
    "    ('rel_counts', rel_counts_scaler),\n",
    "])\n",
    "\n",
    "# Define the final pipeline as a combination of feature extraction and AdaBoost\n",
    "pipeline = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('classifier', AdaBoostClassifier(n_estimators=5000, random_state=67)),\n",
    "])\n",
    "\n",
    "# Read all .pkl files and split into training and test sets\n",
    "data_path = \"cwishareddataset/traindevset/english/pickled-dataframes\"\n",
    "files = os.listdir(data_path)\n",
    "\n",
    "train_data = pd.concat([pd.read_pickle(os.path.join(data_path, file)) for file in files if file.endswith('_Train.pkl')])\n",
    "test_data = pd.concat([pd.read_pickle(os.path.join(data_path, file)) for file in files if file.endswith('_Dev.pkl')])\n",
    "\n",
    "# Rename the columns\n",
    "column_names = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "train_data.columns = column_names\n",
    "test_data.columns = column_names\n",
    "\n",
    "# Extract the target variables from the 10th column of the data\n",
    "train_targets = train_data.iloc[:, 9].values\n",
    "test_targets = test_data.iloc[:, 9].values\n",
    "\n",
    "# Extract the phrase variables from the 4th column of the data\n",
    "train_data['phrase'] = train_data.iloc[:, 4]\n",
    "test_data['phrase'] = test_data.iloc[:, 4]\n",
    "\n",
    "# Check the type of each element in the 'phrase' column\n",
    "non_string_data = train_data[train_data['phrase'].apply(lambda x: not isinstance(x, str))]\n",
    "\n",
    "print(f\"Number of non-string entries: {len(non_string_data)}\")\n",
    "\n",
    "if len(non_string_data) > 0:\n",
    "    print(\"Some examples of non-string entries:\")\n",
    "    print(non_string_data.head())\n",
    "\n",
    "# Use FeatureUnion and pipelines to process features and fit the model\n",
    "pipeline = Pipeline([\n",
    "    ('features', feats),\n",
    "    ('classifier', AdaBoostClassifier(n_estimators=5000, random_state=67)),\n",
    "])\n",
    "\n",
    "pipeline.fit(train_data, train_targets)\n",
    "\n",
    "# Apply the classifier pipeline to the data\n",
    "test_data['complex_prediction'] = pipeline.predict(test_data)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and f1-score\n",
    "accuracy = accuracy_score(test_targets, test_data['complex_prediction'])\n",
    "precision = precision_score(test_targets, test_data['complex_prediction'])\n",
    "recall = recall_score(test_targets, test_data['complex_prediction'])\n",
    "f1 = f1_score(test_targets, test_data['complex_prediction'])\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 Score: \", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70df9668-6ae2-40aa-995e-a5cf552b201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code for returning word feature for MWEs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the pickled DataFrame from the .pkl file\n",
    "data_frame = pd.read_pickle('final_camb_feats/WikiNews_Train_actual')\n",
    "\n",
    "# Save the DataFrame as a .csv file\n",
    "data_frame.to_csv('final_camb_feats/inspect.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b22be288-6d07-411c-ac3f-f7c4a1cda44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populating word Features\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import string\n",
    "import regex as re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Read the .pkl file into a DataFrame\n",
    "file_path = 'cwishareddataset/traindevset/english/pickled-dataframes/News_Dev.pkl'\n",
    "data_frame = pd.read_pickle(file_path)\n",
    "\n",
    "\n",
    "data_frame.columns = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "\n",
    "\n",
    "# Perform data processing\n",
    "data_frame['split'] = data_frame['phrase'].apply(lambda x: x.split())\n",
    "data_frame['count'] = data_frame['split'].apply(lambda x: len(x))\n",
    "words = data_frame[data_frame['count'] == 1]\n",
    "MWEs = data_frame[data_frame['count'] >1]\n",
    "word_set = words.phrase.str.lower().unique()\n",
    "word_set = pd.DataFrame(word_set, columns=['phrase'])\n",
    "remove = string.punctuation.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "pattern = r\"[{}]\".format(remove)\n",
    "word_set['phrase'] = word_set['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6402a2ba-7de6-437d-b86e-4a3f07e103b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#function to obtain syablles for words\n",
    "from datamuse import datamuse\n",
    "api = datamuse.Datamuse()\n",
    "\n",
    "def get_syllables(word):\n",
    "    syllables = 0\n",
    "    word_results = api.words(sp=word, max=1, md='psf')\n",
    "    if len(word_results)>0: \n",
    "        word = word_results[0][\"word\"]\n",
    "        syllables = int(word_results[0][\"numSyllables\"])\n",
    "    return syllables\n",
    "\n",
    "# #Apply function to get syllables\n",
    "# word_set['syllables'] = word_set['phrase'].apply(lambda x: get_syllables(x))\n",
    "\n",
    "# #Apply function to get word length \n",
    "# word_set['length'] = word_set['phrase'].apply(lambda x: len(x))\n",
    "\n",
    "# #take words and merge with values first you will need to clean the phrase column \n",
    "# words['original phrase'] = words['phrase']\n",
    "# words['phrase'] = words['phrase'].str.lower()\n",
    "# words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "word_features = pd.merge(words, word_set)\n",
    "\n",
    "#Now parse\n",
    "import pycorenlp\n",
    "import pandas as pd\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "sentences = data_frame[['sentence', 'ID']].copy()\n",
    "\n",
    "sentences = sentences.drop_duplicates()\n",
    "\n",
    "def removefirsttoken(x):\n",
    "    x = x.split(' ', 1)[1]\n",
    "    return x\n",
    "\n",
    "# if Wikinews:\n",
    "#     sentences['clean sentence'] = sentences['sentence'].apply(lambda x: removefirsttoken(x))\n",
    "\n",
    "\n",
    "sentences['clean sentence'] = sentences['sentence']\n",
    "\n",
    "#function to parse sentences \n",
    "def parse(string):\n",
    "    output = nlp.annotate(string, properties={\n",
    "  'annotators': 'pos,depparse',\n",
    "  'outputFormat': 'json'\n",
    "  })\n",
    "    return output\n",
    "\n",
    "#apply parsing to sentences\n",
    "sentences['parse'] = sentences['clean sentence'].apply(lambda x: parse(x))\n",
    "\n",
    "sentences\n",
    "\n",
    "#Merge \n",
    "word_parse_features = pd.merge(sentences, word_features)\n",
    "word_parse_features\n",
    "\n",
    "def get_pos(row):\n",
    "    word = row['phrase']\n",
    "    parse = json.loads(row['parse'])\n",
    "    for i in range(len(parse['sentences'][0]['tokens'])):\n",
    "        comp_word = parse['sentences'][0]['tokens'][i]['word']\n",
    "        comp_word = comp_word.lower()\n",
    "        comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "        if comp_word == word:\n",
    "            return parse['sentences'][0]['tokens'][i]['pos']\n",
    "\n",
    "\n",
    "def get_dep(row):\n",
    "    number = 0\n",
    "    word = row['phrase']\n",
    "    parse = json.loads(row['parse'])\n",
    "    for i in range(len(parse['sentences'][0]['basicDependencies'])):\n",
    "        comp_word = parse['sentences'][0]['basicDependencies'][i]['governorGloss']\n",
    "        comp_word = comp_word.lower()\n",
    "        comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "\n",
    "        if comp_word == word:\n",
    "            number += 1\n",
    "\n",
    "    return number\n",
    "\n",
    "#Function to get the proper lemma \n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    from nltk.corpus import wordnet\n",
    "\n",
    "    if treebank_tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatiser(row):\n",
    "\n",
    "    word = row['phrase']\n",
    "    pos = row['pos']\n",
    "\n",
    "    try:\n",
    "        lemma = wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        return lemma\n",
    "    except:\n",
    "        try:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "            return lemma\n",
    "        except:\n",
    "            print(word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mrc_features = pd.read_csv('corpus/MRC.csv', names=('id', 'NPHN', 'KFFRQ', 'KFCAT', 'KFSMP', 'T-LFRQ', 'FAM', 'CNC', 'IMG', 'AOA', 'word'), low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def aoa(word):\n",
    "    word = word.upper()  # Convert word to all capitals\n",
    "    try:\n",
    "        df = mrc_features.loc[mrc_features['word'] == word]\n",
    "        fvalue = df.iloc[0]['AOA']\n",
    "        return fvalue    \n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def CNC_fun(word):\n",
    "    word = word.upper()\n",
    "    table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "    if len(table)>0:\n",
    "\n",
    "        CNC = table['CNC'].values[0]\n",
    "        CNC = int(CNC)\n",
    "\n",
    "        return CNC\n",
    "    else: \n",
    "        y=0\n",
    "        return y\n",
    "\n",
    "def img(word):\n",
    "    word = word.upper()\n",
    "    try:\n",
    "        df = mrc_features.loc[mrc_features['word'] == word]\n",
    "        fvalue = df.iloc[0]['IMG']\n",
    "        return fvalue    \n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def KFCAT_fun(word):\n",
    "        word = word.upper()\n",
    "        table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        if len(table)>0:\n",
    "\n",
    "            KFCAT = table['KFCAT'].values[0]\n",
    "            KFCAT = int(KFCAT)\n",
    "\n",
    "            return KFCAT\n",
    "        else: \n",
    "            y=0\n",
    "            return y\n",
    "\n",
    "def FAM_fun(word):\n",
    "        word = word.upper()\n",
    "        table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        if len(table)>0:\n",
    "\n",
    "            FAM = table['FAM'].values[0]\n",
    "            FAM = int(FAM)\n",
    "\n",
    "            return FAM\n",
    "        else: \n",
    "            y=0\n",
    "            return y\n",
    "\n",
    "def KFSMP_fun(word):\n",
    "        word = word.upper()\n",
    "        table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        if len(table)>0:\n",
    "\n",
    "            KFSMP = table['KFSMP'].values[0]\n",
    "            KFSMP = int(KFSMP)\n",
    "\n",
    "            return KFSMP\n",
    "        else: \n",
    "            y=0\n",
    "            return y\n",
    "\n",
    "def KFFRQ_fun(word):\n",
    "        word = word.upper()\n",
    "        table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        if len(table)>0:\n",
    "\n",
    "            KFFRQ = table['KFFRQ'].values[0]\n",
    "            KFFRQ = int(KFFRQ)\n",
    "\n",
    "            return KFFRQ\n",
    "        else: \n",
    "            y=0\n",
    "            return y\n",
    "\n",
    "# def NLET_fun(word):\n",
    "#         word = word.upper()\n",
    "#         table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "#         if len(table)>0:\n",
    "\n",
    "\n",
    "#             NLET = table['NLET'].values[0]\n",
    "#             NLET = int(NLET)\n",
    "\n",
    "#             return NLET\n",
    "#         else: \n",
    "#             y=0\n",
    "#             return y\n",
    "\n",
    "def NPHN_fun(word):\n",
    "        word = word.upper()\n",
    "        table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        if len(table)>0:\n",
    "\n",
    "            NPHN = table['NPHN'].values[0]\n",
    "            NPHN = int(NPHN)\n",
    "\n",
    "            return NPHN\n",
    "        else: \n",
    "            y=0\n",
    "            return y\n",
    "\n",
    "def TLFRQ_fun(word):\n",
    "        word = word.upper()\n",
    "        table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        if len(table)>0:\n",
    "\n",
    "            TLFRQ = table['T-LFRQ'].values[0]\n",
    "            TLFRQ = int(TLFRQ)\n",
    "\n",
    "            return TLFRQ\n",
    "        else: \n",
    "            y=0\n",
    "            return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9750ba3a-2230-47a7-8b41-af507022df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "2#\n",
    "\n",
    "# Create an empty dictionary from the words in the 'split' column of MWEs dataframe\n",
    "word_dict = {word: None for sublist in MWEs['split'] for word in sublist}\n",
    "def create_dict(row):\n",
    "    return {word: None for word in row}\n",
    "\n",
    "# Create a copy of the MWEs dataframe to avoid SettingWithCopyWarning\n",
    "MWEs_copy = MWEs.copy()\n",
    "\n",
    "MWEs_copy['feat_dict'] = MWEs_copy['split'].apply(create_dict)\n",
    "\n",
    "\n",
    "def create_dict(row):\n",
    "    word_dict = {word: None for word in row['split']}\n",
    "    \n",
    "    return word_dict\n",
    "\n",
    "# Apply the function to each row in the MWEs dataframe\n",
    "MWEs_copy = MWEs.copy()\n",
    "MWEs_copy['feat_dict'] = MWEs_copy.apply(create_dict, axis=1)\n",
    "\n",
    "def update_dict(row):\n",
    "    for word in row['split']:\n",
    "        word_dict = row['feat_dict']\n",
    "        word_dict[word] = {\n",
    "            'AOA': aoa(word),\n",
    "            'CNC': CNC_fun(word),\n",
    "            'IMG': img(word),\n",
    "            'KFCAT': KFCAT_fun(word),\n",
    "            'FAM': FAM_fun(word),\n",
    "            'KFSMP': KFSMP_fun(word),\n",
    "            'KFFRQ': KFFRQ_fun(word),\n",
    "            'NPHN': NPHN_fun(word),\n",
    "            'TLFRQ': TLFRQ_fun(word)\n",
    "        }\n",
    "    return word_dict\n",
    "\n",
    "MWEs_copy['feat_dict'] = MWEs_copy.apply(update_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93807a13-5116-4efd-b746-69f68566b72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sparking': {'AOA': '0', 'CNC': 0, 'IMG': '0', 'KFCAT': 0, 'FAM': 0, 'KFSMP': 0, 'KFFRQ': 0, 'NPHN': 0, 'TLFRQ': 0}, 'intense': {'AOA': '0', 'CNC': 361, 'IMG': '340', 'KFCAT': 10, 'FAM': 490, 'KFSMP': 37, 'KFFRQ': 40, 'NPHN': 0, 'TLFRQ': 94}}\n",
      "\n",
      "{'sparking': {'AOA': '0', 'CNC': 0, 'IMG': '0', 'KFCAT': 0, 'FAM': 0, 'KFSMP': 0, 'KFFRQ': 0, 'NPHN': 0, 'TLFRQ': 0}, 'intense': {'AOA': '0', 'CNC': 361, 'IMG': '340', 'KFCAT': 10, 'FAM': 490, 'KFSMP': 37, 'KFFRQ': 40, 'NPHN': 0, 'TLFRQ': 94}, 'clashes': {'AOA': '0', 'CNC': 0, 'IMG': '0', 'KFCAT': 0, 'FAM': 0, 'KFSMP': 0, 'KFFRQ': 0, 'NPHN': 0, 'TLFRQ': 0}}\n",
      "\n",
      "{'intense': {'AOA': '0', 'CNC': 361, 'IMG': '340', 'KFCAT': 10, 'FAM': 490, 'KFSMP': 37, 'KFFRQ': 40, 'NPHN': 0, 'TLFRQ': 94}, 'clashes': {'AOA': '0', 'CNC': 0, 'IMG': '0', 'KFCAT': 0, 'FAM': 0, 'KFSMP': 0, 'KFFRQ': 0, 'NPHN': 0, 'TLFRQ': 0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(MWEs_copy['feat_dict'].iloc[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cea41b8e-1849-4988-acf6-3fc891233ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_index</th>\n",
       "      <th>end_index</th>\n",
       "      <th>phrase</th>\n",
       "      <th>total_native</th>\n",
       "      <th>total_non_native</th>\n",
       "      <th>native_complex</th>\n",
       "      <th>non_native_complex</th>\n",
       "      <th>complex_binary</th>\n",
       "      <th>complex_probabilistic</th>\n",
       "      <th>split</th>\n",
       "      <th>count</th>\n",
       "      <th>feat_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>51</td>\n",
       "      <td>67</td>\n",
       "      <td>sparking intense</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[sparking, intense]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'sparking': None, 'intense': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>51</td>\n",
       "      <td>75</td>\n",
       "      <td>sparking intense clashes</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[sparking, intense, clashes]</td>\n",
       "      <td>3</td>\n",
       "      <td>{'sparking': None, 'intense': None, 'clashes':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "      <td>intense clashes</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[intense, clashes]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'intense': None, 'clashes': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>86</td>\n",
       "      <td>102</td>\n",
       "      <td>bloodied victims</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[bloodied, victims]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'bloodied': None, 'victims': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>The violence in Rastan, in the restive central...</td>\n",
       "      <td>109</td>\n",
       "      <td>133</td>\n",
       "      <td>internationally brokered</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[internationally, brokered]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'internationally': None, 'brokered': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>Banks have until the end of the year to move t...</td>\n",
       "      <td>89</td>\n",
       "      <td>111</td>\n",
       "      <td>asset management firms</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[asset, management, firms]</td>\n",
       "      <td>3</td>\n",
       "      <td>{'asset': None, 'management': None, 'firms': N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>Banks have until the end of the year to move t...</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>fire sale</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[fire, sale]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'fire': None, 'sale': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>Banks have until the end of the year to move t...</td>\n",
       "      <td>146</td>\n",
       "      <td>161</td>\n",
       "      <td>Luis de Guindos</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Luis, de, Guindos]</td>\n",
       "      <td>3</td>\n",
       "      <td>{'Luis': None, 'de': None, 'Guindos': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>The state will put less than 15 billion euros ...</td>\n",
       "      <td>83</td>\n",
       "      <td>95</td>\n",
       "      <td>bank rescues</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[bank, rescues]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'bank': None, 'rescues': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>The state will put less than 15 billion euros ...</td>\n",
       "      <td>146</td>\n",
       "      <td>156</td>\n",
       "      <td>de Guindos</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[de, Guindos]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'de': None, 'Guindos': None}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID  \\\n",
       "6     3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "7     3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "9     3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "12    3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "28    3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "...                              ...   \n",
       "1732  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "1733  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "1736  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "1752  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "1757  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "\n",
       "                                               sentence  start_index  \\\n",
       "6     Syrian troops shelled a rebel-held town on Mon...           51   \n",
       "7     Syrian troops shelled a rebel-held town on Mon...           51   \n",
       "9     Syrian troops shelled a rebel-held town on Mon...           60   \n",
       "12    Syrian troops shelled a rebel-held town on Mon...           86   \n",
       "28    The violence in Rastan, in the restive central...          109   \n",
       "...                                                 ...          ...   \n",
       "1732  Banks have until the end of the year to move t...           89   \n",
       "1733  Banks have until the end of the year to move t...          118   \n",
       "1736  Banks have until the end of the year to move t...          146   \n",
       "1752  The state will put less than 15 billion euros ...           83   \n",
       "1757  The state will put less than 15 billion euros ...          146   \n",
       "\n",
       "      end_index                    phrase  total_native  total_non_native  \\\n",
       "6            67          sparking intense            10                10   \n",
       "7            75  sparking intense clashes            10                10   \n",
       "9            75           intense clashes            10                10   \n",
       "12          102          bloodied victims            10                10   \n",
       "28          133  internationally brokered            10                10   \n",
       "...         ...                       ...           ...               ...   \n",
       "1732        111    asset management firms            10                10   \n",
       "1733        127                 fire sale            10                10   \n",
       "1736        161           Luis de Guindos            10                10   \n",
       "1752         95              bank rescues            10                10   \n",
       "1757        156                de Guindos            10                10   \n",
       "\n",
       "      native_complex  non_native_complex  complex_binary  \\\n",
       "6                  0                   1               1   \n",
       "7                  1                   1               1   \n",
       "9                  0                   2               1   \n",
       "12                 0                   1               1   \n",
       "28                 1                   0               1   \n",
       "...              ...                 ...             ...   \n",
       "1732               4                   0               1   \n",
       "1733               3                   1               1   \n",
       "1736               0                   0               0   \n",
       "1752               0                   0               0   \n",
       "1757               0                   0               0   \n",
       "\n",
       "      complex_probabilistic                         split  count  \\\n",
       "6                      0.05           [sparking, intense]      2   \n",
       "7                      0.10  [sparking, intense, clashes]      3   \n",
       "9                      0.10            [intense, clashes]      2   \n",
       "12                     0.05           [bloodied, victims]      2   \n",
       "28                     0.05   [internationally, brokered]      2   \n",
       "...                     ...                           ...    ...   \n",
       "1732                   0.20    [asset, management, firms]      3   \n",
       "1733                   0.20                  [fire, sale]      2   \n",
       "1736                   0.00           [Luis, de, Guindos]      3   \n",
       "1752                   0.00               [bank, rescues]      2   \n",
       "1757                   0.00                 [de, Guindos]      2   \n",
       "\n",
       "                                              feat_dict  \n",
       "6                   {'sparking': None, 'intense': None}  \n",
       "7     {'sparking': None, 'intense': None, 'clashes':...  \n",
       "9                    {'intense': None, 'clashes': None}  \n",
       "12                  {'bloodied': None, 'victims': None}  \n",
       "28          {'internationally': None, 'brokered': None}  \n",
       "...                                                 ...  \n",
       "1732  {'asset': None, 'management': None, 'firms': N...  \n",
       "1733                       {'fire': None, 'sale': None}  \n",
       "1736        {'Luis': None, 'de': None, 'Guindos': None}  \n",
       "1752                    {'bank': None, 'rescues': None}  \n",
       "1757                      {'de': None, 'Guindos': None}  \n",
       "\n",
       "[262 rows x 14 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MWEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba236a-0d5d-404c-9445-8e5a19c15b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add the necessary imports at the top of your script\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "# Initiate the CoreNLP server connection\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "\n",
    "# Define a function to update the dictionary with these new features\n",
    "def update_dict_with_parsed_data(row):\n",
    "    word_dict = row['feat_dict']\n",
    "    parse_data = row['parse']  # Use the 'parse' column from the DataFrame\n",
    "    \n",
    "    # If the parse data is a string in JSON format, convert it to a dictionary\n",
    "    if isinstance(parse_data, str):\n",
    "        parse_data = json.loads(parse_data)\n",
    "    \n",
    "    for word in row['split']:\n",
    "        # Get the existing word features\n",
    "        word_features = word_dict[word]\n",
    "        \n",
    "        # Update the dictionary with the additional features\n",
    "        word_features.update({\n",
    "            'AOA': aoa(word),\n",
    "            'CNC': CNC_fun(word),\n",
    "            'IMG': img(word),\n",
    "            'KFCAT': KFCAT_fun(word),\n",
    "            'FAM': FAM_fun(word),\n",
    "            'KFSMP': KFSMP_fun(word),\n",
    "            'KFFRQ': KFFRQ_fun(word),\n",
    "            'NPHN': NPHN_fun(word),\n",
    "            'TLFRQ': TLFRQ_fun(word),\n",
    "            'parse': parse_data,  # This is now the parse data from the DataFrame\n",
    "            'pos': get_pos(row),\n",
    "            'dep': get_dep(row),\n",
    "            'lemma': lemmatiser(row)\n",
    "        })\n",
    "        \n",
    "    return word_dict\n",
    "\n",
    "# Apply this function to each row in the DataFrame\n",
    "MWEs_copy['feat_dict'] = MWEs_copy.apply(update_dict_with_parsed_data, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b008109e-ae6c-42de-bc2f-f0f032dc7303",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m mwe \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m word_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmwe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m():\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Create an empty dictionary for the word\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     word_dict[word] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     42\u001b[0m word_dicts\u001b[38;5;241m.\u001b[39mappend(word_dict)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import regex as re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Read the .pkl file into a DataFrame\n",
    "file_path = 'cwishareddataset/traindevset/english/pickled-dataframes/News_Dev.pkl'\n",
    "data_frame = pd.read_pickle(file_path)\n",
    "\n",
    "data_frame.columns = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "\n",
    "# Perform data processing\n",
    "data_frame['split'] = data_frame['phrase'].apply(lambda x: x.split())\n",
    "data_frame['count'] = data_frame['split'].apply(lambda x: len(x))\n",
    "\n",
    "words = data_frame[data_frame['count'] == 1]\n",
    "MWEs = data_frame[data_frame['count'] > 1]\n",
    "\n",
    "word_set = words.phrase.str.lower().unique()\n",
    "word_set = pd.DataFrame(word_set, columns=['phrase'])\n",
    "\n",
    "remove = string.punctuation.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "pattern = r\"[{}]\".format(remove)\n",
    "word_set['phrase'] = word_set['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "# Create an empty dictionary for each word in MWEs DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "word_dicts = []\n",
    "\n",
    "for _, row in MWEs.iterrows():\n",
    "    mwe = row['split']\n",
    "    word_dict = {}\n",
    "    for word in mwe.split():\n",
    "        # Create an empty dictionary for the word\n",
    "        word_dict[word] = {}\n",
    "    \n",
    "    word_dicts.append(word_dict)\n",
    "\n",
    "keys = [\n",
    "    \"parse\",\n",
    "    \"total_non_native\",\n",
    "    \"native_complex\",\n",
    "    \"IMG\",\n",
    "    \"sub_imdb\",\n",
    "    \"google frequency\",\n",
    "    \"KFCAT\",\n",
    "    \"FAM\",\n",
    "    \"KFSMP\",\n",
    "    \"KFFRQ\",\n",
    "    \"AOA\",\n",
    "    \"NPHN\",\n",
    "    \"T-LFRQ\"\n",
    "]\n",
    "\n",
    "result = []\n",
    "\n",
    "for word_dict in word_dicts:\n",
    "    word_result = {}\n",
    "    for word in word_dict:\n",
    "        word_dict[word] = {key: \"\" for key in keys}\n",
    "        word_result[word] = word_dict[word]\n",
    "    result.append(word_result)\n",
    "\n",
    "print(result)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "789edb82-c338-403a-8939-593dfaaf9ada",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (611975966.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[61], line 24\u001b[0;36m\u001b[0m\n\u001b[0;31m    if row['split']\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named MWEs with a column named \"split\"\n",
    "\n",
    "keys = [\n",
    "    \"parse\",\n",
    "    \"total_non_native\",\n",
    "    \"native_complex\",\n",
    "    \"IMG\",\n",
    "    \"sub_imdb\",\n",
    "    \"google frequency\",\n",
    "    \"KFCAT\",\n",
    "    \"FAM\",\n",
    "    \"KFSMP\",\n",
    "    \"KFFRQ\",\n",
    "    \"AOA\",\n",
    "    \"NPHN\",\n",
    "    \"T-LFRQ\"\n",
    "]\n",
    "\n",
    "word_dicts = []\n",
    "\n",
    "for _, row in MWEs.iterrows():\n",
    "    if row['split']\n",
    "        mwe = row['split']\n",
    "        for word in mwe.split():\n",
    "            word_dict = {key: \"\" for key in keys}\n",
    "            word_dicts.append({word: word_dict})\n",
    "\n",
    "print(word_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c74b284-97d9-4fa3-8722-a3313a5d8481",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_native\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_non_native\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnative_complex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_imdb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle frequency\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKFCAT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKFSMP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKFFRQ\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAOA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNPHN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT-LFRQ\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_dicts:\n\u001b[0;32m----> 6\u001b[0m     word_dicts[word] \u001b[38;5;241m=\u001b[39m {feature: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features}\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Update the \"word_dict\" column of the dataframe\u001b[39;00m\n\u001b[1;32m      8\u001b[0m MWEs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_dict\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m MWEs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: word_dict \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m word_dict \u001b[38;5;28;01melse\u001b[39;00m x)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not dict"
     ]
    }
   ],
   "source": [
    "features = ['parse', 'phrase', 'total_native', 'total_non_native',\n",
    "            'native_complex', 'IMG', 'sub_imdb', 'google frequency', 'KFCAT', 'FAM', 'KFSMP', 'KFFRQ',\n",
    "            'AOA', 'NPHN', 'T-LFRQ']\n",
    "\n",
    "for word in word_dicts:\n",
    "    word_dicts[word] = {feature: None for feature in features}\n",
    "# Update the \"word_dict\" column of the dataframe\n",
    "MWEs['word_dict'] = MWEs['word_dict'].apply(lambda x: word_dict if x == word_dict else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaa7cc26-6ef5-4c8c-9162-fb3e4fd43e65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_native\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_non_native\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnative_complex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_imdb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle frequency\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKFCAT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKFSMP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKFFRQ\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAOA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNPHN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT-LFRQ\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_dicts:\n\u001b[0;32m---> 10\u001b[0m     word_dicts[word] \u001b[38;5;241m=\u001b[39m {feature: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features}\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not dict"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming MWEs is the dataframe with a column called \"word_dict\"\n",
    "# And word_dict is the modified dictionary\n",
    "features = ['parse', 'phrase', 'total_native', 'total_non_native',\n",
    "            'native_complex', 'IMG', 'sub_imdb', 'google frequency', 'KFCAT', 'FAM', 'KFSMP', 'KFFRQ',\n",
    "            'AOA', 'NPHN', 'T-LFRQ']\n",
    "\n",
    "for word in word_dicts:\n",
    "    word_dicts[word] = {feature: None for feature in features}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ab124d9-b179-438a-8532-14177a2a7699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start_index</th>\n",
       "      <th>end_index</th>\n",
       "      <th>phrase</th>\n",
       "      <th>total_native</th>\n",
       "      <th>total_non_native</th>\n",
       "      <th>native_complex</th>\n",
       "      <th>non_native_complex</th>\n",
       "      <th>complex_binary</th>\n",
       "      <th>complex_probabilistic</th>\n",
       "      <th>split</th>\n",
       "      <th>count</th>\n",
       "      <th>word_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>51</td>\n",
       "      <td>67</td>\n",
       "      <td>sparking intense</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[sparking, intense]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'sparking': {}, 'intense': {}, 'parse': None,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>51</td>\n",
       "      <td>75</td>\n",
       "      <td>sparking intense clashes</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[sparking, intense, clashes]</td>\n",
       "      <td>3</td>\n",
       "      <td>{'sparking': {}, 'intense': {}, 'clashes': {},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>60</td>\n",
       "      <td>75</td>\n",
       "      <td>intense clashes</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[intense, clashes]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'intense': {}, 'clashes': {}, 'parse': None, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>Syrian troops shelled a rebel-held town on Mon...</td>\n",
       "      <td>86</td>\n",
       "      <td>102</td>\n",
       "      <td>bloodied victims</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[bloodied, victims]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'bloodied': {}, 'victims': {}, 'parse': None,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3Z8UJEJOCZEG603II1EL4BE2PV593A</td>\n",
       "      <td>The violence in Rastan, in the restive central...</td>\n",
       "      <td>109</td>\n",
       "      <td>133</td>\n",
       "      <td>internationally brokered</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[internationally, brokered]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'internationally': {}, 'brokered': {}, 'parse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>Banks have until the end of the year to move t...</td>\n",
       "      <td>89</td>\n",
       "      <td>111</td>\n",
       "      <td>asset management firms</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[asset, management, firms]</td>\n",
       "      <td>3</td>\n",
       "      <td>{'asset': {}, 'management': {}, 'firms': {}, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>Banks have until the end of the year to move t...</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>fire sale</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[fire, sale]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'fire': {}, 'sale': {}, 'parse': None, 'phras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>Banks have until the end of the year to move t...</td>\n",
       "      <td>146</td>\n",
       "      <td>161</td>\n",
       "      <td>Luis de Guindos</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Luis, de, Guindos]</td>\n",
       "      <td>3</td>\n",
       "      <td>{'Luis': {}, 'de': {}, 'Guindos': {}, 'parse':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>The state will put less than 15 billion euros ...</td>\n",
       "      <td>83</td>\n",
       "      <td>95</td>\n",
       "      <td>bank rescues</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[bank, rescues]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'bank': {}, 'rescues': {}, 'parse': None, 'ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>37PGLWGSJT7FDZ4S71CXYQU7J7EIKD</td>\n",
       "      <td>The state will put less than 15 billion euros ...</td>\n",
       "      <td>146</td>\n",
       "      <td>156</td>\n",
       "      <td>de Guindos</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[de, Guindos]</td>\n",
       "      <td>2</td>\n",
       "      <td>{'de': {'parse': None, 'phrase': None, 'total_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ID  \\\n",
       "6     3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "7     3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "9     3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "12    3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "28    3Z8UJEJOCZEG603II1EL4BE2PV593A   \n",
       "...                              ...   \n",
       "1732  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "1733  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "1736  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "1752  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "1757  37PGLWGSJT7FDZ4S71CXYQU7J7EIKD   \n",
       "\n",
       "                                               sentence  start_index  \\\n",
       "6     Syrian troops shelled a rebel-held town on Mon...           51   \n",
       "7     Syrian troops shelled a rebel-held town on Mon...           51   \n",
       "9     Syrian troops shelled a rebel-held town on Mon...           60   \n",
       "12    Syrian troops shelled a rebel-held town on Mon...           86   \n",
       "28    The violence in Rastan, in the restive central...          109   \n",
       "...                                                 ...          ...   \n",
       "1732  Banks have until the end of the year to move t...           89   \n",
       "1733  Banks have until the end of the year to move t...          118   \n",
       "1736  Banks have until the end of the year to move t...          146   \n",
       "1752  The state will put less than 15 billion euros ...           83   \n",
       "1757  The state will put less than 15 billion euros ...          146   \n",
       "\n",
       "      end_index                    phrase  total_native  total_non_native  \\\n",
       "6            67          sparking intense            10                10   \n",
       "7            75  sparking intense clashes            10                10   \n",
       "9            75           intense clashes            10                10   \n",
       "12          102          bloodied victims            10                10   \n",
       "28          133  internationally brokered            10                10   \n",
       "...         ...                       ...           ...               ...   \n",
       "1732        111    asset management firms            10                10   \n",
       "1733        127                 fire sale            10                10   \n",
       "1736        161           Luis de Guindos            10                10   \n",
       "1752         95              bank rescues            10                10   \n",
       "1757        156                de Guindos            10                10   \n",
       "\n",
       "      native_complex  non_native_complex  complex_binary  \\\n",
       "6                  0                   1               1   \n",
       "7                  1                   1               1   \n",
       "9                  0                   2               1   \n",
       "12                 0                   1               1   \n",
       "28                 1                   0               1   \n",
       "...              ...                 ...             ...   \n",
       "1732               4                   0               1   \n",
       "1733               3                   1               1   \n",
       "1736               0                   0               0   \n",
       "1752               0                   0               0   \n",
       "1757               0                   0               0   \n",
       "\n",
       "      complex_probabilistic                         split  count  \\\n",
       "6                      0.05           [sparking, intense]      2   \n",
       "7                      0.10  [sparking, intense, clashes]      3   \n",
       "9                      0.10            [intense, clashes]      2   \n",
       "12                     0.05           [bloodied, victims]      2   \n",
       "28                     0.05   [internationally, brokered]      2   \n",
       "...                     ...                           ...    ...   \n",
       "1732                   0.20    [asset, management, firms]      3   \n",
       "1733                   0.20                  [fire, sale]      2   \n",
       "1736                   0.00           [Luis, de, Guindos]      3   \n",
       "1752                   0.00               [bank, rescues]      2   \n",
       "1757                   0.00                 [de, Guindos]      2   \n",
       "\n",
       "                                              word_dict  \n",
       "6     {'sparking': {}, 'intense': {}, 'parse': None,...  \n",
       "7     {'sparking': {}, 'intense': {}, 'clashes': {},...  \n",
       "9     {'intense': {}, 'clashes': {}, 'parse': None, ...  \n",
       "12    {'bloodied': {}, 'victims': {}, 'parse': None,...  \n",
       "28    {'internationally': {}, 'brokered': {}, 'parse...  \n",
       "...                                                 ...  \n",
       "1732  {'asset': {}, 'management': {}, 'firms': {}, '...  \n",
       "1733  {'fire': {}, 'sale': {}, 'parse': None, 'phras...  \n",
       "1736  {'Luis': {}, 'de': {}, 'Guindos': {}, 'parse':...  \n",
       "1752  {'bank': {}, 'rescues': {}, 'parse': None, 'ph...  \n",
       "1757  {'de': {'parse': None, 'phrase': None, 'total_...  \n",
       "\n",
       "[262 rows x 14 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MWEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc921235-5331-4e01-bc61-294669ce7b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 6:\n",
      "{'sparking': {}, 'intense': {}, 'parse': None, 'phrase': None, 'total_native': None, 'total_non_native': None, 'native_complex': None, 'IMG': None, 'sub_imdb': None, 'google frequency': None, 'KFCAT': None, 'FAM': None, 'KFSMP': None, 'KFFRQ': None, 'AOA': None, 'NPHN': None, 'T-LFRQ': None}\n",
      "\n",
      "Row 7:\n",
      "{'sparking': {}, 'intense': {}, 'clashes': {}, 'parse': None, 'phrase': None, 'total_native': None, 'total_non_native': None, 'native_complex': None, 'IMG': None, 'sub_imdb': None, 'google frequency': None, 'KFCAT': None, 'FAM': None, 'KFSMP': None, 'KFFRQ': None, 'AOA': None, 'NPHN': None, 'T-LFRQ': None}\n",
      "\n",
      "Row 9:\n",
      "{'intense': {}, 'clashes': {}, 'parse': None, 'phrase': None, 'total_native': None, 'total_non_native': None, 'native_complex': None, 'IMG': None, 'sub_imdb': None, 'google frequency': None, 'KFCAT': None, 'FAM': None, 'KFSMP': None, 'KFFRQ': None, 'AOA': None, 'NPHN': None, 'T-LFRQ': None}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_91294/707534530.py:5: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, word_dict in first_three.iteritems():\n"
     ]
    }
   ],
   "source": [
    "# Get the first 3 rows\n",
    "first_three = MWEs['word_dict'].head(3)\n",
    "\n",
    "# Iterate over the dictionaries and print them\n",
    "for idx, word_dict in first_three.iteritems():\n",
    "    print(f\"Row {idx}:\\n{word_dict}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92978cca-846a-4efa-8b52-3ddbf726bc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>ID</th>\n",
       "      <th>clean sentence</th>\n",
       "      <th>parse</th>\n",
       "      <th>start_index</th>\n",
       "      <th>end_index</th>\n",
       "      <th>phrase</th>\n",
       "      <th>total_native</th>\n",
       "      <th>total_non_native</th>\n",
       "      <th>native_complex</th>\n",
       "      <th>...</th>\n",
       "      <th>IMG</th>\n",
       "      <th>sub_imdb</th>\n",
       "      <th>google frequency</th>\n",
       "      <th>KFCAT</th>\n",
       "      <th>FAM</th>\n",
       "      <th>KFSMP</th>\n",
       "      <th>KFFRQ</th>\n",
       "      <th>AOA</th>\n",
       "      <th>NPHN</th>\n",
       "      <th>T-LFRQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#37-1 Guatemalan Supreme Court approves impeac...</td>\n",
       "      <td>3QREJ3J433YH30CYS49AQ6MZ3G0LKZ</td>\n",
       "      <td>Guatemalan Supreme Court approves impeachment ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>31</td>\n",
       "      <td>39</td>\n",
       "      <td>approves</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.132012</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#37-1 Guatemalan Supreme Court approves impeac...</td>\n",
       "      <td>3QREJ3J433YH30CYS49AQ6MZ3G0LKZ</td>\n",
       "      <td>Guatemalan Supreme Court approves impeachment ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>supreme</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.112747</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#37-1 Guatemalan Supreme Court approves impeac...</td>\n",
       "      <td>3QREJ3J433YH30CYS49AQ6MZ3G0LKZ</td>\n",
       "      <td>Guatemalan Supreme Court approves impeachment ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>100</td>\n",
       "      <td>107</td>\n",
       "      <td>supreme</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.112747</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#37-1 Guatemalan Supreme Court approves impeac...</td>\n",
       "      <td>3QREJ3J433YH30CYS49AQ6MZ3G0LKZ</td>\n",
       "      <td>Guatemalan Supreme Court approves impeachment ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>court</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "      <td>122.433514</td>\n",
       "      <td>13</td>\n",
       "      <td>549</td>\n",
       "      <td>64</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#37-1 Guatemalan Supreme Court approves impeac...</td>\n",
       "      <td>3QREJ3J433YH30CYS49AQ6MZ3G0LKZ</td>\n",
       "      <td>Guatemalan Supreme Court approves impeachment ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>108</td>\n",
       "      <td>113</td>\n",
       "      <td>court</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "      <td>122.433514</td>\n",
       "      <td>13</td>\n",
       "      <td>549</td>\n",
       "      <td>64</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>#36-6 At 9 P.M. EST, INDYCAR released a statem...</td>\n",
       "      <td>344M16OZKIG450N98VEM53DJOE4ENA</td>\n",
       "      <td>At 9 P.M. EST, INDYCAR released a statement, a...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>89</td>\n",
       "      <td>95</td>\n",
       "      <td>severe</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>352</td>\n",
       "      <td>0</td>\n",
       "      <td>47.073987</td>\n",
       "      <td>12</td>\n",
       "      <td>526</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6778</th>\n",
       "      <td>#36-6 At 9 P.M. EST, INDYCAR released a statem...</td>\n",
       "      <td>344M16OZKIG450N98VEM53DJOE4ENA</td>\n",
       "      <td>At 9 P.M. EST, INDYCAR released a statement, a...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>head</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>593</td>\n",
       "      <td>1</td>\n",
       "      <td>188.298221</td>\n",
       "      <td>15</td>\n",
       "      <td>611</td>\n",
       "      <td>190</td>\n",
       "      <td>424</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>5047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6779</th>\n",
       "      <td>#36-6 At 9 P.M. EST, INDYCAR released a statem...</td>\n",
       "      <td>344M16OZKIG450N98VEM53DJOE4ENA</td>\n",
       "      <td>At 9 P.M. EST, INDYCAR released a statement, a...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>est</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.486005</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6780</th>\n",
       "      <td>#36-6 At 9 P.M. EST, INDYCAR released a statem...</td>\n",
       "      <td>344M16OZKIG450N98VEM53DJOE4ENA</td>\n",
       "      <td>At 9 P.M. EST, INDYCAR released a statement, a...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>116</td>\n",
       "      <td>124</td>\n",
       "      <td>critical</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>76.384984</td>\n",
       "      <td>10</td>\n",
       "      <td>517</td>\n",
       "      <td>42</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6781</th>\n",
       "      <td>#36-6 At 9 P.M. EST, INDYCAR released a statem...</td>\n",
       "      <td>344M16OZKIG450N98VEM53DJOE4ENA</td>\n",
       "      <td>At 9 P.M. EST, INDYCAR released a statement, a...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>143</td>\n",
       "      <td>149</td>\n",
       "      <td>fallen</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>547</td>\n",
       "      <td>1</td>\n",
       "      <td>78.107312</td>\n",
       "      <td>14</td>\n",
       "      <td>572</td>\n",
       "      <td>106</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6782 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     #37-1 Guatemalan Supreme Court approves impeac...   \n",
       "1     #37-1 Guatemalan Supreme Court approves impeac...   \n",
       "2     #37-1 Guatemalan Supreme Court approves impeac...   \n",
       "3     #37-1 Guatemalan Supreme Court approves impeac...   \n",
       "4     #37-1 Guatemalan Supreme Court approves impeac...   \n",
       "...                                                 ...   \n",
       "6777  #36-6 At 9 P.M. EST, INDYCAR released a statem...   \n",
       "6778  #36-6 At 9 P.M. EST, INDYCAR released a statem...   \n",
       "6779  #36-6 At 9 P.M. EST, INDYCAR released a statem...   \n",
       "6780  #36-6 At 9 P.M. EST, INDYCAR released a statem...   \n",
       "6781  #36-6 At 9 P.M. EST, INDYCAR released a statem...   \n",
       "\n",
       "                                  ID  \\\n",
       "0     3QREJ3J433YH30CYS49AQ6MZ3G0LKZ   \n",
       "1     3QREJ3J433YH30CYS49AQ6MZ3G0LKZ   \n",
       "2     3QREJ3J433YH30CYS49AQ6MZ3G0LKZ   \n",
       "3     3QREJ3J433YH30CYS49AQ6MZ3G0LKZ   \n",
       "4     3QREJ3J433YH30CYS49AQ6MZ3G0LKZ   \n",
       "...                              ...   \n",
       "6777  344M16OZKIG450N98VEM53DJOE4ENA   \n",
       "6778  344M16OZKIG450N98VEM53DJOE4ENA   \n",
       "6779  344M16OZKIG450N98VEM53DJOE4ENA   \n",
       "6780  344M16OZKIG450N98VEM53DJOE4ENA   \n",
       "6781  344M16OZKIG450N98VEM53DJOE4ENA   \n",
       "\n",
       "                                         clean sentence  \\\n",
       "0     Guatemalan Supreme Court approves impeachment ...   \n",
       "1     Guatemalan Supreme Court approves impeachment ...   \n",
       "2     Guatemalan Supreme Court approves impeachment ...   \n",
       "3     Guatemalan Supreme Court approves impeachment ...   \n",
       "4     Guatemalan Supreme Court approves impeachment ...   \n",
       "...                                                 ...   \n",
       "6777  At 9 P.M. EST, INDYCAR released a statement, a...   \n",
       "6778  At 9 P.M. EST, INDYCAR released a statement, a...   \n",
       "6779  At 9 P.M. EST, INDYCAR released a statement, a...   \n",
       "6780  At 9 P.M. EST, INDYCAR released a statement, a...   \n",
       "6781  At 9 P.M. EST, INDYCAR released a statement, a...   \n",
       "\n",
       "                                                  parse  start_index  \\\n",
       "0     {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           31   \n",
       "1     {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           17   \n",
       "2     {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...          100   \n",
       "3     {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           25   \n",
       "4     {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...          108   \n",
       "...                                                 ...          ...   \n",
       "6777  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           89   \n",
       "6778  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           96   \n",
       "6779  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           16   \n",
       "6780  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...          116   \n",
       "6781  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...          143   \n",
       "\n",
       "      end_index    phrase  total_native  total_non_native  native_complex  \\\n",
       "0            39  approves            10                10               0   \n",
       "1            24   supreme            10                10               0   \n",
       "2           107   supreme            10                10               0   \n",
       "3            30     court            10                10               0   \n",
       "4           113     court            10                10               0   \n",
       "...         ...       ...           ...               ...             ...   \n",
       "6777         95    severe            10                10               5   \n",
       "6778        100      head            10                10               0   \n",
       "6779         19       est            10                10               0   \n",
       "6780        124  critical            10                10               4   \n",
       "6781        149    fallen            10                10               0   \n",
       "\n",
       "      ...  IMG  sub_imdb  google frequency KFCAT  FAM KFSMP  KFFRQ  AOA NPHN  \\\n",
       "0     ...    0         1          1.132012     6    0    12     14    0    5   \n",
       "1     ...    0         1         31.112747    11    0    33     51    0    0   \n",
       "2     ...    0         1         31.112747    11    0    33     51    0    0   \n",
       "3     ...  552         1        122.433514    13  549    64    230    0    0   \n",
       "4     ...  552         1        122.433514    13  549    64    230    0    0   \n",
       "...   ...  ...       ...               ...   ...  ...   ...    ...  ...  ...   \n",
       "6777  ...  352         0         47.073987    12  526    33     39    0    4   \n",
       "6778  ...  593         1        188.298221    15  611   190    424  181    0   \n",
       "6779  ...    0         1         14.486005     2    0     3      3    0    0   \n",
       "6780  ...  305         1         76.384984    10  517    42     58    0    8   \n",
       "6781  ...  547         1         78.107312    14  572   106    147    0    3   \n",
       "\n",
       "      T-LFRQ  \n",
       "0        171  \n",
       "1        139  \n",
       "2        139  \n",
       "3        701  \n",
       "4        701  \n",
       "...      ...  \n",
       "6777     119  \n",
       "6778    5047  \n",
       "6779       0  \n",
       "6780      60  \n",
       "6781    1000  \n",
       "\n",
       "[6782 rows x 38 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "file_path = 'final_camb_feats/inspect.csv'\n",
    "\n",
    "# Read the CSV data file into a pandas DataFrame\n",
    "data_frame = pd.read_csv(file_path)\n",
    "\n",
    "# Now you can view the DataFrame\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28026b6-9a00-44f8-b361-e3695d1dc345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
