{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populating word Features\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import string\n",
    "import regex as re\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from datamuse import datamuse\n",
    "import pycorenlp\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to stanford-corenlp-4.5.4 folder and start core with “% java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 \n",
    "\n",
    "Download from: https://stanfordnlp.github.io/CoreNLP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['original phrase'] = words['phrase']\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].str.lower()\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:474: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['original phrase'] = words['phrase']\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].str.lower()\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:474: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['original phrase'] = words['phrase']\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].str.lower()\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_31572/2662797667.py:474: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))\n"
     ]
    }
   ],
   "source": [
    "# Initialize Datamuse API and StanfordCoreNLP\n",
    "api = datamuse.Datamuse()\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# Set the paths for the input and output folders\n",
    "folder_path = \"cwishareddataset/testset/english/pickled-dataframes\"\n",
    "output_folder = \"final_camb_feats_Test\"\n",
    "\n",
    "# Iterate over .pkl files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pkl'):\n",
    "        # Check if the filename contains \"WikiNews\"\n",
    "        Wikinews = True if 'WikiNews' in filename else False\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read the .pkl file into a DataFrame\n",
    "        data_frame = pd.read_pickle(file_path)\n",
    "        \n",
    "       \n",
    "        data_frame.columns = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "\n",
    "\n",
    "        # Perform data processing\n",
    "        data_frame['split'] = data_frame['phrase'].apply(lambda x: x.split())\n",
    "        data_frame['count'] = data_frame['split'].apply(lambda x: len(x))\n",
    "        words = data_frame[data_frame['count'] == 1]\n",
    "        MWEs = data_frame[data_frame['count'] >1]\n",
    "        word_set = words.phrase.str.lower().unique()\n",
    "        word_set = pd.DataFrame(word_set, columns=['phrase'])\n",
    "        remove = string.punctuation.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "        pattern = r\"[{}]\".format(remove)\n",
    "        word_set['phrase'] = word_set['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #function to obtain syablles for words\n",
    "        # from datamuse import datamuse\n",
    "        # api = datamuse.Datamuse()\n",
    "\n",
    "        def get_syllables(word):\n",
    "            syllables = 0\n",
    "            word_results = api.words(sp=word, max=1, md='psf')\n",
    "            if len(word_results)>0: \n",
    "                word = word_results[0][\"word\"]\n",
    "                syllables = int(word_results[0][\"numSyllables\"])\n",
    "            return syllables\n",
    "\n",
    "        #Apply function to get syllables\n",
    "        word_set['syllables'] = word_set['phrase'].apply(lambda x: get_syllables(x))\n",
    "\n",
    "        #Apply function to get word length \n",
    "        word_set['length'] = word_set['phrase'].apply(lambda x: len(x))\n",
    "\n",
    "        #take words and merge with values first you will need to clean the phrase column \n",
    "        words['original phrase'] = words['phrase']\n",
    "        words['phrase'] = words['phrase'].str.lower()\n",
    "        words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "        word_features = pd.merge(words, word_set)\n",
    "        \n",
    "        #Now parse\n",
    "        \n",
    "        # nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "        sentences = data_frame[['sentence', 'ID']].copy()\n",
    "\n",
    "        sentences = sentences.drop_duplicates()\n",
    "\n",
    "        def removefirsttoken(x):\n",
    "            x = x.split(' ', 1)[1]\n",
    "            return x\n",
    "\n",
    "        if Wikinews:\n",
    "            sentences['clean sentence'] = sentences['sentence'].apply(lambda x: removefirsttoken(x))\n",
    "\n",
    "        else:\n",
    "            sentences['clean sentence'] = sentences['sentence']\n",
    "\n",
    "        #function to parse sentences \n",
    "        def parse(string):\n",
    "            output = nlp.annotate(string, properties={\n",
    "          'annotators': 'pos,depparse',\n",
    "          'outputFormat': 'json'\n",
    "          })\n",
    "            return output\n",
    "        \n",
    "        #apply parsing to sentences\n",
    "        sentences['parse'] = sentences['clean sentence'].apply(lambda x: parse(x))\n",
    "\n",
    "        sentences\n",
    "\n",
    "        #Merge \n",
    "        word_parse_features = pd.merge(sentences, word_features)\n",
    "        word_parse_features\n",
    "        \n",
    "        def get_pos(row):\n",
    "            word = row['phrase']\n",
    "            parse = json.loads(row['parse'])\n",
    "            for i in range(len(parse['sentences'][0]['tokens'])):\n",
    "                comp_word = parse['sentences'][0]['tokens'][i]['word']\n",
    "                comp_word = comp_word.lower()\n",
    "                comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "                if comp_word == word:\n",
    "                    return parse['sentences'][0]['tokens'][i]['pos']\n",
    "        \n",
    "\n",
    "        def get_dep(row):\n",
    "            number = 0\n",
    "            word = row['phrase']\n",
    "            parse = json.loads(row['parse'])\n",
    "            for i in range(len(parse['sentences'][0]['basicDependencies'])):\n",
    "                comp_word = parse['sentences'][0]['basicDependencies'][i]['governorGloss']\n",
    "                comp_word = comp_word.lower()\n",
    "                comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "\n",
    "                if comp_word == word:\n",
    "                    number += 1\n",
    "\n",
    "            return number\n",
    "\n",
    "        #Function to get the proper lemma \n",
    "        \n",
    "\n",
    "        def get_wordnet_pos(treebank_tag):\n",
    "            from nltk.corpus import wordnet\n",
    "\n",
    "            if treebank_tag.startswith('JJ'):\n",
    "                return wordnet.ADJ\n",
    "            elif treebank_tag.startswith('VB'):\n",
    "                return wordnet.VERB\n",
    "            elif treebank_tag.startswith('NN'):\n",
    "                return wordnet.NOUN\n",
    "            elif treebank_tag.startswith('RB'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        \n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        def lemmatiser(row):\n",
    "\n",
    "            word = row['phrase']\n",
    "            pos = row['pos']\n",
    "\n",
    "            try:\n",
    "                lemma = wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                return lemma\n",
    "            except:\n",
    "                try:\n",
    "                    lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "                    return lemma\n",
    "                except:\n",
    "                    print(word)\n",
    "                    \n",
    "        #return MRC scores\n",
    "        # mrc_features = pd.read_table('corpus/MRC.csv', names=('word', 'AOA', 'BFRQ', 'CNC', 'KFCAT', 'FAM', 'KFSMP', 'IMG', 'KFFRQ', 'NLET', 'CMEAN', 'PMEAN', 'NPHN', 'T-LFRQ'))\n",
    "        mrc_features = pd.read_csv('corpus/MRC.csv', names=('id', 'NPHN', 'KFFRQ', 'KFCAT', 'KFSMP', 'T-LFRQ', 'FAM', 'CNC', 'IMG', 'AOA', 'word'), low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def aoa(word):\n",
    "            word = word.upper()  # Convert word to all capitals\n",
    "            try:\n",
    "                df = mrc_features.loc[mrc_features['word'] == word]\n",
    "                fvalue = df.iloc[0]['AOA']\n",
    "                return fvalue    \n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "\n",
    "        def CNC_fun(word):\n",
    "            word = word.upper()\n",
    "            table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "            if len(table)>0:\n",
    "\n",
    "                CNC = table['CNC'].values[0]\n",
    "                CNC = int(CNC)\n",
    "\n",
    "                return CNC\n",
    "            else: \n",
    "                y=0\n",
    "                return y\n",
    "\n",
    "        def img(word):\n",
    "            word = word.upper()\n",
    "            try:\n",
    "                df = mrc_features.loc[mrc_features['word'] == word]\n",
    "                fvalue = df.iloc[0]['IMG']\n",
    "                return fvalue    \n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def KFCAT_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFCAT = table['KFCAT'].values[0]\n",
    "                    KFCAT = int(KFCAT)\n",
    "\n",
    "                    return KFCAT\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def FAM_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    FAM = table['FAM'].values[0]\n",
    "                    FAM = int(FAM)\n",
    "\n",
    "                    return FAM\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def KFSMP_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFSMP = table['KFSMP'].values[0]\n",
    "                    KFSMP = int(KFSMP)\n",
    "\n",
    "                    return KFSMP\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def KFFRQ_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFFRQ = table['KFFRQ'].values[0]\n",
    "                    KFFRQ = int(KFFRQ)\n",
    "\n",
    "                    return KFFRQ\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        # def NLET_fun(word):\n",
    "        #         word = word.upper()\n",
    "        #         table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        #         if len(table)>0:\n",
    "\n",
    "\n",
    "        #             NLET = table['NLET'].values[0]\n",
    "        #             NLET = int(NLET)\n",
    "\n",
    "        #             return NLET\n",
    "        #         else: \n",
    "        #             y=0\n",
    "        #             return y\n",
    "\n",
    "        def NPHN_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    NPHN = table['NPHN'].values[0]\n",
    "                    NPHN = int(NPHN)\n",
    "\n",
    "                    return NPHN\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def TLFRQ_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    TLFRQ = table['T-LFRQ'].values[0]\n",
    "                    TLFRQ = int(TLFRQ)\n",
    "\n",
    "                    return TLFRQ\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        #functions using wordnet \n",
    "       \n",
    "        def synonyms(word):\n",
    "            synonyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "                synonyms = len(results)\n",
    "                return synonyms\n",
    "            except:\n",
    "                return synonyms\n",
    "\n",
    "        def hypernyms(word):\n",
    "            hypernyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "                hypernyms = len(results[0].hypernyms())\n",
    "                return hypernyms\n",
    "            except:\n",
    "                return hypernyms\n",
    "\n",
    "        def hyponyms(word):\n",
    "            hyponyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "            except:\n",
    "                return hyponyms\n",
    "            try:\n",
    "                hyponyms = len(results[0].hyponyms())\n",
    "                return hyponyms\n",
    "            except:\n",
    "                return hyponyms\n",
    "\n",
    "        #return CEFR levels\n",
    "        # all_levels = pd.read_table('corpus/CALD.csv', names=('word', 'level'))\n",
    "\n",
    "        # def levels(word):\n",
    "        #     all_levels = pd.read_csv('corpus/cefrj-vocabulary-profile-1.5.csv')\n",
    "        #     word = ''.join(word.split()).lower()\n",
    "        #     df = all_levels.loc[all_levels['headword'] == word]\n",
    "        #     if not df.empty:\n",
    "        #         level = df.iloc[0]['CEFR']\n",
    "        #         return level\n",
    "        #     else:\n",
    "        #         return 0\n",
    "\n",
    "        def levels(word):\n",
    "            word = ''.join(word.split()).lower()\n",
    "            try:\n",
    "                df = all_levels.loc[all_levels['word'] == word]\n",
    "                level = df.iloc[0]['level']\n",
    "                return level\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    df = all_levels.loc[all_levels['word'] == word]\n",
    "                    level = df.iloc[0]['level']\n",
    "                    return level\n",
    "                except:\n",
    "                    return 0\n",
    "                \n",
    "        #Convert tree bank tags to ones that are compatible w google \n",
    "\n",
    "        def is_noun(tag):\n",
    "            return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "        def is_verb(tag):\n",
    "            return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "        def is_adverb(tag):\n",
    "            return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "        def is_adjective(tag):\n",
    "            return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "\n",
    "        def penn_to_wn(tag):\n",
    "            if is_adjective(tag):\n",
    "                return wn.ADJ\n",
    "            elif is_noun(tag):\n",
    "                return wn.NOUN\n",
    "            elif is_adverb(tag):\n",
    "                return wn.ADV\n",
    "            elif is_verb(tag):\n",
    "                return wn.VERB\n",
    "            return None\n",
    "\n",
    "\n",
    "        def penn_to_google(tag):\n",
    "            if is_adjective(tag):\n",
    "                return 'adj'\n",
    "            elif is_noun(tag):\n",
    "                return 'n'\n",
    "            elif is_adverb(tag):\n",
    "                return 'adv'\n",
    "            elif is_verb(tag):\n",
    "                return 'v'\n",
    "            return None\n",
    "\n",
    "        \n",
    "        def get_frequency(row):\n",
    "                nofreq = float(0.000000)\n",
    "                word = row[\"phrase\"]\n",
    "                word = str(word)\n",
    "                tag = row[\"pos\"]\n",
    "                tag = penn_to_google(tag)\n",
    "\n",
    "                try:\n",
    "                    word_results = api.words(sp=word, max=1, md='pf')\n",
    "                    tag_list = (word_results[0]['tags'][:-1])\n",
    "\n",
    "                    frequency = word_results[0]['tags'][-1][2:]\n",
    "\n",
    "                    frequency = float(frequency)\n",
    "\n",
    "                    if tag in tag_list :\n",
    "                        return frequency\n",
    "                    else:\n",
    "                        lemma = row['lemma']\n",
    "                        try:\n",
    "                            word_results = api.words(sp=lemma, max=1, md='pf')\n",
    "                            tag_list = (word_results[0]['tags'][:-1])\n",
    "\n",
    "                            frequency = word_results[0]['tags'][-1][2:]\n",
    "\n",
    "                            frequency = float(frequency)\n",
    "\n",
    "                            if tag in tag_list:\n",
    "                                return frequency\n",
    "                            else:\n",
    "                                return nofreq\n",
    "                        except:\n",
    "                            return nofreq\n",
    "\n",
    "                except:\n",
    "\n",
    "\n",
    "                    return nofreq \n",
    "                \n",
    "\n",
    "        #GET DEP AND POS NUMBER\n",
    "        word_parse_features['pos'] = word_parse_features.apply(get_pos, axis=1)\n",
    "        word_parse_features['dep num'] = word_parse_features.apply(get_dep, axis=1)\n",
    "\n",
    "        #To obtain word lemmas \n",
    "        #Get Lemma\n",
    "        word_parse_features['lemma'] = word_parse_features.apply(lemmatiser, axis=1)\n",
    "\n",
    "        #Apply function to get number of synonyms and hypernyms/hyponyms\n",
    "        word_parse_features['synonyms'] = word_parse_features['lemma'].apply(lambda x: synonyms(x))\n",
    "        word_parse_features['hypernyms'] = word_parse_features['lemma'].apply(lambda x: hypernyms(x))\n",
    "        word_parse_features['hyponyms'] = word_parse_features['lemma'].apply(lambda x: hyponyms(x))\n",
    "\n",
    "        #Apply function to check if contained in Ogden word set\n",
    "        ogden = pd.read_table('binary-features/ogden.txt')\n",
    "        word_parse_features['ogden'] = word_parse_features['lemma'].apply(lambda x : 1 if any(ogden.words == x) else 0) #clean words\n",
    "\n",
    "        #Apply function to check if contained in simple wiki word set\n",
    "        simple_wiki = pd.read_csv('binary-features/Most_Frequent.csv')\n",
    "        word_parse_features['simple_wiki'] = word_parse_features['lemma'].apply(lambda x : 1 if any(simple_wiki.a == x) else 0) #clean words\n",
    "\n",
    "        #Apply function to get the level from Cambridge Advanced Learner Dictionary\n",
    "        # cald = pd.read_csv('binary-features/CALD.csv')\n",
    "        # word_parse_features['cald'] = word_parse_features['phrase'].apply(lambda x : 1 if any(cald.a == x) else 0)\n",
    "        # word_parse_features['cald'] = word_parse_features['phrase'].apply(lambda x: 1 if any(cald['Word'] == x) else 0)\n",
    "\n",
    "\n",
    "        #Get some MRC features\n",
    "        mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))    \n",
    "\n",
    "\n",
    "\n",
    "        # word_parse_features['cnc'] = word_parse_features['lemma'].apply(lambda x: cnc(x))\n",
    "        word_parse_features['CNC'] = word_parse_features['lemma'].apply(lambda x: CNC_fun(x) if x is not None else None)\n",
    "        word_parse_features['IMG'] = word_parse_features['lemma'].apply(lambda x: img(x) if x is not None else None)\n",
    "\n",
    "\n",
    "        #Apply function to check if contained  subimdb word set\n",
    "        subimdb_500 = pd.read_csv('binary-features/subimbd_500.tsv', sep='\\t')\n",
    "        # subimdb_500 = pd.read_pickle('binary-features/subimbd_500.tsv')\n",
    "        word_parse_features['sub_imdb'] = word_parse_features['lemma'].apply(lambda x : 1 if any(subimdb_500.words == x) else 0)\n",
    "\n",
    "        #Apply function for google freq\n",
    "        word_parse_features['google frequency'] = word_parse_features.apply(get_frequency ,axis=1)\n",
    "\n",
    "        word_parse_features['phrase'] = word_parse_features.phrase.astype(str)\n",
    "        word_parse_features['pos'] = word_parse_features.pos.astype(str)\n",
    "\n",
    "        # word_parse_features['cnc'] = word_parse_features['lemma'].apply(lambda x: cnc(x))\n",
    "        word_parse_features['CNC'] = word_parse_features['lemma'].apply(lambda x: CNC_fun(x))\n",
    "        word_parse_features['IMG'] = word_parse_features['lemma'].apply(lambda x: img(x))\n",
    "\n",
    "        word_parse_features['KFCAT']= word_parse_features['lemma'].apply(lambda x: KFCAT_fun(x))\n",
    "        word_parse_features['FAM']= word_parse_features['lemma'].apply(lambda x: FAM_fun(x) )\n",
    "        word_parse_features['KFSMP']= word_parse_features['lemma'].apply(lambda x: KFSMP_fun(x))\n",
    "        word_parse_features['KFFRQ']= word_parse_features['lemma'].apply(lambda x: KFFRQ_fun(x))\n",
    "        word_parse_features['AOA']= word_parse_features['lemma'].apply(lambda x: aoa(x))\n",
    "        word_parse_features['NPHN']= word_parse_features['lemma'].apply(lambda x: NPHN_fun(x))\n",
    "        word_parse_features['T-LFRQ']= word_parse_features['lemma'].apply(lambda x: TLFRQ_fun(x))\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        # Combine single word dataframe and multiple words dataframe\n",
    "        # combined_df = pd.concat([word_set, MWEs])\n",
    "\n",
    "        # Sort combined dataframe by original dataframe's index\n",
    "        # word_parse_features = combined_df.sort_index()\n",
    "        \n",
    "        # Combine word_parse_features and MWEs DataFrames NOT WORKING AS NEEDED\n",
    "        # word_parse_features_with_MWEs = pd.concat([word_parse_features, MWEs])\n",
    "\n",
    "        # Sort the combined DataFrame by the original order (index)\n",
    "        # combined_df.sort_index(inplace=True)\n",
    "        \n",
    "        \n",
    "        # Add back rows with multiple words in the \"phrase\" column in the original order\n",
    "        word_parse_features = pd.concat([word_parse_features, MWEs]).sort_values(by='ID')\n",
    "        \n",
    "         \n",
    "        # Fill any NaN values with zeros\n",
    "        word_parse_features.fillna(0, inplace=True)\n",
    "        \n",
    "        # Save the processed DataFrame\n",
    "        output_filename = os.path.splitext(filename)[0] + '_Final'\n",
    "        output_file_path = os.path.join(output_folder, output_filename)\n",
    "        word_parse_features.to_pickle(output_file_path)\n",
    "        # word_parse_features_with_MWEs.to_pickle(output_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>ID</th>\n",
       "      <th>clean sentence</th>\n",
       "      <th>parse</th>\n",
       "      <th>start_index</th>\n",
       "      <th>end_index</th>\n",
       "      <th>phrase</th>\n",
       "      <th>total_native</th>\n",
       "      <th>total_non_native</th>\n",
       "      <th>native_complex</th>\n",
       "      <th>...</th>\n",
       "      <th>IMG</th>\n",
       "      <th>sub_imdb</th>\n",
       "      <th>google frequency</th>\n",
       "      <th>KFCAT</th>\n",
       "      <th>FAM</th>\n",
       "      <th>KFSMP</th>\n",
       "      <th>KFFRQ</th>\n",
       "      <th>AOA</th>\n",
       "      <th>NPHN</th>\n",
       "      <th>T-LFRQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The teenage girl shot dead in Bellaghy, County...</td>\n",
       "      <td>30OITAWPBQ4V08AHXM3N85FC9DQ9HB</td>\n",
       "      <td>The teenage girl shot dead in Bellaghy, County...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "      <td>londonderry</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>I'd say they were close in life and in death c...</td>\n",
       "      <td>30OITAWPBQ4V08AHXM3N85FC9DQ9HB</td>\n",
       "      <td>I'd say they were close in life and in death c...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>death</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>498</td>\n",
       "      <td>1.0</td>\n",
       "      <td>163.536308</td>\n",
       "      <td>15.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>815.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>I'd say they were close in life and in death c...</td>\n",
       "      <td>30OITAWPBQ4V08AHXM3N85FC9DQ9HB</td>\n",
       "      <td>I'd say they were close in life and in death c...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>close</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>420</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.189275</td>\n",
       "      <td>15.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1862.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>I'd say they were close in life and in death c...</td>\n",
       "      <td>30OITAWPBQ4V08AHXM3N85FC9DQ9HB</td>\n",
       "      <td>I'd say they were close in life and in death c...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "      <td>life</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>1.0</td>\n",
       "      <td>484.140126</td>\n",
       "      <td>15.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Fr Dolan told BBC Radio Ulster: \"When Brenda w...</td>\n",
       "      <td>30OITAWPBQ4V08AHXM3N85FC9DQ9HB</td>\n",
       "      <td>Fr Dolan told BBC Radio Ulster: \"When Brenda w...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>123</td>\n",
       "      <td>127</td>\n",
       "      <td>life</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>482</td>\n",
       "      <td>1.0</td>\n",
       "      <td>484.140126</td>\n",
       "      <td>15.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>715.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Her older sister, aged 21, lived at the rented...</td>\n",
       "      <td>3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0</td>\n",
       "      <td>Her older sister, aged 21, lived at the rented...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>105</td>\n",
       "      <td>116</td>\n",
       "      <td>established</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.498944</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>327.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>Her older sister, aged 21, lived at the rented...</td>\n",
       "      <td>3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0</td>\n",
       "      <td>Her older sister, aged 21, lived at the rented...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>68</td>\n",
       "      <td>73</td>\n",
       "      <td>built</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>399</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.589952</td>\n",
       "      <td>14.0</td>\n",
       "      <td>554.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>Her older sister, aged 21, lived at the rented...</td>\n",
       "      <td>3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0</td>\n",
       "      <td>Her older sister, aged 21, lived at the rented...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>59</td>\n",
       "      <td>67</td>\n",
       "      <td>recently</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49.414182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>It is believed she has been taken to the Royal...</td>\n",
       "      <td>3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0</td>\n",
       "      <td>It is believed she has been taken to the Royal...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>56</td>\n",
       "      <td>64</td>\n",
       "      <td>hospital</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.359238</td>\n",
       "      <td>12.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Police have examined the scene at a house at W...</td>\n",
       "      <td>3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0</td>\n",
       "      <td>Police have examined the scene at a house at W...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>examined</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.853028</td>\n",
       "      <td>10.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>210.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2094 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "0    The teenage girl shot dead in Bellaghy, County...   \n",
       "55   I'd say they were close in life and in death c...   \n",
       "54   I'd say they were close in life and in death c...   \n",
       "53   I'd say they were close in life and in death c...   \n",
       "52   Fr Dolan told BBC Radio Ulster: \"When Brenda w...   \n",
       "..                                                 ...   \n",
       "514  Her older sister, aged 21, lived at the rented...   \n",
       "513  Her older sister, aged 21, lived at the rented...   \n",
       "512  Her older sister, aged 21, lived at the rented...   \n",
       "526  It is believed she has been taken to the Royal...   \n",
       "493  Police have examined the scene at a house at W...   \n",
       "\n",
       "                                 ID  \\\n",
       "0    30OITAWPBQ4V08AHXM3N85FC9DQ9HB   \n",
       "55   30OITAWPBQ4V08AHXM3N85FC9DQ9HB   \n",
       "54   30OITAWPBQ4V08AHXM3N85FC9DQ9HB   \n",
       "53   30OITAWPBQ4V08AHXM3N85FC9DQ9HB   \n",
       "52   30OITAWPBQ4V08AHXM3N85FC9DQ9HB   \n",
       "..                              ...   \n",
       "514  3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0   \n",
       "513  3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0   \n",
       "512  3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0   \n",
       "526  3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0   \n",
       "493  3ZXV7Q5FJBPDKAQEEGPE7A47ZJGFC0   \n",
       "\n",
       "                                        clean sentence  \\\n",
       "0    The teenage girl shot dead in Bellaghy, County...   \n",
       "55   I'd say they were close in life and in death c...   \n",
       "54   I'd say they were close in life and in death c...   \n",
       "53   I'd say they were close in life and in death c...   \n",
       "52   Fr Dolan told BBC Radio Ulster: \"When Brenda w...   \n",
       "..                                                 ...   \n",
       "514  Her older sister, aged 21, lived at the rented...   \n",
       "513  Her older sister, aged 21, lived at the rented...   \n",
       "512  Her older sister, aged 21, lived at the rented...   \n",
       "526  It is believed she has been taken to the Royal...   \n",
       "493  Police have examined the scene at a house at W...   \n",
       "\n",
       "                                                 parse  start_index  \\\n",
       "0    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           47   \n",
       "55   {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           39   \n",
       "54   {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           18   \n",
       "53   {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           27   \n",
       "52   {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...          123   \n",
       "..                                                 ...          ...   \n",
       "514  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...          105   \n",
       "513  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           68   \n",
       "512  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           59   \n",
       "526  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           56   \n",
       "493  {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           12   \n",
       "\n",
       "     end_index       phrase  total_native  total_non_native  native_complex  \\\n",
       "0           58  londonderry            10                10               1   \n",
       "55          44        death            10                10               0   \n",
       "54          23        close            10                10               0   \n",
       "53          31         life            10                10               0   \n",
       "52         127         life            10                10               0   \n",
       "..         ...          ...           ...               ...             ...   \n",
       "514        116  established            10                10               6   \n",
       "513         73        built            10                10               0   \n",
       "512         67     recently            10                10               0   \n",
       "526         64     hospital            10                10               0   \n",
       "493         20     examined            10                10               5   \n",
       "\n",
       "     ...  IMG  sub_imdb  google frequency KFCAT    FAM  KFSMP  KFFRQ  AOA  \\\n",
       "0    ...    0       0.0          0.428829   0.0    0.0    0.0    0.0    0   \n",
       "55   ...  498       1.0        163.536308  15.0  581.0  132.0  277.0    0   \n",
       "54   ...  420       1.0        120.189275  15.0  587.0  166.0  234.0  283   \n",
       "53   ...  482       1.0        484.140126  15.0  598.0  279.0  715.0    0   \n",
       "52   ...  482       1.0        484.140126  15.0  598.0  279.0  715.0    0   \n",
       "..   ...  ...       ...               ...   ...    ...    ...    ...  ...   \n",
       "514  ...    0       0.0         38.498944   9.0    0.0   43.0   58.0    0   \n",
       "513  ...  399       1.0         73.589952  14.0  554.0   56.0   86.0    0   \n",
       "512  ...    0       1.0         49.414182   0.0    0.0    0.0    0.0    0   \n",
       "526  ...  602       1.0         58.359238  12.0  548.0   48.0  110.0  319   \n",
       "493  ...  341       0.0         30.853028  10.0  549.0   31.0   33.0    0   \n",
       "\n",
       "    NPHN  T-LFRQ  \n",
       "0    0.0     0.0  \n",
       "55   3.0   815.0  \n",
       "54   0.0  1862.0  \n",
       "53   3.0  4804.0  \n",
       "52   3.0  4804.0  \n",
       "..   ...     ...  \n",
       "514  8.0   327.0  \n",
       "513  0.0   306.0  \n",
       "512  0.0     0.0  \n",
       "526  0.0   825.0  \n",
       "493  7.0   210.0  \n",
       "\n",
       "[2094 rows x 37 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_parse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data = pd.read_pickle('final_run/Wikipedia_Train_actual')\n",
    "# data.to_csv('final_run/Wikipedia_Train_actual.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
