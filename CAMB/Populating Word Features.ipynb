{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populating word Features\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import string\n",
    "import regex as re\n",
    "import json\n",
    "import os\n",
    "from datamuse import datamuse\n",
    "import pycorenlp\n",
    "import pandas as pd\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['original phrase'] = words['phrase']\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].str.lower()\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:478: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['original phrase'] = words['phrase']\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].str.lower()\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:478: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['original phrase'] = words['phrase']\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].str.lower()\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
      "/var/folders/qm/g52sxk4d4qsg16xphqz6vvtc0000gn/T/ipykernel_41062/3968544490.py:478: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))\n"
     ]
    }
   ],
   "source": [
    "# Initialize Datamuse API and StanfordCoreNLP\n",
    "api = datamuse.Datamuse()\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# Set the paths for the input and output folders\n",
    "folder_path = \"cwishareddataset/testset/english/pickled-dataframes\"\n",
    "output_folder = \"final_camb_feats_Test\"\n",
    "\n",
    "        \n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pkl'):\n",
    "        # Check if the filename contains \"WikiNews\"\n",
    "        Wikinews = True if 'WikiNews' in filename else False\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read the .pkl file into a DataFrame\n",
    "        data_frame = pd.read_pickle(file_path)\n",
    "        \n",
    "       \n",
    "        data_frame.columns = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "\n",
    "\n",
    "        # Perform data processing\n",
    "        data_frame['split'] = data_frame['phrase'].apply(lambda x: x.split())\n",
    "        data_frame['count'] = data_frame['split'].apply(lambda x: len(x))\n",
    "        words = data_frame[data_frame['count'] == 1]\n",
    "        # MWEs = data_frame[data_frame['count'] >1]\n",
    "        word_set = words.phrase.str.lower().unique()\n",
    "        word_set = pd.DataFrame(word_set, columns=['phrase'])\n",
    "        remove = string.punctuation.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "        pattern = r\"[{}]\".format(remove)\n",
    "        word_set['phrase'] = word_set['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #function to obtain syablles for words\n",
    "        # from datamuse import datamuse\n",
    "        # api = datamuse.Datamuse()\n",
    "\n",
    "        def get_syllables(word):\n",
    "            syllables = 0\n",
    "            word_results = api.words(sp=word, max=1, md='psf')\n",
    "            if len(word_results)>0: \n",
    "                word = word_results[0][\"word\"]\n",
    "                syllables = int(word_results[0][\"numSyllables\"])\n",
    "            return syllables\n",
    "\n",
    "        #Apply function to get syllables\n",
    "        word_set['syllables'] = word_set['phrase'].apply(lambda x: get_syllables(x))\n",
    "\n",
    "        #Apply function to get word length \n",
    "        word_set['length'] = word_set['phrase'].apply(lambda x: len(x))\n",
    "\n",
    "        #take words and merge with values first you will need to clean the phrase column \n",
    "        words['original phrase'] = words['phrase']\n",
    "        words['phrase'] = words['phrase'].str.lower()\n",
    "        words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "        word_features = pd.merge(words, word_set)\n",
    "        \n",
    "        #Now parse\n",
    "        # import pycorenlp\n",
    "        # import pandas as pd\n",
    "        # from pycorenlp import StanfordCoreNLP\n",
    "        # nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "        sentences = data_frame[['sentence', 'ID']].copy()\n",
    "\n",
    "        sentences = sentences.drop_duplicates()\n",
    "\n",
    "        def removefirsttoken(x):\n",
    "            x = x.split(' ', 1)[1]\n",
    "            return x\n",
    "\n",
    "        if Wikinews:\n",
    "            sentences['clean sentence'] = sentences['sentence'].apply(lambda x: removefirsttoken(x))\n",
    "\n",
    "        else:\n",
    "            sentences['clean sentence'] = sentences['sentence']\n",
    "\n",
    "        #function to parse sentences \n",
    "        def parse(string):\n",
    "            output = nlp.annotate(string, properties={\n",
    "          'annotators': 'pos,depparse',\n",
    "          'outputFormat': 'json'\n",
    "          })\n",
    "            return output\n",
    "        \n",
    "        #apply parsing to sentences\n",
    "        sentences['parse'] = sentences['clean sentence'].apply(lambda x: parse(x))\n",
    "\n",
    "        sentences\n",
    "\n",
    "        #Merge \n",
    "        word_parse_features = pd.merge(sentences, word_features)\n",
    "        word_parse_features\n",
    "        \n",
    "        def get_pos(row):\n",
    "            word = row['phrase']\n",
    "            parse = json.loads(row['parse'])\n",
    "            for i in range(len(parse['sentences'][0]['tokens'])):\n",
    "                comp_word = parse['sentences'][0]['tokens'][i]['word']\n",
    "                comp_word = comp_word.lower()\n",
    "                comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "                if comp_word == word:\n",
    "                    return parse['sentences'][0]['tokens'][i]['pos']\n",
    "        \n",
    "\n",
    "        def get_dep(row):\n",
    "            number = 0\n",
    "            word = row['phrase']\n",
    "            parse = json.loads(row['parse'])\n",
    "            for i in range(len(parse['sentences'][0]['basicDependencies'])):\n",
    "                comp_word = parse['sentences'][0]['basicDependencies'][i]['governorGloss']\n",
    "                comp_word = comp_word.lower()\n",
    "                comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "\n",
    "                if comp_word == word:\n",
    "                    number += 1\n",
    "\n",
    "            return number\n",
    "\n",
    "        #Function to get the proper lemma \n",
    "        import nltk\n",
    "        from nltk.corpus import wordnet\n",
    "\n",
    "        def get_wordnet_pos(treebank_tag):\n",
    "            from nltk.corpus import wordnet\n",
    "\n",
    "            if treebank_tag.startswith('JJ'):\n",
    "                return wordnet.ADJ\n",
    "            elif treebank_tag.startswith('VB'):\n",
    "                return wordnet.VERB\n",
    "            elif treebank_tag.startswith('NN'):\n",
    "                return wordnet.NOUN\n",
    "            elif treebank_tag.startswith('RB'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        def lemmatiser(row):\n",
    "\n",
    "            word = row['phrase']\n",
    "            pos = row['pos']\n",
    "\n",
    "            try:\n",
    "                lemma = wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                return lemma\n",
    "            except:\n",
    "                try:\n",
    "                    lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "                    return lemma\n",
    "                except:\n",
    "                    print(word)\n",
    "                    \n",
    "        #return MRC scores\n",
    "        # mrc_features = pd.read_table('corpus/MRC.csv', names=('word', 'AOA', 'BFRQ', 'CNC', 'KFCAT', 'FAM', 'KFSMP', 'IMG', 'KFFRQ', 'NLET', 'CMEAN', 'PMEAN', 'NPHN', 'T-LFRQ'))\n",
    "        mrc_features = pd.read_csv('corpus/MRC.csv', names=('id', 'NPHN', 'KFFRQ', 'KFCAT', 'KFSMP', 'T-LFRQ', 'FAM', 'CNC', 'IMG', 'AOA', 'word'), low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def aoa(word):\n",
    "            word = word.upper()  # Convert word to all capitals\n",
    "            try:\n",
    "                df = mrc_features.loc[mrc_features['word'] == word]\n",
    "                fvalue = df.iloc[0]['AOA']\n",
    "                return fvalue    \n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "\n",
    "        def CNC_fun(word):\n",
    "            word = word.upper()\n",
    "            table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "            if len(table)>0:\n",
    "\n",
    "                CNC = table['CNC'].values[0]\n",
    "                CNC = int(CNC)\n",
    "\n",
    "                return CNC\n",
    "            else: \n",
    "                y=0\n",
    "                return y\n",
    "\n",
    "        def img(word):\n",
    "            word = word.upper()\n",
    "            try:\n",
    "                df = mrc_features.loc[mrc_features['word'] == word]\n",
    "                fvalue = df.iloc[0]['IMG']\n",
    "                return fvalue    \n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def KFCAT_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFCAT = table['KFCAT'].values[0]\n",
    "                    KFCAT = int(KFCAT)\n",
    "\n",
    "                    return KFCAT\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def FAM_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    FAM = table['FAM'].values[0]\n",
    "                    FAM = int(FAM)\n",
    "\n",
    "                    return FAM\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def KFSMP_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFSMP = table['KFSMP'].values[0]\n",
    "                    KFSMP = int(KFSMP)\n",
    "\n",
    "                    return KFSMP\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def KFFRQ_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFFRQ = table['KFFRQ'].values[0]\n",
    "                    KFFRQ = int(KFFRQ)\n",
    "\n",
    "                    return KFFRQ\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        # def NLET_fun(word):\n",
    "        #         word = word.upper()\n",
    "        #         table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        #         if len(table)>0:\n",
    "\n",
    "\n",
    "        #             NLET = table['NLET'].values[0]\n",
    "        #             NLET = int(NLET)\n",
    "\n",
    "        #             return NLET\n",
    "        #         else: \n",
    "        #             y=0\n",
    "        #             return y\n",
    "\n",
    "        def NPHN_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    NPHN = table['NPHN'].values[0]\n",
    "                    NPHN = int(NPHN)\n",
    "\n",
    "                    return NPHN\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def TLFRQ_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    TLFRQ = table['T-LFRQ'].values[0]\n",
    "                    TLFRQ = int(TLFRQ)\n",
    "\n",
    "                    return TLFRQ\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        #functions using wordnet \n",
    "        from nltk.corpus import wordnet\n",
    "        def synonyms(word):\n",
    "            synonyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "                synonyms = len(results)\n",
    "                return synonyms\n",
    "            except:\n",
    "                return synonyms\n",
    "\n",
    "        def hypernyms(word):\n",
    "            hypernyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "                hypernyms = len(results[0].hypernyms())\n",
    "                return hypernyms\n",
    "            except:\n",
    "                return hypernyms\n",
    "\n",
    "        def hyponyms(word):\n",
    "            hyponyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "            except:\n",
    "                return hyponyms\n",
    "            try:\n",
    "                hyponyms = len(results[0].hyponyms())\n",
    "                return hyponyms\n",
    "            except:\n",
    "                return hyponyms\n",
    "\n",
    "        #return CEFR levels\n",
    "        # all_levels = pd.read_table('corpus/CALD.csv', names=('word', 'level'))\n",
    "\n",
    "        # def levels(word):\n",
    "        #     all_levels = pd.read_csv('corpus/cefrj-vocabulary-profile-1.5.csv')\n",
    "        #     word = ''.join(word.split()).lower()\n",
    "        #     df = all_levels.loc[all_levels['headword'] == word]\n",
    "        #     if not df.empty:\n",
    "        #         level = df.iloc[0]['CEFR']\n",
    "        #         return level\n",
    "        #     else:\n",
    "        #         return 0\n",
    "\n",
    "        def levels(word):\n",
    "            word = ''.join(word.split()).lower()\n",
    "            try:\n",
    "                df = all_levels.loc[all_levels['word'] == word]\n",
    "                level = df.iloc[0]['level']\n",
    "                return level\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    df = all_levels.loc[all_levels['word'] == word]\n",
    "                    level = df.iloc[0]['level']\n",
    "                    return level\n",
    "                except:\n",
    "                    return 0\n",
    "                \n",
    "        #Convert tree bank tags to ones that are compatible w google \n",
    "\n",
    "        def is_noun(tag):\n",
    "            return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "        def is_verb(tag):\n",
    "            return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "        def is_adverb(tag):\n",
    "            return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "        def is_adjective(tag):\n",
    "            return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "\n",
    "        def penn_to_wn(tag):\n",
    "            if is_adjective(tag):\n",
    "                return wn.ADJ\n",
    "            elif is_noun(tag):\n",
    "                return wn.NOUN\n",
    "            elif is_adverb(tag):\n",
    "                return wn.ADV\n",
    "            elif is_verb(tag):\n",
    "                return wn.VERB\n",
    "            return None\n",
    "\n",
    "\n",
    "        def penn_to_google(tag):\n",
    "            if is_adjective(tag):\n",
    "                return 'adj'\n",
    "            elif is_noun(tag):\n",
    "                return 'n'\n",
    "            elif is_adverb(tag):\n",
    "                return 'adv'\n",
    "            elif is_verb(tag):\n",
    "                return 'v'\n",
    "            return None\n",
    "\n",
    "        \n",
    "        def get_frequency(row):\n",
    "                nofreq = float(0.000000)\n",
    "                word = row[\"phrase\"]\n",
    "                word = str(word)\n",
    "                tag = row[\"pos\"]\n",
    "                tag = penn_to_google(tag)\n",
    "\n",
    "                try:\n",
    "                    word_results = api.words(sp=word, max=1, md='pf')\n",
    "                    tag_list = (word_results[0]['tags'][:-1])\n",
    "\n",
    "                    frequency = word_results[0]['tags'][-1][2:]\n",
    "\n",
    "                    frequency = float(frequency)\n",
    "\n",
    "                    if tag in tag_list :\n",
    "                        return frequency\n",
    "                    else:\n",
    "                        lemma = row['lemma']\n",
    "                        try:\n",
    "                            word_results = api.words(sp=lemma, max=1, md='pf')\n",
    "                            tag_list = (word_results[0]['tags'][:-1])\n",
    "\n",
    "                            frequency = word_results[0]['tags'][-1][2:]\n",
    "\n",
    "                            frequency = float(frequency)\n",
    "\n",
    "                            if tag in tag_list:\n",
    "                                return frequency\n",
    "                            else:\n",
    "                                return nofreq\n",
    "                        except:\n",
    "                            return nofreq\n",
    "\n",
    "                except:\n",
    "\n",
    "\n",
    "                    return nofreq \n",
    "                \n",
    "\n",
    "        #GET DEP AND POS NUMBER\n",
    "        word_parse_features['pos'] = word_parse_features.apply(get_pos, axis=1)\n",
    "        word_parse_features['dep num'] = word_parse_features.apply(get_dep, axis=1)\n",
    "\n",
    "        #To obtain word lemmas \n",
    "        #Get Lemma\n",
    "        word_parse_features['lemma'] = word_parse_features.apply(lemmatiser, axis=1)\n",
    "\n",
    "        #Apply function to get number of synonyms and hypernyms/hyponyms\n",
    "        word_parse_features['synonyms'] = word_parse_features['lemma'].apply(lambda x: synonyms(x))\n",
    "        word_parse_features['hypernyms'] = word_parse_features['lemma'].apply(lambda x: hypernyms(x))\n",
    "        word_parse_features['hyponyms'] = word_parse_features['lemma'].apply(lambda x: hyponyms(x))\n",
    "\n",
    "        #Apply function to check if contained in Ogden word set\n",
    "        ogden = pd.read_table('binary-features/ogden.txt')\n",
    "        word_parse_features['ogden'] = word_parse_features['lemma'].apply(lambda x : 1 if any(ogden.words == x) else 0) #clean words\n",
    "\n",
    "        #Apply function to check if contained in simple wiki word set\n",
    "        simple_wiki = pd.read_csv('binary-features/Most_Frequent.csv')\n",
    "        word_parse_features['simple_wiki'] = word_parse_features['lemma'].apply(lambda x : 1 if any(simple_wiki.a == x) else 0) #clean words\n",
    "\n",
    "        # #Apply function to get the level from Cambridge Advanced Learner Dictionary\n",
    "        # cald = pd.read_csv('binary-features/CALD.csv')\n",
    "        # # word_parse_features['cald'] = word_parse_features['phrase'].apply(lambda x : 1 if any(cald.a == x) else 0)\n",
    "        # word_parse_features['cald'] = word_parse_features['phrase'].apply(lambda x: 1 if any(cald['Word'] == x) else 0)\n",
    "\n",
    "\n",
    "        #Get some MRC features\n",
    "        mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))    \n",
    "\n",
    "\n",
    "\n",
    "        # word_parse_features['cnc'] = word_parse_features['lemma'].apply(lambda x: cnc(x))\n",
    "        word_parse_features['CNC'] = word_parse_features['lemma'].apply(lambda x: CNC_fun(x) if x is not None else None)\n",
    "        word_parse_features['IMG'] = word_parse_features['lemma'].apply(lambda x: img(x) if x is not None else None)\n",
    "\n",
    "\n",
    "        #Apply function to check if contained  subimdb word set\n",
    "        subimdb_500 = pd.read_csv('binary-features/subimbd_500.tsv', sep='\\t')\n",
    "        # subimdb_500 = pd.read_pickle('binary-features/subimbd_500.tsv')\n",
    "        word_parse_features['sub_imdb'] = word_parse_features['lemma'].apply(lambda x : 1 if any(subimdb_500.words == x) else 0)\n",
    "\n",
    "        #Apply function for google freq\n",
    "        word_parse_features['google frequency'] = word_parse_features.apply(get_frequency ,axis=1)\n",
    "\n",
    "        word_parse_features['phrase'] = word_parse_features.phrase.astype(str)\n",
    "        word_parse_features['pos'] = word_parse_features.pos.astype(str)\n",
    "\n",
    "        # word_parse_features['cnc'] = word_parse_features['lemma'].apply(lambda x: cnc(x))\n",
    "        word_parse_features['CNC'] = word_parse_features['lemma'].apply(lambda x: CNC_fun(x))\n",
    "        word_parse_features['IMG'] = word_parse_features['lemma'].apply(lambda x: img(x))\n",
    "\n",
    "        word_parse_features['KFCAT']= word_parse_features['lemma'].apply(lambda x: KFCAT_fun(x))\n",
    "        word_parse_features['FAM']= word_parse_features['lemma'].apply(lambda x: FAM_fun(x) )\n",
    "        word_parse_features['KFSMP']= word_parse_features['lemma'].apply(lambda x: KFSMP_fun(x))\n",
    "        word_parse_features['KFFRQ']= word_parse_features['lemma'].apply(lambda x: KFFRQ_fun(x))\n",
    "        word_parse_features['AOA']= word_parse_features['lemma'].apply(lambda x: aoa(x))\n",
    "        word_parse_features['NPHN']= word_parse_features['lemma'].apply(lambda x: NPHN_fun(x))\n",
    "        word_parse_features['T-LFRQ']= word_parse_features['lemma'].apply(lambda x: TLFRQ_fun(x))\n",
    "        \n",
    "        \n",
    "        # Combine single word dataframe and multiple words dataframe\n",
    "        # combined_df = pd.concat([word_set, MWEs])\n",
    "\n",
    "        # Sort combined dataframe by original dataframe's index\n",
    "        # word_parse_features = combined_df.sort_index()\n",
    "        \n",
    "        # Combine word_parse_features and MWEs DataFrames NOT WORKING AS NEEDED\n",
    "        # word_parse_features_with_MWEs = pd.concat([word_parse_features, MWEs])\n",
    "\n",
    "        # Sort the combined DataFrame by the original order (index)\n",
    "        # combined_df.sort_index(inplace=True)\n",
    "\n",
    "        # Save the processed DataFrame\n",
    "        output_filename = os.path.splitext(filename)[0] + '_actual_Final'\n",
    "        output_file_path = os.path.join(output_folder, output_filename)\n",
    "        word_parse_features.to_pickle(output_file_path)\n",
    "        # word_parse_features_with_MWEs.to_pickle(output_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_parse_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# data = pd.read_pickle('features/WikiNews_Train_actual')\n",
    "# data.to_csv('features/WikiNews_Train_actual.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Initialize Datamuse API and StanfordCoreNLP\n",
    "# api = datamuse.Datamuse()\n",
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# # Set the paths for the input and output folders\n",
    "# folder_path = \"cwishareddataset/traindevset/english/pickled-dataframes\"\n",
    "# output_folder = \"features\"\n",
    "\n",
    "# # Iterate over .pkl files in the folder\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import string\n",
    "# import datamuse\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "# api = datamuse.Datamuse()\n",
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# # Set the paths for the input and output folders\n",
    "# folder_path = \"cwishareddataset/traindevset/english/pickled-dataframes\"\n",
    "# output_folder = \"features\"\n",
    "\n",
    "# # Iterate over .pkl files in the folder\n",
    "        \n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith('.pkl'):\n",
    "#         # Check if the filename contains \"WikiNews\"\n",
    "#         Wikinews = True if 'WikiNews' in filename else False\n",
    "\n",
    "#         # Construct the file path\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "#         # Read the .pkl file into a DataFrame\n",
    "#         data_frame = pd.read_pickle(file_path)\n",
    "        \n",
    "       \n",
    "#         data_frame.columns = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "\n",
    "\n",
    "#         # Perform data processing\n",
    "#         data_frame['split'] = data_frame['phrase'].apply(lambda x: x.split())\n",
    "#         data_frame['count'] = data_frame['split'].apply(lambda x: len(x))\n",
    "#         words = data_frame[data_frame['count'] == 1]\n",
    "#         MWEs = data_frame[data_frame['count'] >1]\n",
    "#         word_set = words.phrase.str.lower().unique()\n",
    "#         word_set = pd.DataFrame(word_set, columns=['phrase'])\n",
    "#         remove = string.punctuation.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "#         pattern = r\"[{}]\".format(remove)\n",
    "#         word_set['phrase'] = word_set['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "\n",
    "#         def get_syllables(word):\n",
    "#             syllables = 0\n",
    "#             word_results = api.words(sp=word, max=1, md='psf')\n",
    "#             if len(word_results)>0: \n",
    "#                 word = word_results[0][\"word\"]\n",
    "#                 syllables = int(word_results[0][\"numSyllables\"])\n",
    "#             return syllables\n",
    "\n",
    "#         #Apply function to get syllables\n",
    "#         word_set['syllables'] = word_set['phrase'].apply(lambda x: get_syllables(x))\n",
    "\n",
    "#         #Apply function to get word length \n",
    "#         word_set['length'] = word_set['phrase'].apply(lambda x: len(x))\n",
    "\n",
    "#         #take words and merge with values first you will need to clean the phrase column \n",
    "#         words['original phrase'] = words['phrase']\n",
    "#         words['phrase'] = words['phrase'].str.lower()\n",
    "#         words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "#         word_features = pd.merge(words, word_set)\n",
    "        \n",
    "        \n",
    "\n",
    "#         sentences = data_frame[['sentence', 'ID']].copy()\n",
    "\n",
    "#         sentences = sentences.drop_duplicates()\n",
    "\n",
    "#         def removefirsttoken(x):\n",
    "#             x = x.split(' ', 1)[1]\n",
    "#             return x\n",
    "\n",
    "#         if Wikinews:\n",
    "#             sentences['clean sentence'] = sentences['sentence'].apply(lambda x: removefirsttoken(x))\n",
    "\n",
    "#         else:\n",
    "#             sentences['clean sentence'] = sentences['sentence']\n",
    "\n",
    "#         #function to parse sentences \n",
    "#         def parse(string):\n",
    "#             output = nlp.annotate(string, properties={\n",
    "#           'annotators': 'pos,depparse',\n",
    "#           'outputFormat': 'json'\n",
    "#           })\n",
    "#             return output\n",
    "        \n",
    "#         #apply parsing to sentences\n",
    "#         sentences['parse'] = sentences['clean sentence'].apply(lambda x: parse(x))\n",
    "\n",
    "#         sentences\n",
    "\n",
    "#         #Merge \n",
    "#         word_parse_features = pd.merge(sentences, word_features)\n",
    "#         word_parse_features\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MWEs = data_frame[data_frame['count'] >1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # Combine single word dataframe and multiple words dataframe\n",
    "# combined_df = pd.concat([word_parse_features, MWEs])\n",
    "\n",
    "#         # Sort combined dataframe by original dataframe's index\n",
    "# word_parse_features = combined_df.sort_index()\n",
    "        \n",
    "#         # Combine word_parse_features and MWEs DataFrames NOT WORKING AS NEEDED\n",
    "# word_parse_features_with_MWEs = pd.concat([word_parse_features, MWEs])\n",
    "\n",
    "#         # Sort the combined DataFrame by the original order (index)\n",
    "# combined_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>ID</th>\n",
       "      <th>clean sentence</th>\n",
       "      <th>parse</th>\n",
       "      <th>start_index</th>\n",
       "      <th>end_index</th>\n",
       "      <th>phrase</th>\n",
       "      <th>total_native</th>\n",
       "      <th>total_non_native</th>\n",
       "      <th>native_complex</th>\n",
       "      <th>non_native_complex</th>\n",
       "      <th>complex_binary</th>\n",
       "      <th>complex_probabilistic</th>\n",
       "      <th>split</th>\n",
       "      <th>count</th>\n",
       "      <th>original phrase</th>\n",
       "      <th>syllables</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>considered</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[considered]</td>\n",
       "      <td>1</td>\n",
       "      <td>considered</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>victims</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[victims]</td>\n",
       "      <td>1</td>\n",
       "      <td>victims</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>status</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[status]</td>\n",
       "      <td>1</td>\n",
       "      <td>status</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>83</td>\n",
       "      <td>90</td>\n",
       "      <td>changed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[changed]</td>\n",
       "      <td>1</td>\n",
       "      <td>changed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>94</td>\n",
       "      <td>101</td>\n",
       "      <td>suspect</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[suspect]</td>\n",
       "      <td>1</td>\n",
       "      <td>suspect</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>87</td>\n",
       "      <td>Other Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[Other, Backward, Class]</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>87</td>\n",
       "      <td>Other Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[Other, Backward, Class]</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>87</td>\n",
       "      <td>Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Backward, Class]</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>87</td>\n",
       "      <td>Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Backward, Class]</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>87</td>\n",
       "      <td>Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Backward, Class]</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1057 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "0    #26-7 Initially, all three were considered vic...   \n",
       "1    #26-7 Initially, all three were considered vic...   \n",
       "2    #26-7 Initially, all three were considered vic...   \n",
       "3    #26-7 Initially, all three were considered vic...   \n",
       "4    #26-7 Initially, all three were considered vic...   \n",
       "..                                                 ...   \n",
       "859  #40-9 However, they want their community to be...   \n",
       "859  #40-9 However, they want their community to be...   \n",
       "862  #40-9 However, they want their community to be...   \n",
       "862  #40-9 However, they want their community to be...   \n",
       "862  #40-9 However, they want their community to be...   \n",
       "\n",
       "                                 ID  \\\n",
       "0    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "1    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "2    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "3    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "4    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "..                              ...   \n",
       "859  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "859  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "862  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "862  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "862  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "\n",
       "                                        clean sentence  \\\n",
       "0    Initially, all three were considered victims, ...   \n",
       "1    Initially, all three were considered victims, ...   \n",
       "2    Initially, all three were considered victims, ...   \n",
       "3    Initially, all three were considered victims, ...   \n",
       "4    Initially, all three were considered victims, ...   \n",
       "..                                                 ...   \n",
       "859                                                NaN   \n",
       "859                                                NaN   \n",
       "862                                                NaN   \n",
       "862                                                NaN   \n",
       "862                                                NaN   \n",
       "\n",
       "                                                 parse  start_index  \\\n",
       "0    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           32   \n",
       "1    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           43   \n",
       "2    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           60   \n",
       "3    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           83   \n",
       "4    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           94   \n",
       "..                                                 ...          ...   \n",
       "859                                                NaN           67   \n",
       "859                                                NaN           67   \n",
       "862                                                NaN           73   \n",
       "862                                                NaN           73   \n",
       "862                                                NaN           73   \n",
       "\n",
       "     end_index                phrase  total_native  total_non_native  \\\n",
       "0           42            considered            10                10   \n",
       "1           50               victims            10                10   \n",
       "2           66                status            10                10   \n",
       "3           90               changed            10                10   \n",
       "4          101               suspect            10                10   \n",
       "..         ...                   ...           ...               ...   \n",
       "859         87  Other Backward Class            10                10   \n",
       "859         87  Other Backward Class            10                10   \n",
       "862         87        Backward Class            10                10   \n",
       "862         87        Backward Class            10                10   \n",
       "862         87        Backward Class            10                10   \n",
       "\n",
       "     native_complex  non_native_complex  complex_binary  \\\n",
       "0                 2                   3               1   \n",
       "1                 4                   2               1   \n",
       "2                 3                   1               1   \n",
       "3                 0                   2               1   \n",
       "4                 3                   2               1   \n",
       "..              ...                 ...             ...   \n",
       "859               0                   1               1   \n",
       "859               0                   1               1   \n",
       "862               0                   0               0   \n",
       "862               0                   0               0   \n",
       "862               0                   0               0   \n",
       "\n",
       "     complex_probabilistic                     split  count original phrase  \\\n",
       "0                     0.25              [considered]      1      considered   \n",
       "1                     0.30                 [victims]      1         victims   \n",
       "2                     0.20                  [status]      1          status   \n",
       "3                     0.10                 [changed]      1         changed   \n",
       "4                     0.25                 [suspect]      1         suspect   \n",
       "..                     ...                       ...    ...             ...   \n",
       "859                   0.05  [Other, Backward, Class]      3             NaN   \n",
       "859                   0.05  [Other, Backward, Class]      3             NaN   \n",
       "862                   0.00         [Backward, Class]      2             NaN   \n",
       "862                   0.00         [Backward, Class]      2             NaN   \n",
       "862                   0.00         [Backward, Class]      2             NaN   \n",
       "\n",
       "     syllables  length  \n",
       "0          3.0    10.0  \n",
       "1          2.0     7.0  \n",
       "2          2.0     6.0  \n",
       "3          1.0     7.0  \n",
       "4          2.0     7.0  \n",
       "..         ...     ...  \n",
       "859        NaN     NaN  \n",
       "859        NaN     NaN  \n",
       "862        NaN     NaN  \n",
       "862        NaN     NaN  \n",
       "862        NaN     NaN  \n",
       "\n",
       "[1057 rows x 18 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 142\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Get linguistic features using StanfordCoreNLP\u001b[39;00m\n\u001b[1;32m    141\u001b[0m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(json\u001b[38;5;241m.\u001b[39mloads)\n\u001b[0;32m--> 142\u001b[0m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mwords_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdep num\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m words_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;28msum\u001b[39m(dep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgovernorGloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mtranslate({\u001b[38;5;28mord\u001b[39m(char): \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m remove_chars}) \u001b[38;5;241m==\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasicDependencies\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    144\u001b[0m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m words_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: WordNetLemmatizer()\u001b[38;5;241m.\u001b[39mlemmatize(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m], get_wordnet_pos(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m])), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python3-9-16/lib/python3.9/site-packages/pandas/core/series.py:4626\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4517\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4518\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4524\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4525\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4624\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python3-9-16/lib/python3.9/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python3-9-16/lib/python3.9/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/python3-9-16/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 142\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Get linguistic features using StanfordCoreNLP\u001b[39;00m\n\u001b[1;32m    141\u001b[0m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(json\u001b[38;5;241m.\u001b[39mloads)\n\u001b[0;32m--> 142\u001b[0m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    143\u001b[0m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdep num\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m words_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;28msum\u001b[39m(dep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgovernorGloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mtranslate({\u001b[38;5;28mord\u001b[39m(char): \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m remove_chars}) \u001b[38;5;241m==\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m dep \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasicDependencies\u001b[39m\u001b[38;5;124m'\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    144\u001b[0m words_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m words_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: WordNetLemmatizer()\u001b[38;5;241m.\u001b[39mlemmatize(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphrase\u001b[39m\u001b[38;5;124m'\u001b[39m], get_wordnet_pos(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m])), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# # New code with functions outside of For loop to try and optimise \n",
    "# #  Check results in FEatures file\n",
    "\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import string\n",
    "# from pycorenlp import StanfordCoreNLP\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# from datamuse import datamuse\n",
    "# import json\n",
    "\n",
    "# # Initialize Datamuse API and StanfordCoreNLP\n",
    "# api = datamuse.Datamuse()\n",
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "\n",
    "# # Set the path for the input file\n",
    "# file_path = \"cwishareddataset/traindevset/english/pickled-dataframes/Wikipedia_Dev_small.pkl\"\n",
    "\n",
    "# # Read the .pkl file into a DataFrame\n",
    "# data_frame = pd.read_pickle(file_path)\n",
    "\n",
    "# # # Set the paths for the input and output folders\n",
    "# # folder_path = \"cwishareddataset/traindevset/english/pickled-dataframes\"\n",
    "# output_folder = \"features\"\n",
    "\n",
    "# # Function to obtain syllables for words\n",
    "# def get_syllables(word):\n",
    "#     word_results = api.words(sp=word, max=1, md='psf')\n",
    "#     if word_results:\n",
    "#         return int(word_results[0][\"numSyllables\"])\n",
    "#     return 0\n",
    "\n",
    "# # Function to parse sentences\n",
    "# def parse(string):\n",
    "#     output = nlp.annotate(string, properties={\n",
    "#         'annotators': 'pos,depparse',\n",
    "#         'outputFormat': 'json'\n",
    "#     })\n",
    "#     return json.dumps(output)\n",
    "\n",
    "# # Function to get the proper lemma \n",
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     if treebank_tag.startswith('JJ'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('VB'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('NN'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('RB'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Function to get the number of synonyms for a word\n",
    "# def synonyms(word):\n",
    "#     try:\n",
    "#         results = wordnet.synsets(word)\n",
    "#         return len(results)\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# # Function to get the number of hypernyms for a word\n",
    "# def hypernyms(word):\n",
    "#     try:\n",
    "#         results = wordnet.synsets(word)\n",
    "#         return len(results[0].hypernyms())\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# # Function to get the number of hyponyms for a word\n",
    "# def hyponyms(word):\n",
    "#     try:\n",
    "#         results = wordnet.synsets(word)\n",
    "#         return len(results[0].hyponyms())\n",
    "#     except:\n",
    "#         return 0\n",
    "\n",
    "# # Function to get the Cambridge Advanced Learner Dictionary (CALD) level for a word\n",
    "# # def levels(word):\n",
    "# #     word = ''.join(word.split()).lower()\n",
    "# #     try:\n",
    "# #         level = all_levels.loc[all_levels['word'] == word, 'level'].values[0]\n",
    "# #         return level\n",
    "# #     except:\n",
    "# #         return 0\n",
    "\n",
    "# # Function to get the frequency of a word using Datamuse API\n",
    "# def get_frequency(word, tag):\n",
    "#     nofreq = float(0.000000)\n",
    "#     try:\n",
    "#         word_results = api.words(sp=word, max=1, md='pf')\n",
    "#         tag_list = word_results[0]['tags'][:-1]\n",
    "#         frequency = word_results[0]['tags'][-1][2:]\n",
    "#         frequency = float(frequency)\n",
    "#         if tag in tag_list:\n",
    "#             return frequency\n",
    "#     except:\n",
    "#         pass\n",
    "#     return nofreq\n",
    "\n",
    "# # Set of characters to remove from words\n",
    "# remove_chars = string.punctuation.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "\n",
    "# # Iterate over .pkl files in the folder\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith('.pkl'):\n",
    "#         # Check if the filename contains \"WikiNews\"\n",
    "#         Wikinews = True if 'WikiNews' in filename else False\n",
    "\n",
    "#         # Construct the file path\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "#         # Read the .pkl file into a DataFrame\n",
    "#         data_frame = pd.read_pickle(file_path)\n",
    "\n",
    "#         # Name the columns\n",
    "#         data_frame.columns = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "\n",
    "#         # Process data_frame to get words and their syllables\n",
    "#         data_frame['phrase'] = data_frame['phrase'].str.lower().str.translate({ord(char): None for char in remove_chars})\n",
    "#         data_frame['split'] = data_frame['phrase'].apply(lambda x: x.split())\n",
    "#         words_df = data_frame[data_frame['split'].apply(lambda x: len(x) == 1)].copy()\n",
    "#         words_df['syllables'] = words_df['phrase'].apply(get_syllables)\n",
    "\n",
    "#         # Parse sentences and merge with words_df\n",
    "#         sentences_df = data_frame[['sentence', 'ID']].drop_duplicates()\n",
    "\n",
    "#         if Wikinews:\n",
    "#             # For Wikinews dataset, remove the first word in the sentence\n",
    "#             sentences_df['clean sentence'] = sentences_df['sentence'].apply(lambda x: ' '.join(x.split()[1:]))\n",
    "#         else:\n",
    "#             sentences_df['clean sentence'] = sentences_df['sentence']\n",
    "\n",
    "#         sentences_df['parse'] = sentences_df['clean sentence'].apply(parse)\n",
    "#         words_df = words_df.merge(sentences_df, on='ID')\n",
    "\n",
    "#         # Get linguistic features using StanfordCoreNLP\n",
    "#         words_df['parse'] = words_df['parse'].apply(json.loads)\n",
    "#         words_df['pos'] = words_df['parse'].apply(lambda x: x['sentences'][0]['tokens'][0]['pos'])\n",
    "#         words_df['dep num'] = words_df.apply(lambda row: sum(dep['governorGloss'].lower().translate({ord(char): None for char in remove_chars}) == row['phrase'] for dep in row['parse']['sentences'][0]['basicDependencies']), axis=1)\n",
    "#         words_df['lemma'] = words_df.apply(lambda row: WordNetLemmatizer().lemmatize(row['phrase'], get_wordnet_pos(row['pos'])), axis=1)\n",
    "\n",
    " \n",
    "#         # Get additional linguistic features\n",
    "#         words_df['synonyms'] = words_df['lemma'].apply(synonyms)\n",
    "#         words_df['hypernyms'] = words_df['lemma'].apply(hypernyms)\n",
    "#         words_df['hyponyms'] = words_df['lemma'].apply(hyponyms)\n",
    "\n",
    "#         # Check if words are in specific word sets\n",
    "#         ogden = pd.read_table('binary-features/ogden.txt')\n",
    "#         words_df['ogden'] = words_df['lemma'].isin(ogden['words']).astype(int)\n",
    "\n",
    "#         simple_wiki = pd.read_csv('binary-features/Most_Frequent.csv')\n",
    "#         words_df['simple_wiki'] = words_df['lemma'].isin(simple_wiki['a']).astype(int)\n",
    "\n",
    "#         subimdb_500 = pd.read_csv('binary-features/subimbd_500.tsv', sep='\\t')\n",
    "#         words_df['sub_imdb'] = words_df['lemma'].isin(subimdb_500['words']).astype(int)\n",
    "\n",
    "#         # Get MRC features\n",
    "#         mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))\n",
    "#         words_df = words_df.merge(mrc_features, how='left', left_on='lemma', right_on='word', suffixes=('', '_mrc'))\n",
    "\n",
    "#         # # Get Cambridge Advanced Learner Dictionary (CALD) level\n",
    "#         # all_levels = pd.read_csv('binary-features/CALD.csv')\n",
    "#         # words_df['cald'] = words_df['phrase'].apply(levels)\n",
    "\n",
    "#         # Get word frequency\n",
    "#         words_df['frequency'] = words_df.apply(lambda x: get_frequency(x['phrase'], x['pos']), axis=1)\n",
    "\n",
    "#         # Save the processed DataFrame\n",
    "#         output_filename = os.path.splitext(filename)[0] + '_actual'\n",
    "#         output_file_path = os.path.join(output_folder, output_filename)\n",
    "#         words_df.to_pickle(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# def create_small_pkl_file(large_pkl_path, small_pkl_path, num_elements_to_keep):\n",
    "#     # Load the larger .pkl file\n",
    "#     with open(large_pkl_path, 'rb') as large_file:\n",
    "#         data = pickle.load(large_file)\n",
    "\n",
    "#     # Extract a subset of data\n",
    "#     if num_elements_to_keep > len(data):\n",
    "#         raise ValueError(\"The specified number of elements is larger than the original data.\")\n",
    "#     subset_data = data[:num_elements_to_keep]  # You can change this to randomly sample the data.\n",
    "\n",
    "#     # Save the smaller .pkl file\n",
    "#     with open(small_pkl_path, 'wb') as small_file:\n",
    "#         pickle.dump(subset_data, small_file)\n",
    "\n",
    "# # Example usage:\n",
    "# large_pkl_file_path = 'cwishareddataset/traindevset/english/pickled-dataframes/Wikipedia_Dev.pkl'\n",
    "# small_pkl_file_path = 'cwishareddataset/traindevset/english/pickled-dataframes/Wikipedia_Dev_small.pkl'\n",
    "# num_elements_to_keep = 100  # Set the number of elements you want to keep in the smaller .pkl file\n",
    "\n",
    "# create_small_pkl_file(large_pkl_file_path, small_pkl_file_path, num_elements_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def pkl_to_csv(pkl_file, output_dir):\n",
    "    # Load the data from the .pkl file\n",
    "    with open(pkl_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Convert the data to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate the output .csv file path\n",
    "    output_csv = os.path.join(output_dir, os.path.basename(pkl_file).replace('_actual', '.csv'))\n",
    "    \n",
    "    # Save the DataFrame to a .csv file\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "def convert_pkl_to_csv_in_folder(folder_path):\n",
    "    # Create an output folder to store the converted .csv files\n",
    "    output_dir = os.path.join(folder_path, 'csv_files')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get a list of all files ending with \"_actual\" in the folder\n",
    "    pkl_files = glob.glob(os.path.join(folder_path, '*_actual'))\n",
    "\n",
    "    # Convert each file to .csv\n",
    "    for pkl_file in pkl_files:\n",
    "        pkl_to_csv(pkl_file, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"features\"  # Replace with the actual path to your folder containing \"_actual\" files\n",
    "    convert_pkl_to_csv_in_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
