{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populating word Features\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import string\n",
    "import regex as re\n",
    "import json\n",
    "import os\n",
    "from datamuse import datamuse\n",
    "import pycorenlp\n",
    "import pandas as pd\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Datamuse API and StanfordCoreNLP\n",
    "api = datamuse.Datamuse()\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "# Set the paths for the input and output folders\n",
    "folder_path = \"cwishareddataset/testset/english/pickled-dataframes\"\n",
    "output_folder = \"final_camb_feats_Test\"\n",
    "\n",
    "        \n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pkl'):\n",
    "        # Check if the filename contains \"WikiNews\"\n",
    "        Wikinews = True if 'WikiNews' in filename else False\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read the .pkl file into a DataFrame\n",
    "        data_frame = pd.read_pickle(file_path)\n",
    "        \n",
    "       \n",
    "        data_frame.columns = ['ID', 'sentence', 'start_index', 'end_index', 'phrase', 'total_native', 'total_non_native', 'native_complex', 'non_native_complex', 'complex_binary', 'complex_probabilistic']\n",
    "\n",
    "\n",
    "        # Perform data processing\n",
    "        data_frame['split'] = data_frame['phrase'].apply(lambda x: x.split())\n",
    "        data_frame['count'] = data_frame['split'].apply(lambda x: len(x))\n",
    "        words = data_frame[data_frame['count'] == 1]\n",
    "        # MWEs = data_frame[data_frame['count'] >1]\n",
    "        word_set = words.phrase.str.lower().unique()\n",
    "        word_set = pd.DataFrame(word_set, columns=['phrase'])\n",
    "        remove = string.punctuation.replace(\"-\", \"\").replace(\"'\", \"\") + '“”'\n",
    "        pattern = r\"[{}]\".format(remove)\n",
    "        word_set['phrase'] = word_set['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #function to obtain syablles for words\n",
    "        # from datamuse import datamuse\n",
    "        # api = datamuse.Datamuse()\n",
    "\n",
    "        def get_syllables(word):\n",
    "            syllables = 0\n",
    "            word_results = api.words(sp=word, max=1, md='psf')\n",
    "            if len(word_results)>0: \n",
    "                word = word_results[0][\"word\"]\n",
    "                syllables = int(word_results[0][\"numSyllables\"])\n",
    "            return syllables\n",
    "\n",
    "        #Apply function to get syllables\n",
    "        word_set['syllables'] = word_set['phrase'].apply(lambda x: get_syllables(x))\n",
    "\n",
    "        #Apply function to get word length \n",
    "        word_set['length'] = word_set['phrase'].apply(lambda x: len(x))\n",
    "\n",
    "        #take words and merge with values first you will need to clean the phrase column \n",
    "        words['original phrase'] = words['phrase']\n",
    "        words['phrase'] = words['phrase'].str.lower()\n",
    "        words['phrase'] = words['phrase'].apply(lambda x: x.translate({ord(char): None for char in remove}))\n",
    "\n",
    "        word_features = pd.merge(words, word_set)\n",
    "        \n",
    "        #Now parse\n",
    "        # import pycorenlp\n",
    "        # import pandas as pd\n",
    "        # from pycorenlp import StanfordCoreNLP\n",
    "        # nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "        sentences = data_frame[['sentence', 'ID']].copy()\n",
    "\n",
    "        sentences = sentences.drop_duplicates()\n",
    "\n",
    "        def removefirsttoken(x):\n",
    "            x = x.split(' ', 1)[1]\n",
    "            return x\n",
    "\n",
    "        if Wikinews:\n",
    "            sentences['clean sentence'] = sentences['sentence'].apply(lambda x: removefirsttoken(x))\n",
    "\n",
    "        else:\n",
    "            sentences['clean sentence'] = sentences['sentence']\n",
    "\n",
    "        #function to parse sentences \n",
    "        def parse(string):\n",
    "            output = nlp.annotate(string, properties={\n",
    "          'annotators': 'pos,depparse',\n",
    "          'outputFormat': 'json'\n",
    "          })\n",
    "            return output\n",
    "        \n",
    "        #apply parsing to sentences\n",
    "        sentences['parse'] = sentences['clean sentence'].apply(lambda x: parse(x))\n",
    "\n",
    "        sentences\n",
    "\n",
    "        #Merge \n",
    "        word_parse_features = pd.merge(sentences, word_features)\n",
    "        word_parse_features\n",
    "        \n",
    "        def get_pos(row):\n",
    "            word = row['phrase']\n",
    "            parse = json.loads(row['parse'])\n",
    "            for i in range(len(parse['sentences'][0]['tokens'])):\n",
    "                comp_word = parse['sentences'][0]['tokens'][i]['word']\n",
    "                comp_word = comp_word.lower()\n",
    "                comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "                if comp_word == word:\n",
    "                    return parse['sentences'][0]['tokens'][i]['pos']\n",
    "        \n",
    "\n",
    "        def get_dep(row):\n",
    "            number = 0\n",
    "            word = row['phrase']\n",
    "            parse = json.loads(row['parse'])\n",
    "            for i in range(len(parse['sentences'][0]['basicDependencies'])):\n",
    "                comp_word = parse['sentences'][0]['basicDependencies'][i]['governorGloss']\n",
    "                comp_word = comp_word.lower()\n",
    "                comp_word = comp_word.translate({ord(char): None for char in remove})\n",
    "\n",
    "                if comp_word == word:\n",
    "                    number += 1\n",
    "\n",
    "            return number\n",
    "\n",
    "        #Function to get the proper lemma \n",
    "        import nltk\n",
    "        from nltk.corpus import wordnet\n",
    "\n",
    "        def get_wordnet_pos(treebank_tag):\n",
    "            from nltk.corpus import wordnet\n",
    "\n",
    "            if treebank_tag.startswith('JJ'):\n",
    "                return wordnet.ADJ\n",
    "            elif treebank_tag.startswith('VB'):\n",
    "                return wordnet.VERB\n",
    "            elif treebank_tag.startswith('NN'):\n",
    "                return wordnet.NOUN\n",
    "            elif treebank_tag.startswith('RB'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        def lemmatiser(row):\n",
    "\n",
    "            word = row['phrase']\n",
    "            pos = row['pos']\n",
    "\n",
    "            try:\n",
    "                lemma = wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "                return lemma\n",
    "            except:\n",
    "                try:\n",
    "                    lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "                    return lemma\n",
    "                except:\n",
    "                    print(word)\n",
    "                    \n",
    "        #return MRC scores\n",
    "        # mrc_features = pd.read_table('corpus/MRC.csv', names=('word', 'AOA', 'BFRQ', 'CNC', 'KFCAT', 'FAM', 'KFSMP', 'IMG', 'KFFRQ', 'NLET', 'CMEAN', 'PMEAN', 'NPHN', 'T-LFRQ'))\n",
    "        mrc_features = pd.read_csv('corpus/MRC.csv', names=('id', 'NPHN', 'KFFRQ', 'KFCAT', 'KFSMP', 'T-LFRQ', 'FAM', 'CNC', 'IMG', 'AOA', 'word'), low_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def aoa(word):\n",
    "            word = word.upper()  # Convert word to all capitals\n",
    "            try:\n",
    "                df = mrc_features.loc[mrc_features['word'] == word]\n",
    "                fvalue = df.iloc[0]['AOA']\n",
    "                return fvalue    \n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "\n",
    "        def CNC_fun(word):\n",
    "            word = word.upper()\n",
    "            table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "            if len(table)>0:\n",
    "\n",
    "                CNC = table['CNC'].values[0]\n",
    "                CNC = int(CNC)\n",
    "\n",
    "                return CNC\n",
    "            else: \n",
    "                y=0\n",
    "                return y\n",
    "\n",
    "        def img(word):\n",
    "            word = word.upper()\n",
    "            try:\n",
    "                df = mrc_features.loc[mrc_features['word'] == word]\n",
    "                fvalue = df.iloc[0]['IMG']\n",
    "                return fvalue    \n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def KFCAT_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFCAT = table['KFCAT'].values[0]\n",
    "                    KFCAT = int(KFCAT)\n",
    "\n",
    "                    return KFCAT\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def FAM_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    FAM = table['FAM'].values[0]\n",
    "                    FAM = int(FAM)\n",
    "\n",
    "                    return FAM\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def KFSMP_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFSMP = table['KFSMP'].values[0]\n",
    "                    KFSMP = int(KFSMP)\n",
    "\n",
    "                    return KFSMP\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def KFFRQ_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    KFFRQ = table['KFFRQ'].values[0]\n",
    "                    KFFRQ = int(KFFRQ)\n",
    "\n",
    "                    return KFFRQ\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        # def NLET_fun(word):\n",
    "        #         word = word.upper()\n",
    "        #         table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "        #         if len(table)>0:\n",
    "\n",
    "\n",
    "        #             NLET = table['NLET'].values[0]\n",
    "        #             NLET = int(NLET)\n",
    "\n",
    "        #             return NLET\n",
    "        #         else: \n",
    "        #             y=0\n",
    "        #             return y\n",
    "\n",
    "        def NPHN_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    NPHN = table['NPHN'].values[0]\n",
    "                    NPHN = int(NPHN)\n",
    "\n",
    "                    return NPHN\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        def TLFRQ_fun(word):\n",
    "                word = word.upper()\n",
    "                table = mrc_features[mrc_features['word']==word]\n",
    "\n",
    "                if len(table)>0:\n",
    "\n",
    "                    TLFRQ = table['T-LFRQ'].values[0]\n",
    "                    TLFRQ = int(TLFRQ)\n",
    "\n",
    "                    return TLFRQ\n",
    "                else: \n",
    "                    y=0\n",
    "                    return y\n",
    "\n",
    "        #functions using wordnet \n",
    "        from nltk.corpus import wordnet\n",
    "        def synonyms(word):\n",
    "            synonyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "                synonyms = len(results)\n",
    "                return synonyms\n",
    "            except:\n",
    "                return synonyms\n",
    "\n",
    "        def hypernyms(word):\n",
    "            hypernyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "                hypernyms = len(results[0].hypernyms())\n",
    "                return hypernyms\n",
    "            except:\n",
    "                return hypernyms\n",
    "\n",
    "        def hyponyms(word):\n",
    "            hyponyms=0\n",
    "            try:\n",
    "                results = wordnet.synsets(word)\n",
    "            except:\n",
    "                return hyponyms\n",
    "            try:\n",
    "                hyponyms = len(results[0].hyponyms())\n",
    "                return hyponyms\n",
    "            except:\n",
    "                return hyponyms\n",
    "\n",
    "        #return CEFR levels\n",
    "        # all_levels = pd.read_table('corpus/CALD.csv', names=('word', 'level'))\n",
    "\n",
    "        # def levels(word):\n",
    "        #     all_levels = pd.read_csv('corpus/cefrj-vocabulary-profile-1.5.csv')\n",
    "        #     word = ''.join(word.split()).lower()\n",
    "        #     df = all_levels.loc[all_levels['headword'] == word]\n",
    "        #     if not df.empty:\n",
    "        #         level = df.iloc[0]['CEFR']\n",
    "        #         return level\n",
    "        #     else:\n",
    "        #         return 0\n",
    "\n",
    "        def levels(word):\n",
    "            word = ''.join(word.split()).lower()\n",
    "            try:\n",
    "                df = all_levels.loc[all_levels['word'] == word]\n",
    "                level = df.iloc[0]['level']\n",
    "                return level\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    df = all_levels.loc[all_levels['word'] == word]\n",
    "                    level = df.iloc[0]['level']\n",
    "                    return level\n",
    "                except:\n",
    "                    return 0\n",
    "                \n",
    "        #Convert tree bank tags to ones that are compatible w google \n",
    "\n",
    "        def is_noun(tag):\n",
    "            return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "        def is_verb(tag):\n",
    "            return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "        def is_adverb(tag):\n",
    "            return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "        def is_adjective(tag):\n",
    "            return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "\n",
    "        def penn_to_wn(tag):\n",
    "            if is_adjective(tag):\n",
    "                return wn.ADJ\n",
    "            elif is_noun(tag):\n",
    "                return wn.NOUN\n",
    "            elif is_adverb(tag):\n",
    "                return wn.ADV\n",
    "            elif is_verb(tag):\n",
    "                return wn.VERB\n",
    "            return None\n",
    "\n",
    "\n",
    "        def penn_to_google(tag):\n",
    "            if is_adjective(tag):\n",
    "                return 'adj'\n",
    "            elif is_noun(tag):\n",
    "                return 'n'\n",
    "            elif is_adverb(tag):\n",
    "                return 'adv'\n",
    "            elif is_verb(tag):\n",
    "                return 'v'\n",
    "            return None\n",
    "\n",
    "        \n",
    "        def get_frequency(row):\n",
    "                nofreq = float(0.000000)\n",
    "                word = row[\"phrase\"]\n",
    "                word = str(word)\n",
    "                tag = row[\"pos\"]\n",
    "                tag = penn_to_google(tag)\n",
    "\n",
    "                try:\n",
    "                    word_results = api.words(sp=word, max=1, md='pf')\n",
    "                    tag_list = (word_results[0]['tags'][:-1])\n",
    "\n",
    "                    frequency = word_results[0]['tags'][-1][2:]\n",
    "\n",
    "                    frequency = float(frequency)\n",
    "\n",
    "                    if tag in tag_list :\n",
    "                        return frequency\n",
    "                    else:\n",
    "                        lemma = row['lemma']\n",
    "                        try:\n",
    "                            word_results = api.words(sp=lemma, max=1, md='pf')\n",
    "                            tag_list = (word_results[0]['tags'][:-1])\n",
    "\n",
    "                            frequency = word_results[0]['tags'][-1][2:]\n",
    "\n",
    "                            frequency = float(frequency)\n",
    "\n",
    "                            if tag in tag_list:\n",
    "                                return frequency\n",
    "                            else:\n",
    "                                return nofreq\n",
    "                        except:\n",
    "                            return nofreq\n",
    "\n",
    "                except:\n",
    "\n",
    "\n",
    "                    return nofreq \n",
    "                \n",
    "\n",
    "        #GET DEP AND POS NUMBER\n",
    "        word_parse_features['pos'] = word_parse_features.apply(get_pos, axis=1)\n",
    "        word_parse_features['dep num'] = word_parse_features.apply(get_dep, axis=1)\n",
    "\n",
    "        #To obtain word lemmas \n",
    "        #Get Lemma\n",
    "        word_parse_features['lemma'] = word_parse_features.apply(lemmatiser, axis=1)\n",
    "\n",
    "        #Apply function to get number of synonyms and hypernyms/hyponyms\n",
    "        word_parse_features['synonyms'] = word_parse_features['lemma'].apply(lambda x: synonyms(x))\n",
    "        word_parse_features['hypernyms'] = word_parse_features['lemma'].apply(lambda x: hypernyms(x))\n",
    "        word_parse_features['hyponyms'] = word_parse_features['lemma'].apply(lambda x: hyponyms(x))\n",
    "\n",
    "        #Apply function to check if contained in Ogden word set\n",
    "        ogden = pd.read_table('binary-features/ogden.txt')\n",
    "        word_parse_features['ogden'] = word_parse_features['lemma'].apply(lambda x : 1 if any(ogden.words == x) else 0) #clean words\n",
    "\n",
    "        #Apply function to check if contained in simple wiki word set\n",
    "        simple_wiki = pd.read_csv('binary-features/Most_Frequent.csv')\n",
    "        word_parse_features['simple_wiki'] = word_parse_features['lemma'].apply(lambda x : 1 if any(simple_wiki.a == x) else 0) #clean words\n",
    "\n",
    "        # #Apply function to get the level from Cambridge Advanced Learner Dictionary\n",
    "        # cald = pd.read_csv('binary-features/CALD.csv')\n",
    "        # # word_parse_features['cald'] = word_parse_features['phrase'].apply(lambda x : 1 if any(cald.a == x) else 0)\n",
    "        # word_parse_features['cald'] = word_parse_features['phrase'].apply(lambda x: 1 if any(cald['Word'] == x) else 0)\n",
    "\n",
    "\n",
    "        #Get some MRC features\n",
    "        mrc_features = pd.read_csv('corpus/MRC.csv', names=('id','NPHN','KFFRQ','KFCAT','KFSMP','T-LFRQ','FAM','CNC','IMG','AOA', 'word'))    \n",
    "\n",
    "\n",
    "\n",
    "        # word_parse_features['cnc'] = word_parse_features['lemma'].apply(lambda x: cnc(x))\n",
    "        word_parse_features['CNC'] = word_parse_features['lemma'].apply(lambda x: CNC_fun(x) if x is not None else None)\n",
    "        word_parse_features['IMG'] = word_parse_features['lemma'].apply(lambda x: img(x) if x is not None else None)\n",
    "\n",
    "\n",
    "        #Apply function to check if contained  subimdb word set\n",
    "        subimdb_500 = pd.read_csv('binary-features/subimbd_500.tsv', sep='\\t')\n",
    "        # subimdb_500 = pd.read_pickle('binary-features/subimbd_500.tsv')\n",
    "        word_parse_features['sub_imdb'] = word_parse_features['lemma'].apply(lambda x : 1 if any(subimdb_500.words == x) else 0)\n",
    "\n",
    "        #Apply function for google freq\n",
    "        word_parse_features['google frequency'] = word_parse_features.apply(get_frequency ,axis=1)\n",
    "\n",
    "        word_parse_features['phrase'] = word_parse_features.phrase.astype(str)\n",
    "        word_parse_features['pos'] = word_parse_features.pos.astype(str)\n",
    "\n",
    "        # word_parse_features['cnc'] = word_parse_features['lemma'].apply(lambda x: cnc(x))\n",
    "        word_parse_features['CNC'] = word_parse_features['lemma'].apply(lambda x: CNC_fun(x))\n",
    "        word_parse_features['IMG'] = word_parse_features['lemma'].apply(lambda x: img(x))\n",
    "\n",
    "        word_parse_features['KFCAT']= word_parse_features['lemma'].apply(lambda x: KFCAT_fun(x))\n",
    "        word_parse_features['FAM']= word_parse_features['lemma'].apply(lambda x: FAM_fun(x) )\n",
    "        word_parse_features['KFSMP']= word_parse_features['lemma'].apply(lambda x: KFSMP_fun(x))\n",
    "        word_parse_features['KFFRQ']= word_parse_features['lemma'].apply(lambda x: KFFRQ_fun(x))\n",
    "        word_parse_features['AOA']= word_parse_features['lemma'].apply(lambda x: aoa(x))\n",
    "        word_parse_features['NPHN']= word_parse_features['lemma'].apply(lambda x: NPHN_fun(x))\n",
    "        word_parse_features['T-LFRQ']= word_parse_features['lemma'].apply(lambda x: TLFRQ_fun(x))\n",
    "        \n",
    "        \n",
    "        # Combine single word dataframe and multiple words dataframe\n",
    "        # combined_df = pd.concat([word_set, MWEs])\n",
    "\n",
    "        # Sort combined dataframe by original dataframe's index\n",
    "        # word_parse_features = combined_df.sort_index()\n",
    "        \n",
    "        # Combine word_parse_features and MWEs DataFrames NOT WORKING AS NEEDED\n",
    "        # word_parse_features_with_MWEs = pd.concat([word_parse_features, MWEs])\n",
    "\n",
    "        # Sort the combined DataFrame by the original order (index)\n",
    "        # combined_df.sort_index(inplace=True)\n",
    "\n",
    "        # Save the processed DataFrame\n",
    "        output_filename = os.path.splitext(filename)[0] + '_actual_Final'\n",
    "        output_file_path = os.path.join(output_folder, output_filename)\n",
    "        word_parse_features.to_pickle(output_file_path)\n",
    "        # word_parse_features_with_MWEs.to_pickle(output_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>ID</th>\n",
       "      <th>clean sentence</th>\n",
       "      <th>parse</th>\n",
       "      <th>start_index</th>\n",
       "      <th>end_index</th>\n",
       "      <th>phrase</th>\n",
       "      <th>total_native</th>\n",
       "      <th>total_non_native</th>\n",
       "      <th>native_complex</th>\n",
       "      <th>non_native_complex</th>\n",
       "      <th>complex_binary</th>\n",
       "      <th>complex_probabilistic</th>\n",
       "      <th>split</th>\n",
       "      <th>count</th>\n",
       "      <th>original phrase</th>\n",
       "      <th>syllables</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>considered</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[considered]</td>\n",
       "      <td>1</td>\n",
       "      <td>considered</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>victims</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[victims]</td>\n",
       "      <td>1</td>\n",
       "      <td>victims</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>status</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[status]</td>\n",
       "      <td>1</td>\n",
       "      <td>status</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>83</td>\n",
       "      <td>90</td>\n",
       "      <td>changed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[changed]</td>\n",
       "      <td>1</td>\n",
       "      <td>changed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#26-7 Initially, all three were considered vic...</td>\n",
       "      <td>374UMBUHN5QN3F8F90U3OEJ8SKCTCW</td>\n",
       "      <td>Initially, all three were considered victims, ...</td>\n",
       "      <td>{\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...</td>\n",
       "      <td>94</td>\n",
       "      <td>101</td>\n",
       "      <td>suspect</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[suspect]</td>\n",
       "      <td>1</td>\n",
       "      <td>suspect</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>87</td>\n",
       "      <td>Other Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[Other, Backward, Class]</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>87</td>\n",
       "      <td>Other Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[Other, Backward, Class]</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>87</td>\n",
       "      <td>Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Backward, Class]</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>87</td>\n",
       "      <td>Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Backward, Class]</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>#40-9 However, they want their community to be...</td>\n",
       "      <td>3P6ENY9P79XOB93K1G90LYEFUV2HID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>87</td>\n",
       "      <td>Backward Class</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[Backward, Class]</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1057 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "0    #26-7 Initially, all three were considered vic...   \n",
       "1    #26-7 Initially, all three were considered vic...   \n",
       "2    #26-7 Initially, all three were considered vic...   \n",
       "3    #26-7 Initially, all three were considered vic...   \n",
       "4    #26-7 Initially, all three were considered vic...   \n",
       "..                                                 ...   \n",
       "859  #40-9 However, they want their community to be...   \n",
       "859  #40-9 However, they want their community to be...   \n",
       "862  #40-9 However, they want their community to be...   \n",
       "862  #40-9 However, they want their community to be...   \n",
       "862  #40-9 However, they want their community to be...   \n",
       "\n",
       "                                 ID  \\\n",
       "0    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "1    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "2    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "3    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "4    374UMBUHN5QN3F8F90U3OEJ8SKCTCW   \n",
       "..                              ...   \n",
       "859  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "859  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "862  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "862  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "862  3P6ENY9P79XOB93K1G90LYEFUV2HID   \n",
       "\n",
       "                                        clean sentence  \\\n",
       "0    Initially, all three were considered victims, ...   \n",
       "1    Initially, all three were considered victims, ...   \n",
       "2    Initially, all three were considered victims, ...   \n",
       "3    Initially, all three were considered victims, ...   \n",
       "4    Initially, all three were considered victims, ...   \n",
       "..                                                 ...   \n",
       "859                                                NaN   \n",
       "859                                                NaN   \n",
       "862                                                NaN   \n",
       "862                                                NaN   \n",
       "862                                                NaN   \n",
       "\n",
       "                                                 parse  start_index  \\\n",
       "0    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           32   \n",
       "1    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           43   \n",
       "2    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           60   \n",
       "3    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           83   \n",
       "4    {\\n  \"sentences\": [\\n    {\\n      \"index\": 0,\\...           94   \n",
       "..                                                 ...          ...   \n",
       "859                                                NaN           67   \n",
       "859                                                NaN           67   \n",
       "862                                                NaN           73   \n",
       "862                                                NaN           73   \n",
       "862                                                NaN           73   \n",
       "\n",
       "     end_index                phrase  total_native  total_non_native  \\\n",
       "0           42            considered            10                10   \n",
       "1           50               victims            10                10   \n",
       "2           66                status            10                10   \n",
       "3           90               changed            10                10   \n",
       "4          101               suspect            10                10   \n",
       "..         ...                   ...           ...               ...   \n",
       "859         87  Other Backward Class            10                10   \n",
       "859         87  Other Backward Class            10                10   \n",
       "862         87        Backward Class            10                10   \n",
       "862         87        Backward Class            10                10   \n",
       "862         87        Backward Class            10                10   \n",
       "\n",
       "     native_complex  non_native_complex  complex_binary  \\\n",
       "0                 2                   3               1   \n",
       "1                 4                   2               1   \n",
       "2                 3                   1               1   \n",
       "3                 0                   2               1   \n",
       "4                 3                   2               1   \n",
       "..              ...                 ...             ...   \n",
       "859               0                   1               1   \n",
       "859               0                   1               1   \n",
       "862               0                   0               0   \n",
       "862               0                   0               0   \n",
       "862               0                   0               0   \n",
       "\n",
       "     complex_probabilistic                     split  count original phrase  \\\n",
       "0                     0.25              [considered]      1      considered   \n",
       "1                     0.30                 [victims]      1         victims   \n",
       "2                     0.20                  [status]      1          status   \n",
       "3                     0.10                 [changed]      1         changed   \n",
       "4                     0.25                 [suspect]      1         suspect   \n",
       "..                     ...                       ...    ...             ...   \n",
       "859                   0.05  [Other, Backward, Class]      3             NaN   \n",
       "859                   0.05  [Other, Backward, Class]      3             NaN   \n",
       "862                   0.00         [Backward, Class]      2             NaN   \n",
       "862                   0.00         [Backward, Class]      2             NaN   \n",
       "862                   0.00         [Backward, Class]      2             NaN   \n",
       "\n",
       "     syllables  length  \n",
       "0          3.0    10.0  \n",
       "1          2.0     7.0  \n",
       "2          2.0     6.0  \n",
       "3          1.0     7.0  \n",
       "4          2.0     7.0  \n",
       "..         ...     ...  \n",
       "859        NaN     NaN  \n",
       "859        NaN     NaN  \n",
       "862        NaN     NaN  \n",
       "862        NaN     NaN  \n",
       "862        NaN     NaN  \n",
       "\n",
       "[1057 rows x 18 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
